---
title: "从零开始自己手写语言模型 (一): 大语言模型简介"
date: 2025-07-18T09:11:01-05:00
author: "郝鸿涛"
slug: llm
draft: false
toc: false
tags: llm
---
[这里](https://github.com/skindhu/Build-A-Large-Language-Model-CN)有 Build a large language model (from scratch) 的中文翻译。原版英文 PDF 也可以找到。不过我建议买得起原版书的同学还是自己花钱在[官网上](https://www.manning.com/books/build-a-large-language-model-from-scratch)支持一下原作者，毕竟创作不易。

下面是我的总结和笔记，并非翻译。

LLM 的基础是 Transformers。Transformers 一开始是用来做机器翻译的。

{{< figure src="/media/cnblog/llms/transformer.png" title="Transformer 架构" width="600">}}

它主要有两个模块：encoder（编码器）和 decoder（解码器）。Encoder 的作用是把输入文本变成一组上下文相关的向量表示 (contextualized embeddings)，具体的流程：

- 每个 token 在 ebmedding matrix 中找到自己的位置 (即，一个规定维度的向量)
- 给每个 token 加上顺序信息 (positional encoding)，比如，「我是一个好学生」，你要让计算机知道第一个词是「我」，第二个是「是」，等等。
- 让每个词注意到其他的词，然后调整自己的向量表示。这就是 self-attention 机制。

Decoder 本质上是一个自回归模型。根据 Encoder 的结果以及自己之前已经输出的 token，一步一步生成新的 token，知道任务完成。

BERT 是 encoder，而 GPT 是 decoder。

{{< figure src="/media/cnblog/llms/bert-gpt.png" title="Bert 和 GPT 的区别" width="600">}}

GPT 在 zero-shot learning 和 few-shot learning 任务上有出色的表现：

{{< figure src="/media/cnblog/llms/zero-few-shot.png" title="GPT 在 zero-shot learning 和 few-shot learning 任务上有出色的表现" width="600">}}

Transformers 和 LLMs 不是一回事。Transformers 也可以用在别的领域，比如计算机视觉。而 LLMs 也有基于非 Transform 框架的，比如基于 convolutional 框架的 LLMs。 

GPT 模型的全称：Generative Pre-Trained Transformer。它只包含 Transformer 中的 Decoder 框架。作用是根据之前的句子，预测下一个 token。

{{< figure src="/media/cnblog/llms/gpt-architecture.png" title="GPT 架构" width="600">}}

那什么是 Pre-training?

LLM 的预训练 (pretraining) 阶段是采用自监督学习 (self-supervised learning) 在没有标注的数据上训练。自监督学习算法可以自己根据数据生成标注。预训练的结果是基础模型 (foundation model)，其主要功能是 text completion。它也有能力完成 few-shot 任务，也就是说你给它几个简单的例子，它可以在其他例子上做任务，比如分类任务。

LLM 的 fine-tuning 是在预训练之后，在标注数据上训练，以让 LLM 更适合做一些更具体的任务，比如文本分类、总结、翻译、做数学题等。

{{< figure src="/media/cnblog/llms/pretraining_and_finetuning.png" title="Pretraining 和 finetuning，p.9" width="600">}}

大语言模型的预训练 (Pretraining) 耗资巨大，因为数据量非常大。但是 finetuning 可能只需要很少的数据量。

{{< figure src="/media/cnblog/llms/pretraining-dataset.png" title="GPT-3 预训练数据来源" width="600">}}

之后，我们会自己动手从零开始写一个语言模型，包含预训练和 finetuning：

{{< figure src="/media/cnblog/llms/build.png" title="从零开始写一个语言模型的流程" width="800">}}


