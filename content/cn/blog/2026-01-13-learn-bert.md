---
title: "ä»é›¶å¼€å§‹è®­ç»ƒç®€æ˜“ç‰ˆ BERT"
date: 2026-02-01
author: "éƒé¸¿æ¶›"
slug: bert
draft: false
toc: true
tags: llm
---

Bert çš„ä½œç”¨æ˜¯å¾—åˆ°ä¸Šä¸‹æ–‡ç›¸å…³çš„å‘é‡è¡¨ç¤ºã€‚ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œ

- ã€Œä½ æ¥å°±æ¥ï¼Œè¿˜å¸¦ç¤¼ç‰©ã€‚ä½ å‡ ä¸ªæ„æ€å•Šï¼Ÿã€
- ã€Œåˆ«å¤šæƒ³ã€‚æˆ‘å°±æ„æ€ä¸€ä¸‹ã€
- ã€Œæˆ‘çœ‹ä¸æ‡‚è¿™å¥è¯é‡Œè¿™ä¸ªè¯æ˜¯ä»€ä¹ˆæ„æ€ã€
- ã€Œä½ è¿™ä¹ˆè¯´å°±æ²¡æ„æ€äº†å•Šã€

ä½ çœ‹ï¼Œè¿™å‡ å¥è¯é‡Œçš„ã€Œæ„æ€ã€ï¼Œå®ƒçš„å«ä¹‰æ˜¯ä¸åŒçš„ã€‚å‘é‡è¡¨ç¤ºçš„æ—¶å€™æˆ‘ä»¬è‚¯å®šä¸èƒ½é™æ€åœ°ç»™åŒä¸€ä¸ªè¯ç›¸åŒçš„å‘é‡è¡¨ç¤ºã€‚

ä¸Šä¸‹æ–‡ç›¸å…³çš„å‘é‡è¡¨ç¤ºï¼Œå°±æ˜¯è¯´å³ä½¿æ˜¯åŒä¸€ä¸ªè¯ï¼Œåœ¨ä¸åŒçš„è¯­å¢ƒä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒçš„å‘é‡è¡¨ç¤ºæ˜¯ä¸åŒçš„ã€‚å¦‚ä½•å®ç°è¿™ä¸€ç‚¹å‘¢ï¼Ÿè¿™å°±éœ€è¦ã€Œæ³¨æ„åŠ›æœºåˆ¶ã€ï¼Œå°±æ˜¯é‚£ç¯‡å¤§åé¼é¼çš„ [Attention is all you need](https://arxiv.org/abs/1706.03762) çš„è®ºæ–‡ã€‚è¿™ç¯‡è®ºæ–‡æåˆ°çš„ Transformer æ¶æ„ï¼ŒåŒ…æ‹¬äº† Encoder å’Œ Decoderï¼Œä½† Bert åªæ¶‰åŠ Encoderã€‚

åœ¨è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬è¦åšä¸€ä¸ªã€Œå­—-å‘é‡ã€æŸ¥æ‰¾è¡¨ï¼Œä»¥ä¸‹ç®€ç§°ã€Œè¯è¡¨ã€ã€‚æˆ‘ä»¬éœ€è¦æŠŠæ‰€æœ‰çš„å­—æ•´åˆåœ¨ä¸€èµ·ã€‚æœ€å¼€å§‹ï¼Œæˆ‘ä»¬å¯ä»¥éšæœºåˆ†é…å‘é‡è¡¨ç¤ºã€‚æ¯”æ–¹è¯´ï¼Œã€Œä½ ã€çš„å‘é‡è¡¨ç¤ºä¸º `[0.1, -0.5, 0.9]` ä¹‹ç±»ã€‚å½“ç„¶ï¼ŒçœŸæ­£çš„è®­ç»ƒä¸­ï¼Œç»´åº¦ä¼šæ›´é«˜ï¼Œä½†æˆ‘ä»¬è¿™é‡Œåªæ˜¯ä¸ºäº†æ¼”ç¤ºï¼Œæ‰€ä»¥é‡åœ¨ç®€å•ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒåŒä¸€ä¸ªè¯çš„å‘é‡è¡¨ç¤ºæ˜¯ä¸€æ ·çš„ã€‚å¦å¤–ï¼Œ

Bert çš„è®­ç»ƒæ ¸å¿ƒæ˜¯è¿™æ ·å­ã€‚å…ˆéšæœºæ‰¾ä¸€ä¸ªå¥å­ï¼Œæ¯”å¦‚ ã€Œä½ è¿™ä¹ˆè¯´å°±æ²¡æ„æ€äº†å•Šã€ï¼Œç„¶åé®ä½ä¸€ä¸ªå­—ï¼Œæ¯”å¦‚ ã€Œè¯´ã€:ã€Œä½ è¿™ä¹ˆ MASK å°±æ²¡æ„æ€äº†å•Šã€ã€‚æˆ‘ä»¬æŠŠ MASK å½“ä½œä¸€ä¸ªå…·ä½“çš„å­—ã€‚

1. æ¯ä¸ªå­—é€šè¿‡è¯è¡¨æŸ¥æ‰¾è¡¨æ¥å¾—åˆ°å‘é‡è¡¨ç¤ºã€‚
2. åŠ ä¸Šä½ç½®è¡¨ç¤ºï¼Œä¹Ÿå°±æ˜¯è¯´è¦å‘Šè¯‰æ¨¡å‹ï¼Œæ¯ä¸ªå­—çš„ä½ç½®ã€‚
3. ç»è¿‡å¤šå±‚çš„ Self-Attentionï¼Œå¾—åˆ°æ¯ä¸ªå­—æ–°çš„å‘é‡è¡¨ç¤ºã€‚è¿™ä¸€ç‚¹æˆ‘ä»¬æ¥ä¸‹æ¥è®²ã€‚
4. Mask è¿™ä¸ªå­—çš„è¾“å‡ºå‘é‡ç»è¿‡åˆ†ç±»å™¨ä¹‹åï¼Œæˆ‘ä»¬å¾—åˆ°ä¸Šé¢è¯è¡¨ä¸­æ¯ä¸€ä¸ªå­—æ˜¯è¿™ä¸ªè¢«é®ä½è¯çš„æ¦‚ç‡ã€‚

è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶ã€Œæ„æ€ã€æ˜¯ä¸€ä¸ªè¯ï¼Œä¸åº”è¯¥åˆ†å¼€ï¼Œä½†æ˜¯ä¸­æ–‡åˆ†è¯å¾ˆéš¾ï¼Œè€Œä¸”å®¹æ˜“å‡ºé”™ã€‚BERT çš„å¼ºå¤§ä¹‹å¤„åœ¨äºï¼šå³ä½¿è¾“å…¥çš„æ˜¯å•ç‹¬çš„ã€Œæ„ã€å’Œã€Œæ€ã€ï¼Œç»è¿‡ Attention æœºåˆ¶åï¼Œã€Œæ„ã€çš„å‘é‡ä¼šå¸æ”¶ã€Œæ€ã€çš„ä¿¡æ¯ï¼Œä»è€Œæ¥è¿‘ã€Œæ„æ€ã€è¿™ä¸ªæ¦‚å¿µçš„å‘é‡è¡¨ç¤ºã€‚

## æ‰‹ç®— Self-Attention 

ä¸ºäº†ææ‡‚ Self-Attention åˆ°åº•æ˜¯æ€ä¹ˆç®—çš„ï¼Œæˆ‘æŠ›å¼€å¤æ‚çš„å…¬å¼ï¼Œä¸¾ä¸€ä¸ªæœ€ç®€å•çš„ä¾‹å­ã€‚

**åœºæ™¯è®¾å®šï¼š**
* è¾“å…¥åªæœ‰ä¸¤ä¸ªè‹±æ–‡è¯ï¼š**one cat**ã€‚
* å‡è®¾å‘é‡ç»´åº¦ (`$d_{model}$`) ä¸º **3**ã€‚
* è¾“å…¥å‘é‡ (`$X$`) å¦‚ä¸‹ï¼š
{{< indentedblock >}}
- one: `[1, 0, 2]`
- cat: `[-1, 2, 0]`
{{< /indentedblock >}}
å³ï¼š

`$$ X = \begin{bmatrix} 1 & 0 & 2 \\ -1 &2 & 0 \end{bmatrix} $$`

### ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡æƒé‡çŸ©é˜µ (Q, K, V)

Self-Attention éœ€è¦ä¸‰ä¸ªæƒé‡çŸ©é˜µ `$W^Q, W^K, W^V$`ã€‚ä¸ºäº†æ–¹ä¾¿æ‰‹ç®—ï¼Œæˆ‘å‡è®¾æ¨¡å‹ç›®å‰å­¦åˆ°çš„æƒé‡çŸ©é˜µå¦‚ä¸‹ã€‚ä¸ºäº†æ¼”ç¤ºæ•ˆæœï¼Œæˆ‘ç‰¹æ„è®¾è®¡äº†ç®€å•çš„æ•´æ•°çŸ©é˜µï¼š

`$$
W^Q = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}, \quad
W^K = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix}, \quad
W^V = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2 \end{bmatrix}
$$`

### ç¬¬äºŒæ­¥ï¼šè®¡ç®— Q, K, V å‘é‡

è®¡ç®—å…¬å¼ä¸ºï¼šå‘é‡ `$\times$` çŸ©é˜µã€‚

**1. è®¡ç®— Query (Q)**
`$$Q = X \times W^Q = \begin{bmatrix} 1 & 0 & 2 \\ -1 & 2 & 0 \end{bmatrix}$$`
* `$q_{\text{one}} = [1, 0, 2]$`
* `$q_{\text{cat}} = [-1, 2, 0]$`

**2. è®¡ç®— Key (K)**
`$$K = X \times W^K = \begin{bmatrix} 1 & 2 & 0 \\ -1 & 0 & 2 \end{bmatrix}$$`
* `$k_{\text{one}} = [1, 2, 0]$`  
* `$k_{\text{cat}} = [-1, 0, 2]$`

**3. è®¡ç®— Value (V)**
`$$V = X \times W^V = \begin{bmatrix} 2 & 0 & 4 \\ -2 & 4 & 0 \end{bmatrix}$$`
* `$v_{\text{one}} = [2, 0, 4]$`
* `$v_{\text{cat}} = [-2, 4, 0]$`

### ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (Attention Score)

è¿™ä¸€æ­¥æ˜¯è®¡ç®—ç›¸ä¼¼åº¦ã€‚æˆ‘ä»¬æ‹¿ **Query** å»ç‚¹ç§¯ **Key** çš„è½¬ç½® (`$Q \cdot K^T$`)ã€‚

`$$
\text{Score} = \begin{bmatrix}
q_{\text{one}} \cdot k_{\text{one}} & q_{\text{one}} \cdot k_{\text{cat}} \\
q_{\text{cat}} \cdot k_{\text{one}} & q_{\text{cat}} \cdot k_{\text{cat}}
\end{bmatrix}
$$`

**å…·ä½“è®¡ç®—è¿‡ç¨‹ï¼š**

* **One å¯¹ One**: `$1\times1 + 0\times2 + 2\times0 = \mathbf{1}$`
* **One å¯¹ Cat**: `$1\times(-1) + 0\times0 + 2\times2 = -1 + 4 = \mathbf{3}$`
* **Cat å¯¹ One**: `$(-1)\times1 + 2\times2 + 0\times0 = -1 + 4 = \mathbf{3}$`
* **Cat å¯¹ Cat**: `$(-1)\times(-1) + 2\times0 + 0\times2 = \mathbf{1}$`

**å¾—åˆ°åˆ†æ•°çŸ©é˜µï¼š**
`$$
\text{Scores} = \begin{bmatrix} 1 & 3 \\ 3 & 1 \end{bmatrix}
$$`

### ç¬¬å››æ­¥ï¼šç¼©æ”¾ (Scaling)

ä¸ºäº†é˜²æ­¢æ•°å€¼è¿‡å¤§ï¼Œæˆ‘ä»¬éœ€è¦é™¤ä»¥ `$\sqrt{d_k}$`ã€‚è¿™é‡Œ `$\sqrt{3} \approx 1.732$`ã€‚

`$$
\text{Scaled Scores} = \begin{bmatrix} 1/1.732 & 3/1.732 \\ 3/1.732 & 1/1.732 \end{bmatrix} \approx \begin{bmatrix} 0.58 & 1.73 \\ 1.73 & 0.58 \end{bmatrix}
$$`

### ç¬¬äº”æ­¥ï¼šå½’ä¸€åŒ– (Softmax)

æˆ‘ä»¬è¦æŠŠåˆ†æ•°å˜æˆæ¦‚ç‡ï¼Œå³å¯¹æ¯ä¸€è¡Œåš Softmaxï¼š`$\frac{e^x}{\sum e^x}$`ã€‚

**ä»¥ç¬¬ä¸€è¡Œ (one) ä¸ºä¾‹ï¼š**
* `$e^{0.58} \approx 1.79$`
* `$e^{1.73} \approx 5.64$`
* æ€»å’Œ `$= 7.43$`
* **å¯¹è‡ªå·±çš„æƒé‡**: `$1.79 / 7.43 \approx \mathbf{0.24}$`
* **å¯¹ cat çš„æƒé‡**: `$5.64 / 7.43 \approx \mathbf{0.76}$`

åŒç†è®¡ç®—ç¬¬äºŒè¡Œï¼Œæœ€ç»ˆå¾—åˆ° **Attention Matrix**ï¼š

`$$
A = \begin{bmatrix}
0.24 & 0.76 \\
0.76 & 0.24
\end{bmatrix}
$$`

> **å«ä¹‰è§£è¯»**ï¼šè®¡ç®— "one" çš„æ–°å‘é‡æ—¶ï¼Œæ¨¡å‹è®¤ä¸ºåº”è¯¥ä¿ç•™ 24% çš„åŸæ„ï¼Œå¹¶å¸æ”¶ 76% å…³äº "cat" çš„ä¿¡æ¯ã€‚

### ç¬¬å…­æ­¥ï¼šåŠ æƒæ±‚å’Œ (Multiply by V)

æœ€åï¼Œç”¨è®¡ç®—å‡ºçš„æƒé‡å»æ··åˆ `$V$` å‘é‡ã€‚

**1. è®¡ç®— one çš„æ–°å‘é‡ (`$Z_{\text{one}}$`)ï¼š**
`$$
\begin{aligned}
Z_{\text{one}} &= 0.24 \cdot v_{\text{one}} + 0.76 \cdot v_{\text{cat}} \\
&= 0.24 \cdot [2, 0, 4] + 0.76 \cdot [-2, 4, 0] \\
&= [0.48, 0, 0.96] + [-1.52, 3.04, 0] \\
&= \mathbf{[-1.04, 3.04, 0.96]}
\end{aligned}
$$`

**2. è®¡ç®— cat çš„æ–°å‘é‡ (`$Z_{\text{cat}}$`)ï¼š**
`$$
\begin{aligned}
Z_{\text{cat}} &= 0.76 \cdot v_{\text{one}} + 0.24 \cdot v_{\text{cat}} \\
&= 0.76 \cdot [2, 0, 4] + 0.24 \cdot [-2, 4, 0] \\
&= [1.52, 0, 3.04] + [-0.48, 0.96, 0] \\
&= \mathbf{[1.04, 0.96, 3.04]}
\end{aligned}
$$`

**æœ€ç»ˆç»“æœå¯¹æ¯”ï¼š**

Self-Attention å±‚çš„è¾“å‡ºçŸ©é˜µ `$Z$` ä¸ºï¼š
`$$
Z = \begin{bmatrix}
-1.04 & 3.04 & 0.96 \\
1.04 & 0.96 & 3.04
\end{bmatrix}
$$`

* **One çš„å˜åŒ–**ï¼šä» `[1, 0, 2]` å˜ä¸º `[-1.04, 3.04, 0.96]`ã€‚å®ƒå¸æ”¶äº†å¤§é‡ cat çš„ç‰¹å¾ï¼ˆç‰¹åˆ«æ˜¯ä¸­é—´ç»´åº¦å˜æˆäº† 3.04ï¼‰ã€‚
* **Cat çš„å˜åŒ–**ï¼šä» `[-1, 2, 0]` å˜ä¸º `[1.04, 0.96, 3.04]`ã€‚å®ƒä¹Ÿå¸æ”¶äº†å¤§é‡ one çš„ç‰¹å¾ï¼ˆç‰¹åˆ«æ˜¯ç¬¬ä¸‰ç»´å˜æˆäº† 3.04ï¼‰ã€‚

è¿™å°±æ˜¯ BERT è·å–ä¸Šä¸‹æ–‡èƒ½åŠ›çš„æ•°å­¦æœ¬è´¨ï¼šé€šè¿‡äº’ç›¸å…³æ³¨ï¼Œå°†åˆ«äººçš„ç‰¹å¾èå…¥è‡ªå·±çš„å‘é‡ä¸­ã€‚

## å†™ä»£ç è®¡ç®— Self Attention

ä»£ç å†™èµ·æ¥ä¼šç®€å•å¾ˆå¤šï¼Œæˆ‘ä»¬é¡ºä¾¿ä¹ŸéªŒè¯ä¸€ä¸‹ä¸Šé¢ç®—çš„å¯¹ä¸å¯¹ã€‚


```python
import numpy as np 
from typing import List, Dict 
```


```python
def softmax(X:np.ndarray):
    '''å¯¹æ¯ä¸€è¡Œåš softmax
    '''
    # p_i  = e^x_i / sum (e^x_ij). j is all items in X_i
    # åˆ†å­åˆ†æ¯åŒæ—¶ä¹˜ä»¥ e^-max(x_i)
    X = np.asarray(X)
    X_max = np.max(X, axis = -1, keepdims=True)
    exp_X = np.exp(X - X_max)
    return exp_X / np.sum(exp_X, axis = -1, keepdims=True)

def self_attention(X: np.ndarray, w_Q: np.ndarray, w_K: np.ndarray, w_V: np.ndarray):
    # 1. è®¡ç®— Q, K, V
    Q = X @ w_Q
    K = X @ w_K
    V = X @ w_V
    
    # 2. è·å–ç»´åº¦ d_k ç”¨äºç¼©æ”¾
    d_k = Q.shape[-1]
    
    # 3. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    attention_scores = Q @ K.T
    
    # 4. ç¼©æ”¾
    scaled_scores = attention_scores / np.sqrt(d_k)
    
    # 5. Softmax å½’ä¸€åŒ–å¾—åˆ°æ³¨æ„åŠ›æƒé‡çŸ©é˜µ A
    A = softmax(scaled_scores)
    
    # æ‰“å°ä¸­é—´ç»“æœç”¨äºå¯¹æ¯”æ•™ç¨‹
    print("--- éªŒè¯ä¸­é—´ç»“æœ ---")
    print(f"Scaled Scores:\n{np.round(scaled_scores, 2)}")
    print(f"\nAttention Matrix (A):\n{np.round(A, 2)}")
    
    # 6. åŠ æƒæ±‚å’Œ
    return A @ V
```


```python
X = np.array([
    [1, 0, 2], 
    [-1, 2, 0]
], dtype=float)

w_Q = np.array([
    [1, 0, 0],
    [0, 1, 0],
    [0, 0, 1]
])

w_K = np.array([
    [1, 0, 0],
    [0, 0, 1],
    [0, 1, 0]
])

w_V = np.array([
    [2, 0, 0],
    [0, 2, 0],
    [0, 0, 2]
])

print("--- è¾“å…¥ ---")
print(f"X:\n{X}")

output = self_attention(X, w_Q, w_K, w_V)

print("\n--- æœ€ç»ˆè¾“å‡º (Z) ---")
print(np.round(output, 2))
```

{{< indentedblock >}}
--- è¾“å…¥ ---
X:
[[ 1.  0.  2.]
[-1.  2.  0.]]
--- éªŒè¯ä¸­é—´ç»“æœ ---
Scaled Scores:
[[0.58 1.73]
[1.73 0.58]]

Attention Matrix (A):
[[0.24 0.76]
[0.76 0.24]]

--- æœ€ç»ˆè¾“å‡º (Z) ---
[[-1.04  3.04  0.96]
[ 1.04  0.96  3.04]]
{{< /indentedblock >}}
æˆ‘ä»¬çœ‹åˆ°ï¼Œæˆ‘ä»¬ä¸Šé¢æ‰‹ç®—çš„ç»“æœæ˜¯æ­£ç¡®çš„ã€‚

## BERT æ¨¡å‹è®­ç»ƒ

BERT æ˜¯ä¸€ç§ Masked Language Model (MLM)ã€‚ä»€ä¹ˆæ„æ€ï¼Ÿ

æˆ‘ä»¬è¯´æœ€é‡è¦çš„ï¼šæˆ‘ä»¬ä¸ºä»€ä¹ˆç”¨ BERTï¼Ÿæœ€æ ¸å¿ƒçš„ç†ç”±æ˜¯æˆ‘ä»¬æƒ³æŠŠä¸€æ®µè¯ç”šè‡³ä¸€æ•´ç¯‡æ–‡ç« è½¬åŒ–æˆä¸Šä¸‹æ–‡ç›¸å…³çš„ Embeddingã€‚æˆ–è€…è¯´ï¼Œæˆ‘ä»¬éœ€è¦çš„æ˜¯æœ€åæˆ‘ä»¬ç®—å‡ºæ¥çš„ `$Z$`ã€‚

é‚£ä¸Šé¢å‘Šè¯‰æˆ‘ä»¬äº†ï¼Œä¸ºäº†å¾—åˆ° `$Z$`ï¼Œæˆ‘ä»¬éœ€è¦ `$X$`, `$W^Q$`, `$W^K$`, `$W^V$`ã€‚ä½†è¿™äº›å¹¶ä¸æ˜¯å¹³ç™½æ— æ•…å°±å‡ºç°äº†ã€‚å¦‚ä½•å¾—åˆ°ï¼Ÿæ¨¡å‹è®­ç»ƒã€‚

æˆ‘ä»¬é¦–å…ˆå¯¹ `$W^Q$`, `$W^K$`, `$W^V$` è¿›è¡Œéšæœºåˆå§‹åŒ–ã€‚é‚£æ€ä¹ˆè®­ç»ƒå‘¢ï¼Ÿ

æˆ‘ç”¨ Claude ç”Ÿæˆäº† [100 ä¸ªç®€å•çš„è‹±æ–‡å¥å­]('./files/simple_sentences.txt)ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¾—åˆ°æ‰€æœ‰å•ç‹¬çš„è¯ï¼Œåˆ¶ä½œä¸€ä¸ª TOKEN-EMBEDDING æŸ¥æ‰¾è¡¨ï¼Œè¿™é‡Œè¦åŠ ä¸Šä¸€ä¸ª `[MASK]`ã€‚æˆ‘ä»¬æŠŠè¿™ä¸ªè¡¨å«åš `$E$`ï¼Œä¹Ÿæ˜¯éšæœºåˆå§‹åŒ–ã€‚

`$$E \in \mathbb{R}^{m \times d}$$`

å…¶ä¸­ `$m$` æ˜¯è¯è¡¨å¤§å°ï¼Œ`$d$` æ˜¯ embedding çš„ç»´åº¦ã€‚

### è®­ç»ƒæµç¨‹

æˆ‘ä»¬çœ‹ç¬¬ä¸€å¥è¯ï¼š`the cat sits on the mat`ã€‚æˆ‘ä»¬æŠŠ `sits` é®ä½ï¼Œå˜æˆ `the cat [MASK] on the mat`ï¼Œç”¨æ¥è®­ç»ƒã€‚

è¿™å¥è¯ç»è¿‡ Self-Attention è®¡ç®—åï¼Œå¾—åˆ° `$Z$`ã€‚æˆ‘ä»¬é‡ç‚¹çœ‹ `$Z_{\text{mask}}$`ï¼Œå®ƒåŒ…å«äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ï¼Œè®©æ¨¡å‹æ¥é¢„æµ‹è¿™ä¸ªè¢«é®ä½çš„è¯æ˜¯ä»€ä¹ˆã€‚å‡†ç¡®æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—è¯è¡¨ä¸­æ¯ä¸€ä¸ªè¯æ˜¯è¿™ä¸ªè¢«é®ä½è¯çš„æ¦‚ç‡ï¼Œç„¶åæ‰¾åˆ° `sits` çš„æ¦‚ç‡ï¼Œè®¡ç®— lossï¼Œæœ€ååå‘ä¼ æ’­ã€‚

### ä» `$Z_{\text{mask}}$` åˆ°é¢„æµ‹

åœ¨çœŸå®çš„ BERT è®­ç»ƒä¸­ï¼Œç›´æ¥å¾—åˆ°çš„ `$Z_{\text{mask}}$` å¹¶ä¸ä¼šç›´æ¥ç”¨æ¥åšé¢„æµ‹ï¼Œè€Œæ˜¯è¦ç»è¿‡ä¸€äº›å¤„ç†ã€‚è¿™ä¹ˆåšçš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬æœ‰æ›´å¤šçš„å‚æ•°å¯ä»¥è°ƒèŠ‚ï¼Œè¿™æ ·å¯ä»¥è®©ç»“æœæ›´å‡†ç¡®ã€‚

#### ç¬¬ä¸€æ­¥ï¼šç‰¹å¾æ•´ç†ï¼ˆRefinementï¼‰

å¾—åˆ° `$Z_{\text{mask}}$` ä¹‹åï¼Œæˆ‘ä»¬å…ˆåšä¸€ç‚¹ç‰¹å¾æ•´ç†ï¼š

`$$Z_{\text{refined}} = \text{Activation}(Z_{\text{mask}} \cdot W_{\text{dense}} + b_{\text{dense}})$$`

å…¶ä¸­ï¼š
- `$Z_{\text{mask}} \in \mathbb{R}^{1 \times d}$`ï¼ˆ[MASK] ä½ç½®çš„è¾“å‡ºå‘é‡ï¼‰
- `$W_{\text{dense}} \in \mathbb{R}^{d \times d}$`ï¼ˆå…¨è¿æ¥å±‚æƒé‡ï¼‰
- `$b_{\text{dense}} \in \mathbb{R}^{1 \times d}$`ï¼ˆå…¨è¿æ¥å±‚åç½®ï¼‰
- Activation é€šå¸¸æ˜¯ GELU æˆ– ReLU

æœ€åçš„ç»“æœ `$Z_{\text{refined}} \in \mathbb{R}^{1 \times d}$`ã€‚

#### ç¬¬äºŒæ­¥ï¼šè®¡ç®— Logits

ç°åœ¨æˆ‘ä»¬è¦è€ƒå¯Ÿï¼š`$Z_{\text{refined}}$` åˆ°åº•åƒè°ï¼Ÿ

æˆ‘ä»¬æŠŠ `$Z_{\text{refined}}$` æ‹¿å»å’Œå­—å…¸ `$E$` é‡Œçš„æ¯ä¸€ä¸ªè¯æ¯”å¯¹ï¼š

`$$\text{Logits} = Z_{\text{refined}} \cdot E^T + b_{\text{vocab}}$$`

å…¶ä¸­ï¼š
- `$E \in \mathbb{R}^{m \times d}$`ï¼ˆEmbedding Matrixï¼Œå°±æ˜¯è¯è¡¨ï¼‰
- `$E^T \in \mathbb{R}^{d \times m}$`ï¼ˆE çš„è½¬ç½®ï¼‰
- `$b_{\text{vocab}} \in \mathbb{R}^{1 \times m}$`ï¼ˆè¯è¡¨åç½®ï¼‰
- `$\text{Logits} \in \mathbb{R}^{1 \times m}$`ï¼ˆæ¯ä¸ªè¯çš„åŸå§‹åˆ†æ•°ï¼‰

ä¸ºä»€ä¹ˆè¦ä¹˜ä»¥ `$E^T$`ï¼Ÿ

`$E$` æ˜¯é‚£æœ¬å­—å…¸ï¼ˆæ‰€æœ‰è¯çš„å‘é‡ï¼‰ã€‚`$Z_{\text{refined}}$` æ˜¯æ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡"çŒœ"å‡ºæ¥çš„å‘é‡ã€‚

`$Z_{\text{refined}} \cdot E^T$` æœ¬è´¨ä¸Šæ˜¯åœ¨åšç‚¹ç§¯ï¼ˆDot Productï¼‰ï¼Œä¹Ÿå°±æ˜¯è®¡ç®—ç›¸ä¼¼åº¦ã€‚æ¨¡å‹åœ¨é—®ï¼š"æˆ‘çŒœå‡ºæ¥çš„è¿™ä¸ªå‘é‡ï¼Œè·Ÿå­—å…¸é‡Œçš„ `cat` åƒä¸åƒï¼Ÿè·Ÿ `sits` åƒä¸åƒï¼Ÿè·Ÿ `mat` åƒä¸åƒï¼Ÿ"

`$b_{\text{vocab}}$` åœ¨è¿™é‡Œæœ‰ç”¨æ˜¯å› ä¸ºæœ‰çš„è¯æœ¬èº«å°±å¾ˆå¸¸è§ï¼Œæ¯”å¦‚ an, the, a, for ç­‰ã€‚å®ƒèµ·åˆ°çš„ä½œç”¨å°±åƒä¸€ä¸ªè´å¶æ–¯ç»Ÿè®¡é‡Œçš„å…ˆéªŒæ¦‚ç‡ã€‚

#### ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—æ¦‚ç‡åˆ†å¸ƒ

å¯¹ logits åš softmaxï¼Œå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒï¼š

`$$P = \text{softmax}(\text{Logits}) = \frac{e^{\text{Logits}_i}}{\sum_{j=1}^{m} e^{\text{Logits}_j}}$$`

ç°åœ¨ `$P$` ä¸­çš„æ¯ä¸ªå€¼è¡¨ç¤ºå¯¹åº”è¯æ˜¯è¢«é®ä½è¯çš„æ¦‚ç‡ï¼Œæ‰€æœ‰æ¦‚ç‡åŠ èµ·æ¥ç­‰äº 1ã€‚

#### ç¬¬å››æ­¥ï¼šè®¡ç®— Loss

ä½¿ç”¨äº¤å‰ç†µæŸå¤±ã€‚å‡è®¾æ­£ç¡®ç­”æ¡ˆ `sits` åœ¨è¯è¡¨ä¸­çš„ç´¢å¼•æ˜¯ `$k$`ï¼š

`$$\text{Loss} = -\log(P_k)$$`

å°±æ˜¯è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œåªçœ‹æ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ã€‚

### åå‘ä¼ æ’­ï¼šæ›´æ–°æ‰€æœ‰å‚æ•°

è®¡ç®—å®Œ Loss åï¼Œæˆ‘ä»¬é€šè¿‡åå‘ä¼ æ’­æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚éœ€è¦æ›´æ–°çš„å‚æ•°æœ‰ï¼š

| å‚æ•° | å½¢çŠ¶ | ä½œç”¨ |
|------|------|------|
| `$E$` | `$m \times d$` | Embedding Matrixï¼Œè¯å‘é‡æŸ¥æ‰¾è¡¨ |
| `$W^Q$` | `$d \times d$` | è®¡ç®— Query |
| `$W^K$` | `$d \times d$` | è®¡ç®— Key |
| `$W^V$` | `$d \times d$` | è®¡ç®— Value |
| `$W_{\text{dense}}$` | `$d \times d$` | ç‰¹å¾æ•´ç†å±‚æƒé‡ |
| `$b_{\text{dense}}$` | `$1 \times d$` | ç‰¹å¾æ•´ç†å±‚åç½® |
| `$b_{\text{vocab}}$` | `$1 \times m$` | è¯è¡¨åç½® |

æ³¨æ„ï¼šè®¡ç®— Logits æ—¶ç”¨çš„ `$E^T$` å°±æ˜¯ Embedding Matrix çš„è½¬ç½®ï¼Œä¸æ˜¯é¢å¤–çš„å‚æ•°ã€‚è¿™å«åš **weight tying**ï¼ˆæƒé‡ç»‘å®šï¼‰ï¼Œè¾“å…¥å’Œè¾“å‡ºå…±äº«åŒä¸€å¥—è¯å‘é‡ã€‚

æ›´æ–°è§„åˆ™ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰ï¼š

`$$\theta \leftarrow \theta - \eta \cdot \frac{\partial \text{Loss}}{\partial \theta}$$`

å…¶ä¸­ `$\theta$` ä»£è¡¨ä»»æ„å‚æ•°ï¼Œ`$\eta$` æ˜¯å­¦ä¹ ç‡ã€‚

### è®­ç»ƒå¾ªç¯

é‡å¤ä»¥ä¸‹æ­¥éª¤ï¼Œç›´åˆ° Loss æ”¶æ•›ï¼š
1. éšæœºé€‰ä¸€ä¸ªå¥å­
2. éšæœºé®ä½ä¸€ä¸ªè¯ï¼Œè®°å½•æ­£ç¡®ç­”æ¡ˆ
3. å‰å‘ä¼ æ’­ï¼š
{{< indentedblock >}}
- æŸ¥ Embedding Matrix å¾—åˆ° X
- è®¡ç®— Self-Attention å¾—åˆ° Z
- å– Z_maskï¼Œåšç‰¹å¾æ•´ç†å¾—åˆ° Z_refined
- å’Œ E^T åšç‚¹ç§¯å¾—åˆ° Logits
- Softmax å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
{{< /indentedblock >}}
4. è®¡ç®— Loss
5. åå‘ä¼ æ’­ï¼šè®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦
6. æ›´æ–°å‚æ•°

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹å­¦ä¼šäº†æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹è¢«é®ä½çš„è¯ï¼ŒåŒæ—¶ Embedding Matrix å’Œ Self-Attention å‚æ•°éƒ½å˜å¾—æœ‰æ„ä¹‰äº†ã€‚

## è®­ç»ƒä»£ç 

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å§‹å®æˆ˜å†™ä»£ç ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå› ä¸ºæ¢¯åº¦è®¡ç®—éœ€è¦ç”¨åˆ°çŸ©é˜µæ±‚å¯¼ï¼Œæˆ‘ä»¬æš‚ä¸”ä¸å»ç®¡æ•°å­¦ï¼Œè€Œæ˜¯ç”¨ PyTorch è‡ªåŠ¨æ±‚å¯¼ã€‚ä¹‹ååœ¨è¿›é˜¶ç‰ˆï¼Œæˆ‘ä»¬ä¼šå†™æ¸…æ¥šæ¯ä¸€æ­¥çš„æ¢¯åº¦ã€‚


```python
import torch 
import re 
import torch.nn as nn

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)
```


```python
def load_data(filename:str):
    with open(filename, 'r', encoding='utf-8') as f:
        text = f.read().lower() # å…¨éƒ¨è½¬å°å†™
    
    # å»æ ‡ç‚¹ç¬¦å·ï¼Œåªä¿ç•™å•è¯
    text = re.sub(r'[^\w\s]', '', text) 
    sentences = text.strip().split('\n')

    # å»ºç«‹è¯è¡¨
    words = text.split()
    unique_words = sorted(list(set(words)))
    unique_words += ['[MASK]'] 

    word2id = {w:i for i, w in enumerate(unique_words)}
    id2word = {i:w for i, w in enumerate(unique_words)}

    return sentences, word2id, id2word 

sentences, word2id, id2word = load_data('data/simple_sentences.txt')
vocab_size = len(word2id)

print(f"è¯è¡¨å¤§å°: {vocab_size}")
print(f"æ‰€æœ‰çš„è¯: {list(word2id.keys())}")
```

{{< indentedblock >}}
è¯è¡¨å¤§å°: 89
æ‰€æœ‰çš„è¯: ['a', 'an', 'apple', 'at', 'ball', 'barks', 'bed', 'bench', 'bird', 'book', 'boy', 'branch', 'bread', 'by', 'cat', 'catches', 'chair', 'chases', 'child', 'clouds', 'coffee', 'digs', 'dog', 'doll', 'door', 'drinks', 'east', 'eats', 'fire', 'fish', 'flies', 'float', 'from', 'garden', 'girl', 'grass', 'hide', 'hides', 'hill', 'house', 'in', 'juice', 'jumps', 'man', 'mat', 'meat', 'milk', 'moon', 'mouse', 'night', 'on', 'over', 'paper', 'park', 'plays', 'reads', 'rises', 'river', 'rock', 'runs', 'school', 'sea', 'seed', 'sets', 'shine', 'shines', 'sings', 'sits', 'sky', 'sleeps', 'stars', 'store', 'sun', 'swims', 'table', 'tea', 'the', 'to', 'tree', 'under', 'walks', 'wall', 'watches', 'water', 'west', 'window', 'with', 'woman', '[MASK]']
{{< /indentedblock >}}
```python
class MiniBert(nn.Module):
    def __init__(self, vocab_size:int, d_model:int):
        super().__init__()
        # æŠŠåˆå§‹åŒ–çš„æ–¹å·®è°ƒå°ï¼Œä¹˜ä»¥ 0.01 æˆ–è€… 0.1
        scale = 0.01

        # 1. Token Embedding è¡¨
        self.E = nn.Parameter(
            torch.randn(vocab_size, d_model) * scale
        )
        # è¿™é‡Œéœ€è¦å¼ºè°ƒä¸€ä¸‹ï¼ŒE å…¶å®æ˜¯æœ‰é¡ºåºçš„ï¼Œå°±æ˜¯ word2id é‡Œçš„é¡ºåº
        # å› ä¸ºä¹‹åæˆ‘ä»¬ç”¨åˆ° input_ids = [word2id[w] for w in words]

        # 2. Attention æƒé‡
        self.W_Q = nn.Parameter(
            torch.randn(d_model, d_model) * scale
        )
        self.W_K = nn.Parameter(
            torch.randn(d_model, d_model) * scale
        )
        self.W_V = nn.Parameter(
            torch.randn(d_model, d_model) * scale
        )

        # 3. MLM Head ä¸­é—´æ˜ å°„
        self.W_dense = nn.Parameter(
            torch.randn(d_model, d_model) * scale
        )
        self.b_dense = nn.Parameter(
            torch.zeros(d_model)
        )

        # 4. vocab biasï¼ˆembedding æƒé‡å…±äº«ï¼‰
        self.b_vocab = nn.Parameter(
            torch.zeros(vocab_size)
        )

        self.dropout = nn.Dropout(p=0.1) # BERT æ ‡å‡†æ˜¯ 0.1 (10%)
        self.layer_norm = nn.LayerNorm(d_model)
    
    def bert_forward(self, input_ids):
        X = self.E[input_ids] # Shape: [seq_len, d_model]

        # 2. è®¡ç®— Q, K, V
        Q = X @ self.W_Q # Shape: [seq_len, d_model]
        K = X @ self.W_K # Shape: [seq_len, d_model]
        V = X @ self.W_V # Shape: [seq_len, d_model]

        # 3. Self Attention è®¡ç®—
        d_k = Q.shape[-1]
        # transpose(-2, -1) å€’æ•°ç¬¬2ä¸ªç»´åº¦ å’Œ å€’æ•°ç¬¬1ä¸ªç»´åº¦ äº’æ¢
        # è¿™æ˜¯ä¸ºäº†åº”å¯¹ä¹‹å torch çš„ batchã€‚ä½†è¿™é‡Œçš„è¯ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ Q @ K.T
        scores = Q @ K.transpose(-2, -1)
        # scores = Q @ K.T 
        # Shape: [seq_len, seq_len]
        scaled_scores = scores/torch.sqrt(torch.tensor(d_k)) 
        A = torch.softmax(scaled_scores, dim = -1) # Shape: [seq_len, seq_len]

        Z = A @ V # (seq_len, d_model)

        # å¯¹ Attention çš„ç»“æœåšä¸€æ¬¡ Dropout
        Z = self.dropout(Z) # 

        # æ®‹å·®è¿æ¥ + LayerNorm
        # å…¬å¼çš„æœ¬è´¨ï¼šOutput = Norm(Input + Attention(Input))
        # è¿™è®©æ¢¯åº¦æœ‰äº†â€œé«˜é€Ÿå…¬è·¯â€ï¼Œå¯ä»¥ç›´æ¥æµå› Input
        Z = self.layer_norm(Z + X) # (seq_len, d_model)

        # 4. Refinement (Dense Layer + Activation)
        # Z_refined = GELU(Z * W + b)
        # Shape: (seq_len, d_model)
        Z_refined = torch.nn.functional.gelu(Z @ self.W_dense + self.b_dense)
        
        # 5. Output Logits (Weight Tying)
        # å¤ç”¨ E çš„è½¬ç½®è¿›è¡Œé¢„æµ‹
        # å› ä¸º E æ˜¯ word2id çš„é¡ºåºï¼Œæ‰€ä»¥ logits ä¹Ÿæ˜¯ word2id çš„é¡ºåº
        logits = Z_refined @ self.E.T + self.b_vocab # Shape: (seq_len, vocab_size)
        
        return logits, A
```


```python
def train(vocab_size:int, d_model:int, learning_rate:float, epoches:int):
    model = MiniBert(vocab_size=vocab_size, d_model=d_model)
    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

    for epoch in range(epoches):
        total_loss = 0 
        for sent in sentences:
            words = sent.split()
            if len(words) < 2: continue # å°‘äºä¸¤ä¸ªè¯ï¼Œæˆ‘ä»¬ç›´æ¥ç•¥è¿‡

            # 1. æŠŠæ–‡å­—è½¬æˆ ID
            input_ids = [word2id[w] for w in words]
            
            # 2. éšæœºé€‰ä¸€ä¸ªä½ç½®é®ä½
            mask_pos = np.random.randint(0, len(words))
            target_word_id = input_ids[mask_pos] # æ­£ç¡®ç­”æ¡ˆ
            
            # 3. æŠŠè¾“å…¥é‡Œçš„è¿™ä¸ªä½ç½®æ›¿æ¢æˆ [MASK]
            input_ids[mask_pos] = word2id['[MASK]']
            
            # è½¬æˆ Tensor å–‚ç»™æ¨¡å‹
            input_tensor = torch.tensor(input_ids) 

            # --- å‰å‘ä¼ æ’­ ---
            optimizer.zero_grad() # æ¸…ç©ºä¸Šä¸€æ­¥çš„æ¢¯åº¦
            
            logits, _ = model.bert_forward(input_tensor)

            # ==========================================
            #   è®¡ç®— Loss å¼€å§‹
            # ==========================================

            # 1. é”å®šä½ç½®ï¼šæˆ‘ä»¬åªå…³å¿ƒ [MASK] è¿™ä¸ªä½ç½®çš„é¢„æµ‹ç»“æœ
            # mask_logits æ˜¯ä¸€ä¸ªé•¿é•¿çš„å‘é‡ï¼Œä»£è¡¨æ¯ä¸ªè¯çš„åˆ†æ•°
            mask_logits = logits[mask_pos] 
            
            # 2. æ‰‹åŠ¨ Softmaxï¼šæŠŠåˆ†æ•°å˜æˆæ¦‚ç‡ (0.0 ~ 1.0)
            # dim=0 è¡¨ç¤ºå¯¹è¿™ä¸€ä¸ªå‘é‡å†…éƒ¨åšå½’ä¸€åŒ–
            probs = torch.softmax(mask_logits, dim=0)
            
            # 3. é”å®šç­”æ¡ˆï¼šæŠŠâ€œæ­£ç¡®ç­”æ¡ˆâ€å¯¹åº”çš„é‚£ä¸ªæ¦‚ç‡æŠ å‡ºæ¥
            # æ¯”å¦‚ target_word_id æ˜¯ 5ï¼Œæˆ‘ä»¬å°±çœ‹ç¬¬ 5 ä¸ªæ¦‚ç‡æ˜¯å¤šå°‘
            target_prob = probs[target_word_id]
            
            # 4. è®¡ç®— Lossï¼š-log(æ¦‚ç‡)
            # cross entropy loss: - sum (y_i * log(p_i))
            # æ¦‚ç‡è¶Šæ¥è¿‘ 1ï¼Œlog åè¶Šæ¥è¿‘ 0ï¼Œloss è¶Šå°
            # åŠ ä¸€ä¸ªæå°å€¼ 1e-9 æ˜¯ä¸ºäº†é˜²æ­¢ target_prob ä¸º 0 æ—¶ log æŠ¥é”™
            loss = -torch.log(target_prob + 1e-9)

            # ==========================================
            #   è®¡ç®— Loss ç»“æŸ
            # ==========================================

            # --- åå‘ä¼ æ’­ & æ›´æ–°å‚æ•° ---
            loss.backward()  # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨ç®—å‡º W_Q, E ç­‰æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦
            optimizer.step() # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨æ‰§è¡Œ param -= lr * grad
            
            total_loss += loss.item()

        if epoch % 50 == 0:
            print(f"Epoch {epoch}, Loss: {total_loss/len(sentences):.4f}")
        
    print("è®­ç»ƒå®Œæˆï¼")
    return model 
```


```python
trained_model = train(
    vocab_size=vocab_size, 
    d_model=128, 
    learning_rate=0.001, 
    epoches=500
)
```

{{< indentedblock >}}
Epoch 0, Loss: 3.8556
Epoch 50, Loss: 1.2090
Epoch 100, Loss: 0.6556
Epoch 150, Loss: 0.2926
Epoch 200, Loss: 0.2074
Epoch 250, Loss: 0.1870
Epoch 300, Loss: 0.1895
Epoch 350, Loss: 0.2070
Epoch 400, Loss: 0.0788
Epoch 450, Loss: 0.1339
è®­ç»ƒå®Œæˆï¼
{{< /indentedblock >}}
è¿™é‡Œæœ‰ä¸å°‘å€¼å¾—è¯´çš„ç‚¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸Šé¢åªæ˜¯ç”¨åˆ°äº† `$Z_\text{mask}$` ä½†æ˜¯æˆ‘ä»¬åœ¨ä¸Šé¢çš„è®­ç»ƒä»£ç ä¸­ï¼Œå´æŠŠå…¨éƒ¨çš„ `$Z$` éƒ½ç”¨èµ·æ¥äº†ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ

- GPU è®¡ç®—çš„æ˜¯å¹¶è¡Œè®¡ç®—ï¼Œæ‰€ä»¥åªç®— `$Z_\text{mask}$` å’Œå…¨éƒ¨çš„ `$Z$` éƒ½ç®—ï¼Œåœ¨è®¡ç®—èµ„æºçš„ä½¿ç”¨é‡ä¸Šå¹¶æ²¡æœ‰æ˜¾è‘—å·®å¼‚ã€‚
- åœ¨çœŸå®çš„ BERT è®­ç»ƒä¸­ï¼Œå¹¶ä¸åªæ˜¯éšæœºé®ä½ä¸€ä¸ªå­—ï¼Œè€Œæ˜¯å¤šä¸ªå­—ã€‚
- åœ¨çœŸå®çš„ BERT è®­ç»ƒä¸­ï¼ŒSelf Attention æœ‰å¾ˆå¤šå±‚ï¼Œæ¯”å¦‚ 12 å±‚ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€å±‚ã€‚é‚£ç¬¬äºŒå±‚çš„è¾“å…¥æ˜¯ `$Z$`ï¼Œè€Œä¸æ˜¯ `$Z_\text{mask}$`ã€‚
å¦å¤–ï¼Œæˆ‘ä»¬ç”¨åˆ°äº† dropoutï¼š`Z = self.dropout(Z)`ã€‚è¿™ä¸ªå…¶å®å¾ˆå¥½ç†è§£ã€‚é¦–å…ˆï¼Œ`$Z$` çš„ç»´åº¦æ˜¯ `$m \times d$`ã€‚Dropout æ˜¯è¯´ï¼Œæ¯ä¸€ä¸ªå…ƒç´ ï¼Œæˆ–è€…è¯´ï¼Œæ¯ä¸€è¡Œï¼ˆå³ï¼Œæ¯ä¸€ä¸ªå­—ï¼‰çš„æ¯ä¸€ä¸ªæ•°å­—ï¼Œéƒ½æœ‰ç™¾åˆ†ä¹‹åï¼ˆè¿™æ˜¯ä¸€ä¸ªå‚æ•°ï¼Œå¯ä»¥è°ƒèŠ‚ï¼‰çš„æ¦‚ç‡å˜ä¸º 0ã€‚ç„¶åå‘¢ï¼Œæ¯ä¸€è¡Œï¼Œæˆ‘ä»¬è¦ç¡®ä¿ dropout å‰åçš„æœŸæœ›å€¼ä¸å˜ï¼Œé‚£å°±éœ€è¦æŠŠåˆ«çš„æ•°å­—æ”¹å˜ä¸€ä¸‹ã€‚å…¶å®å°±æ˜¯æ¯ä¸ªæ•°å­—é™¤ä»¥ `$0.9$` å°±å¯ä»¥ã€‚è¿™å°±æ˜¯æ‰€è°“çš„ inverted dropout. è¿™ä¹ˆåšæœ‰ä»€ä¹ˆç”¨ï¼Ÿå®ƒå¯ä»¥é€¼è¿«æ¨¡å‹åœ¨ç¼ºå°‘æ•°æ®çš„æƒ…å†µä¸‹æ‹¼å°½å…¨åŠ›å»åšå¥½é¢„æµ‹ã€‚

æˆ‘ä»¬è¿˜ç”¨åˆ°äº† `Z = self.layer_norm(Z + X)`ã€‚å®ƒçš„æœ¬è´¨æ˜¯ Output = Norm(Input + Attention(Input)). è¿™æ˜¯ä½•å‡¯æ˜å¤§ç¥é‚£ç¯‡è‘—åçš„ ResNet è®ºæ–‡çš„çš„ç²¾é«“ã€‚å®ƒå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„æ‹Ÿåˆã€‚

è¿˜æœ‰å°±æ˜¯æˆ‘ä»¬çœ‹åˆ° Loss å¤§è‡´æ˜¯åœ¨ä¸æ–­ä¸‹é™çš„ï¼Œä½†æ˜¯å¶å°”ä¼šæœ‰èµ·ä¼ã€‚è¿™å¾ˆæ­£å¸¸ï¼Œå› ä¸ºé®çš„è¯æ˜¯éšæœºçš„ï¼Œæœ‰æ—¶å€™å°±æ¯”è¾ƒéš¾ã€‚

æˆ‘ä»¬ç°åœ¨æ¥ç”¨å‡ ä¸ªå¥å­æ¥æ£€æµ‹ä¸€ä¸‹ï¼š

- The man eats the MASK
- The man read the MASK
- The man MASK the book

æ³¨æ„ï¼Œè¿™ä¸¤å¥è¯å¹¶æ²¡æœ‰å‡ºç°åœ¨è®­ç»ƒé›†é‡Œï¼Œä½†æ˜¯æ‰€æœ‰çš„è¯éƒ½æ˜¯è®­ç»ƒæ—¶è§è¿‡çš„ã€‚æˆ‘ä»¬çœ‹çœ‹æ¨¡å‹æ˜¯å¦å¯ä»¥å‡†ç¡®é¢„æµ‹å‡ºæ­£ç¡®çš„è¯ã€‚


```python
def predict(model:MiniBert, sentence:str, word2id:Dict, id2word:Dict):
    # --- 1. å‡†å¤‡å·¥ä½œ ---
    model.eval() # å…³é—­ Dropout

    words = sentence.split()
    try:
        mask_idx = words.index('[MASK]')
    except ValueError:
        print(f"å‡ºé”™: å¥å­ '{sentence}' é‡Œæ²¡æœ‰æ‰¾åˆ° [MASK]")
        return
    
    input_ids = [word2id.get(w,0) for w in words]
    input_tensor = torch.tensor([input_ids]) # è½¬æˆ Tensor å¹¶å¢åŠ  Batch ç»´åº¦: [seq_len] -> [1, seq_len]

    # --- 2. æ¨¡å‹æ¨ç† ---
    with torch.no_grad(): # é¢„æµ‹æ¨¡å¼ä¸éœ€è¦ç®—æ¢¯åº¦ï¼Œçœå†…å­˜
        logits, _ = model.bert_forward(input_tensor)
    
    # --- 3. æå–ç»“æœ ---
    # logits å½¢çŠ¶: [batch=1, seq_len, vocab_size]
    # æˆ‘ä»¬åªå…³å¿ƒ [MASK] é‚£ä¸ªä½ç½®çš„é¢„æµ‹ç»“æœ
    # å› ä¸º E æ˜¯ word2id çš„é¡ºåºï¼Œæ‰€ä»¥ logits ä¹Ÿæ˜¯ word2id çš„é¡ºåº
    mask_logits = logits[0, mask_idx, :] # Shape: (1, 1, vocab_size)

    probs = torch.softmax(mask_logits, dim = -1)

    # å–å‡ºæ¦‚ç‡æœ€å¤§çš„å‰ 5 ä¸ªè¯
    top_k = torch.topk(probs, k=5)
    top_ids = top_k.indices.tolist()
    top_values = top_k.values.tolist()

    # --- 4. æ‰“å°ç»“æœ ---
    print(f"\nğŸ“ è¾“å…¥å¥å­: {sentence}")
    print(f"ğŸ“ [MASK] ä½ç½®: ç¬¬ {mask_idx} ä¸ªè¯")
    print("ğŸ¤– æ¨¡å‹é¢„æµ‹ç»“æœ:")
    print("-" * 30)
    for i in range(len(top_ids)):
        word = id2word[top_ids[i]]
        score = top_values[i]
        print(f"  {i+1}. {word:<10} (ä¿¡å¿ƒ: {score:.2%})")
    print("-" * 30)
```


```python
# æµ‹è¯•ç”¨ä¾‹ 1: è€ƒå¯Ÿå®¾è¯­ (Object)
# é¢„æœŸï¼šeats åé¢åº”è¯¥æ˜¯é£Ÿç‰© (apple, fish, meat)
predict(trained_model, "the man eats the [MASK]", word2id, id2word)
```

{{< indentedblock >}}
ğŸ“ è¾“å…¥å¥å­: the man eats the [MASK]
ğŸ“ [MASK] ä½ç½®: ç¬¬ 4 ä¸ªè¯
ğŸ¤– æ¨¡å‹é¢„æµ‹ç»“æœ:
------------------------------
1. meat       (ä¿¡å¿ƒ: 99.51%)
2. ball       (ä¿¡å¿ƒ: 0.31%)
3. woman      (ä¿¡å¿ƒ: 0.06%)
4. bread      (ä¿¡å¿ƒ: 0.05%)
5. fish       (ä¿¡å¿ƒ: 0.02%)
------------------------------
{{< /indentedblock >}}
```python
# æµ‹è¯•ç”¨ä¾‹ 2: 
predict(trained_model, "the man read the [MASK]", word2id, id2word)
```

{{< indentedblock >}}
ğŸ“ è¾“å…¥å¥å­: the man read the [MASK]
ğŸ“ [MASK] ä½ç½®: ç¬¬ 4 ä¸ªè¯
ğŸ¤– æ¨¡å‹é¢„æµ‹ç»“æœ:
------------------------------
1. book       (ä¿¡å¿ƒ: 47.85%)
2. meat       (ä¿¡å¿ƒ: 20.08%)
3. house      (ä¿¡å¿ƒ: 11.64%)
4. store      (ä¿¡å¿ƒ: 7.70%)
5. bread      (ä¿¡å¿ƒ: 3.66%)
------------------------------
{{< /indentedblock >}}
```python
# æµ‹è¯•ç”¨ä¾‹ 3: è€ƒå¯Ÿè°“è¯­ (Verb) / è¯­æ³•
# é¢„æœŸï¼šman å’Œ book ä¸­é—´åº”è¯¥æ˜¯åŠ¨ä½œ (reads, likes, buys, sees)
predict(trained_model, "the man [MASK] the book", word2id, id2word)
```

{{< indentedblock >}}
ğŸ“ è¾“å…¥å¥å­: the man [MASK] the book
ğŸ“ [MASK] ä½ç½®: ç¬¬ 2 ä¸ªè¯
ğŸ¤– æ¨¡å‹é¢„æµ‹ç»“æœ:
------------------------------
1. man        (ä¿¡å¿ƒ: 71.53%)
2. plays      (ä¿¡å¿ƒ: 22.49%)
3. woman      (ä¿¡å¿ƒ: 5.23%)
4. over       (ä¿¡å¿ƒ: 0.59%)
5. cat        (ä¿¡å¿ƒ: 0.08%)
------------------------------
{{< /indentedblock >}}
æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå‰ä¸¤ä¸ªè¿˜è¡Œï¼Œç¬¬ä¸‰ä¸ªå°±å¾ˆä¸€èˆ¬äº†ã€‚
