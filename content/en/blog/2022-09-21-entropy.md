---
title: "What on Earth is Entropy?"
date: 2022-09-21T15:33:20-05:00
author: "Hongtao Hao"
slug: entropy
draft: false
toc: false
tags: ML
---
<!-- ## Understanding Bits, Bit by Bit

To understand entropy, we need to first understand bit. 

Let's play a game. Say we have four types of race cars:

  - Ferrari
  - Mclaren
  - Mercedes-Benz
  - Honda

You are facing the four cars. A friend is standing right next to you back to back. 

This is the rule of the game: Both you and your friend know there are four cars of different makes and both of you know these four makes. I will pick one car from the four, and you are asked to write down the type of the car on a piece of paper and hand it to your friend. Each letter or number you write will take up one square. **To win the game, you need to use (1) as few symbols as possible and (2) as few squares as possible**. We consider (1) first. If there are two teams and Team A uses more symbols than Team B, then no matter what's the difference between the two teams in the number of squares, Team B wins.  

Clearly, it's insufficient to write down the full name of a car make because it will (1) use many symbols and (2) take up many squares. To take up fewer squares, you might agree with your friend with this "map":

  - Ferrari: A
  - Mclaren: B
  - Mercedes-Benz: C
  - Honda: D

Whenever you write down 'A', your friend knows it's a Ferrari. However, this won't guarantee that you will win: you are using four symbols but other teams might use fewer symbols. 

The question now is, how many symbols do we need to communicate four names? Or any number of names? Clearly, one symbol is not enough as it can only represent one entity. Two symbols? Bingo! 

**Two symbols can represent as many entities as you want!**

You can use any two symbols, say, 'A'-'B', '&'-'%', '^'-")", etc. But the convention is to use 0 and 1. But how can we represent the above four car makes with only 0 and 1?

  - Ferrari: 00
  - Mclaren: 01
  - Mercedes-Benz: 10
  - Honda: 11

Is this the most efficient way? No. We can do this:

  - Ferrari: 0
  - Mclaren: 1
  - Mercedes-Benz: 00
  - Honda: 01

We define these two patterns as one bit: 
  - 0
  - 1

https://www.youtube.com/watch?v=X40ft1Lt1f0 -->

Imagine you have a 4-sided fair die, with numbers of '1', '2', '3', and '4'. You roll the die and ask your friend, who have his eyes closed, to figure out the outcome. Your friend can ask you any questions, for example, 'Is the outcome 1?' or 'Is it below 3?'. However, you can only answer with 'Yes' or 'No'. 

Here is the challenge: which strategy can guarantee that, on average, least questions are required to figure out the outcome?

Let's take one strategy as an example: Ask 'Is the outcome 1?', 'Is the outcome 2?', 'Is the outcome 3?', and 'Is the outcome 4?' in a row. 

Let's say you roll the die 100 times. Since it's a fair die, you would expect each number to occur 25 times. When the outcome is '1', your friend asks only 1 question. When it is '2', 2 questions. '3' -> 3 questions and '4' -> 4 questions. In total, your friend will have to ask `$25 \times (1 + 2 + 3 + 4) = 250$` questions. Therefore, on average, your friend asks `$\frac{250}{100} = 2.5$` questions. 

You can explore other strategies, but it turns out the optimal strategy is one that, if possible, keep asking questions that half the probability of the outcome space. In our case, we can ask, for example,'Is it below 3?'. If yes, we can ask 'Is it 2?'. If no, we can ask 'is it 3?'. Using this strategy, we make sure that for every outcome, two questions suffice. 

Let's take a look at how many questions on average your friend will ask if he employs this strategy. Again, we roll the die 100 times and each number occurs 25 times. No matter what the outcome is, your friend will always ask two questions, so in total, `$100 \times 2 = 200$` questions. On average, 2 questions, which is lower than 2.5 questions. 

This, in fact, is the definition of "Bit" and "Entropy". A question will decrease the level of uncertainty your friend faces. When he employs the optimal strategy, we say that each question decrease his uncertainty by one "bit" and that the entropy of rolling a fair 4-sided die is 2 bits (because on average, it requires at least 2 question to figure out what the outcome is).

## Further

What's the entropy of rolling an eight-sided fair die? We know that using the optimal strategy, no matter which outcome occurs, your friend only asks 3 questions. So, if you roll 120 times, on average, your friend asks

`$$\frac{120 \times 3}{120}$$`

3 questions. So the entropy is 3 bits. 










