---

title: Understanding K-Nearest Neighbors Using MNIST Dataset
date: 2022-09-26
author: Hongtao Hao
slug: knn
draft: false
toc: true
tags: ML

---

To me, K Nearest Neighbors (KNN) is very intuitive. Take classification tasks as an example. What KNN does is to find K training examples that are "closest" to the test example. These K training examples have K labels. We find the most common label, and it is the predicted label for the test example in question. 

How to quantify "closest"? Two simple methods are Euclidean distance or Manhattan distance. 

I'll take the MNIST dataset as an example. 


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
```

## Introduction to MNIST dataset


```python
file = '../static/files/large/mnist_train.csv'
# you can download the files here: https://pjreddie.com/projects/mnist-in-csv/
```


```python
df = pd.read_csv(file)
```


```python
df.shape
```




    (59999, 785)




```python
# df.head()
```

![](/en/blog/2022-09-26-knn_files/knn-df-head.png)

The above is the MNIST dataset. it has close to 60K training examples, i.e., images. Each training example has 785 columns. The first column is the label and the remaining 784 (28 `$\times$` 28) columns are the input. Each of these 784 columns correspondes to a pixel, which I will explain later. 

The raw inputs are RGB values which range from 0 to 255:


```python
np.max(df.to_numpy())
```




    255



We normally would [normalize the inputs](https://stackoverflow.com/a/37421202) so that they values are between 0 (black) and 1 (white). To do that, we divide each pixel value by the number of 255. 

Let's take a look at one example. 


```python
example_to_draw = df.iloc[0, 1:].to_numpy().reshape(28,28)
plt.imshow(example_to_draw, cmap='gray')
plt.show()
```


    
![png](/en/blog/2022-09-26-knn_files/2022-09-26-knn_12_0.png)
    


## KNN

### Training examples

Since there are 60K examples, for simplicity, I'll randomly choose 10K of them as training examples. 


```python
training_samples = df.sample(10000, random_state = 222)
training_indices = training_samples.index
```


```python
x_train = (training_samples.iloc[:, 1:]/255).to_numpy()
y_train = training_samples.iloc[:, 0].to_numpy()
```


```python
x_train.shape, y_train.shape
```




    ((10000, 784), (10000,))




```python
Counter(y_train)
```




    Counter({4: 957,
             5: 920,
             2: 1005,
             7: 1060,
             8: 1019,
             1: 1160,
             0: 966,
             9: 974,
             3: 958,
             6: 981})




```python
label_counts = pd.DataFrame(Counter(y_train).items(), columns = ['label', 'frequency'])
sns.barplot(data = label_counts, x = 'label', y = 'frequency')
```




    <AxesSubplot:xlabel='label', ylabel='frequency'>




    
![png](/en/blog/2022-09-26-knn_files/2022-09-26-knn_19_1.png)
    


### Test examples

I will have 100 images as test examples. 


```python
testing_samples = df[~df.index.isin(training_indices)].sample(100, random_state = 300)
```


```python
x_test = (testing_samples.iloc[:, 1:]/255).to_numpy()
y_test = testing_samples.iloc[:, 0].to_numpy()
```


```python
x_test.shape, y_test.shape
```




    ((100, 784), (100,))




```python
label_counts = pd.DataFrame(Counter(y_test).items(), columns = ['label', 'frequency'])
sns.barplot(data = label_counts, x = 'label', y = 'frequency')
```




    <AxesSubplot:xlabel='label', ylabel='frequency'>




    
![png](/en/blog/2022-09-26-knn_files/2022-09-26-knn_24_1.png)
    


### K-Nearest Neighbors


```python
def eu_distance(v1, v2):
    '''to calculate the euclidean distance of two vectors
        input: vector1, vector2
    '''
    s = np.sum((v1-v2)**2)
    d = np.sqrt(s)
    return d
```


```python
def knn(test_example, x_train, k):
    
    # calculate the euclidean distance between the test example and 
    # each example x_train. Accumulate these distances to sort later
    distances = np.array([eu_distance(test_example, i) for i in x_train])
    
    # np.argsort sorts the distances in ascending order, and return the index 
    # of items in the original (unsorted) array 
    sorted_idx = np.argsort(distances)
    
    # we only want the k smallest distances
    return sorted_idx[0:k]
```

Let's take one example. Exciting moment!


```python
## example 1:
eg1 = x_test[1]
plt.imshow(eg1.reshape(28, 28), cmap='gray')
```




    <matplotlib.image.AxesImage at 0x11492a3b0>




    
![png](/en/blog/2022-09-26-knn_files/2022-09-26-knn_29_1.png)
    



```python
# Let's say k = 3
k = 3
knn_results = knn(eg1, x_train, k)
knn_results
```




    array([7572, 4714, 5533])




```python
f, axs = plt.subplots(1,3,
                      figsize=(14,5),
                      sharex=False,
                      sharey=False,)
for i in range(k):
    result = knn_results[i]
    axs[i].imshow(x_train[result].reshape(28,28), cmap='gray')
```


    
![png](/en/blog/2022-09-26-knn_files/2022-09-26-knn_31_0.png)
    


These three nearest neighbors have the label of '6', so the label for `eg1` should be '6' as well. 


```python
y_train[knn_results]
```




    array([6, 6, 6])



Let's have another example:


```python
# example 2
eg2 = x_test[90]
plt.imshow(eg2.reshape(28, 28), cmap='gray')
```




    <matplotlib.image.AxesImage at 0x11439f070>




    
![png](/en/blog/2022-09-26-knn_files/2022-09-26-knn_35_1.png)
    



```python
# Let's say k = 3
k = 3
knn_results = knn(eg2, x_train, k)
knn_results
```




    array([9273, 7022, 4709])




```python
f, axs = plt.subplots(1,3,
                      figsize=(14,5),
                      sharex=False,
                      sharey=False,)
for i in range(k):
    result = knn_results[i]
    axs[i].imshow(x_train[result].reshape(28,28), cmap='gray')
```


    
![png](/en/blog/2022-09-26-knn_files/2022-09-26-knn_37_0.png)
    


These three nearest neighbors have the label of '7', so the label for `eg2` should be '7' as well. 


```python
y_train[knn_results]
```




    array([7, 7, 7])


