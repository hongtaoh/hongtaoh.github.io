---

title: "Explaining Infinite Gaussian Mixuture Model"
date: 2024-12-16T14:00:00-06:00
author: Hongtao Hao
slug: igmm
draft: false
toc: true
tags: Stats
---

# Infinite Gaussian Mixture Model

## Concept Overview

The Infinite Gaussian Mixture Model (IGMM) is an extension of the [Gaussian Mixture Model (GMM)](/2024/11/21/gmm/). It handles situations where we don't know the number of clusters. In such cases, we can't use the E-M algorithm used in GMM.

The solution is to assume there are infinite clusters. Let's say the cluster assignment for data point `$x_i$` is `$z_i = k$`, where `$k$` represents a specific cluster. We assume the data in each cluster follows a normal distribution, `$\mathcal{N}(\mu_k, \sigma_k^2)$`. We want to obtain two probability distributions:

1. For a data point `$x_i$`, we want to know the probability that this data point belongs to a certain cluster, `$P(z_i = k|x_i)$`, which is a discrete distribution.
2. Given a set of data points belonging to a cluster `$X_k = \{x_i|z_i = k\}$`, we want to know the posterior distribution of that cluster's parameters, i.e., the probability distribution of different `$\mu_k$` and `$\sigma_k^2$` values, `$P(\mu_k, \sigma_k^2|X_k)$`, which is a continuous distribution.

To calculate these probabilities, we need to use Bayes' theorem:

\begin{equation}
P(z_i = k|x_i) \propto P(z_i = k) \cdot P(x_i|z_i = k) =  P(z_i = k) \cdot P(x_i|\mu_k, \sigma_k^2) \tag{1}
\end{equation}

\begin{equation}
P(\mu_k, \sigma_k^2|X_k) \propto P(\mu_k, \sigma_k^2) \cdot \prod_{x_i \in X_k} P(x_i|\mu_k, \sigma_k^2)\tag{2}
\end{equation}

For equation (1):

`$P(z_i = k)$` is the prior probability of clusters. In other words, without any information, how do we infer which cluster a data point `$x_i$` belongs to.

`$P(x_i|z_i = k)$` represents the likelihood of the data point given it belongs to cluster `$k$`. For a Gaussian distribution, this likelihood is the density value (PDF) at a particular `$x$` coordinate. That is, `$P(x_i|z_i = k) = P(x_i|\mu_k, \sigma_k^2)$`

For equation (2):

The target `$P(\mu_k, \sigma_k^2|X_k)$` represents our understanding of the cluster parameters given the information `$X_k$`. What distribution should we choose to describe our understanding of the parameters?

For the cluster mean `$\mu_k$`, we have many options like normal distribution, T distribution, Laplace distribution, etc. For `$\sigma_k^2$`, since we need to ensure `$\sigma_k^2 \ge 0$`, we also have many choices like gamma distribution, inverse gamma distribution, chi-square distribution, log-normal distribution, exponential distribution, etc. All these can ensure the result (random variable of the distribution) is non-negative.

Additionally, we need to note that `$P(\mu_k, \sigma_k^2|X_k)$` is a joint probability because `$\mu_k$` and `$\sigma_k^2$` are related. When `$\sigma_k^2$` is small, our uncertainty about the mean is also small. Therefore, the posterior distribution also needs to describe their relationship.

`$\prod_{x_i \in X_k} P(x_i|\mu_k, \sigma_k^2)$` is the product of multiple normal distribution density functions. However, in actual computation, we calculate point by point:

\begin{equation}
P(\mu_k, \sigma_k^2|x_i, z_i = k) \propto P(\mu_k, \sigma_k^2) \cdot P(x_i|\mu_k, \sigma_k^2)\tag{3}
\end{equation}

Computing for all points in `$X_k$` gives us `$P(\mu_k, \sigma_k^2|X_k)$`

Why do point-by-point calculation and product calculation give the same result? Consider that for the first data point in set `$X_k$`, the posterior from `$P(\mu_k, \sigma_k^2) \cdot P(x_1|\mu_k, \sigma_k^2)$` becomes the prior when calculating for `$x_2$`. So calculating point by point and calculating all points together give the same result.

The question now is how to choose distributions for mean and variance to satisfy the above two requirements: (1) Realistic - choose from the listed possible distributions. (2) It's a joint probability. Additionally, it's best to satisfy a third requirement (3) Easy to compute.

Scientists discovered something clever: if we choose a normal distribution to describe the mean and an inverse gamma distribution to describe the variance as our prior probabilities, then after multiplying with `$\prod_{x_i \in X_k} P(x_i|\mu_k, \sigma_k^2)$`, the resulting distribution has the same form as the prior distribution, also a "Normal-Inverse-Gamma Distribution". This greatly simplifies the computation, so we choose to use a normal distribution for the mean and an inverse gamma distribution for the variance. The data determines the inverse gamma function, and the inverse gamma function determines the normal distribution. This way, all three requirements are satisfied.

For more examples of such clever cases, refer to "Conjugate Priors".

## Detailed Calculation Process

Let's discuss how to perform the calculations. We calculate point by point, iterating through each data point.

### Equation (3)

Let's first look at equation (3).

Assume the current prior `$P(\mu_k, \sigma_k^2)$` is a Normal-Inverse-Gamma distribution:

`$$\sigma^2 \sim \text{InvGamma}(\alpha_0, \beta_0)$$`

`$$\mu|\sigma^2 \sim \mathcal{N}(\mu_0, \frac{\sigma^2}{\kappa_0})$$`

where:

- `$\alpha_0, \beta_0$`: control the shape of the variance distribution
- `$\mu_0$`: prior mean of the mean
- `$\kappa_0$`: degree of belief in the prior mean

The likelihood function for a new data point `$x_i$` belonging to cluster `$k$`:

`$$P(x_i|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi\sigma_k^2}}\exp\left( -\frac{(x_i - \mu_k)^2}{2\sigma_k^2} \right)$$`

Note that `$\mu_k$` and `$\sigma_k^2$` are the current posterior distribution for that cluster's parameters, i.e., `$P(\mu_k, \sigma_k^2|X_k)$`. This means they themselves are distributions, not specific values. For the first data point in that cluster, `$\mu_k$` and `$\sigma_k^2$` are just the prior distributions.

Since `$\mu_k$` and `$\sigma_k^2$` are distributions, not specific values, but `$P(x_i|\mu_k, \sigma_k^2)$` needs to be a specific value, what should we do? We need to consider all possible `$\mu$` and `$\sigma^2$` values and take their weighted average. The weights are the density values of each `$\mu$` and `$\sigma^2$` in `$P(\mu_k, \sigma_k^2|X_k)$`:

`$$P(x_i|\mu_k, \sigma_k^2) = \int_{\mu} \int_{\sigma^2} P(x_i|\mu, \sigma^2) \cdot P(\mu, \sigma^2|X_k) \cdot d\mu \cdot d\sigma^2$$`

This double integral still needs to be multiplied by the prior distribution `$P(\mu_k, \sigma_k^2)$`, which makes it extremely difficult to calculate.

Fortunately, mathematicians have already solved this: if `$P(\mu_k, \sigma_k^2)$` uses a Normal-Inverse-Gamma distribution, plugging into the above formula gives a result that is still a Normal-Inverse-Gamma distribution - this is "conjugate prior".

This way, we can directly update the Normal-Inverse-Gamma distribution parameters based on these predecessors' "conjugate prior" properties:

- `$\kappa_n = \kappa_0 + n$`
- `$\mu_n = \frac{\kappa_0\mu_0 + n \bar{x}}{\kappa_n}$`
- `$\alpha_n = \alpha_0 + \frac{n}{2}$`
- `$\beta_n = \beta_0 + \frac{1}{2}\sum(x_i - \bar{x})^2 + \frac{\kappa_0n(\bar{x}-\mu_0)^2}{2\kappa_n}$`

So our posterior distribution is:

`$$\sigma_\text{post}^2 \sim \text{InvGamma}(\alpha_n, \beta_n)$$`

`$$\mu_\text{post}|\sigma_\text{post}^2 \sim \mathcal{N}(\mu_n, \frac{\sigma_\text{post}^2}{\kappa_n})$$`

Two points need special attention. First, in the formulas above, if we're updating the posterior distribution parameters point by point (incremental updating), then `$n = 1, \bar{x} = x_i$`. If we're batch updating, meaning we calculate all data points first and then update parameters after knowing all points in each cluster, then `$n$` is the total number of points in that cluster, and `$\bar{x}$` is the mean of these data points. Usually, for computational convenience, we prefer batch updating.

This brings up the second issue: we don't know which cluster this data point belongs to. According to equation (1), any data point's cluster assignment is a distribution, `$P(z_i = k|x_i)$`, not a specific value. This makes it impossible to calculate `$n$`, the number of data points corresponding to `$x_i$`'s cluster.

We mainly have two handling methods:

1. Maximum A Posteriori (MAP) estimation. Find the `$k^*$` corresponding to the maximum value of `$P(z_i = k|x_i)$`.

2. Random sampling. Randomly sample from `$P(z_i = k|x_i)$` according to weights to get a specific `$k$`.

We usually choose the second method to preserve uncertainty.

### Equation (1)

But when calculating equation (1), we're not so lucky. If calculating precisely, we must use the double integral formula above to calculate `$P(x_i|\mu_k, \sigma_k^2)$`. This calculation is very complex. To simplify, we'd better let `$\mu_k$` and `$\sigma_k^2$` be specific values rather than distributions, so we can just plug them into the normal distribution density function. How do we find these two specific values?

We know `$\mu_k, \sigma_k^2 \sim P(\mu_k, \sigma_k^2|X_k)$`, so we can:

1. Random sampling. Randomly sample multiple values from this posterior distribution, calculate `$P(x_i|\mu_k, \sigma_k^2)$`, then take the average:

`$$P(x_i|\mu_k, \sigma_k^2) = \frac{1}{S} \sum_{s=1}^S P(x_i|\mu_k^{(s)}, \sigma_k^{2(s)})$$`

where `$S$` is the number of samples. This method is usually implemented using Gibbs sampling or MCMC methods. We won't expand on this here.

2. Maximum A Posteriori (MAP) estimation, taking the values with the highest density.

Generally, we choose random sampling to preserve uncertainty.

For `$P(z_i = k)$`, most textbooks directly use Dirichlet Process or Chinese Restaurant Process, but for simplicity, we won't assume infinite clusters. Instead, we'll say: we know there might be two clusters, we're uncertain, but there definitely won't be more than 10 clusters. Then we can use a Uniform Distribution. Of course, we could also use Multinomial Distribution and others. Since with enough data, the prior distribution isn't so important, we'll use the uniform distribution for simple calculation.

### Summary

Let's summarize the calculation process, i.e., the specific algorithm.

We calculate point by point: iterate through each data point. For equation (2), based on conjugate priors, using the number of data points `$n$` in that cluster and the mean of these data points `$\bar{x}$`, we can directly update the cluster parameter posterior distribution `$P(\mu_k,\sigma_k^2|X_k)$`. But since a data point's cluster distribution `$P(z_i=k|x_i)$` isn't a specific value, we choose single random sampling to get a specific `$k$`. This way, after iterating through all data points, we know each cluster's `$n$` and `$\bar{x}$`.

In equation (1), to avoid the intense computation of double integrals, we choose to do multiple random samplings of the parameter posterior distribution `$P(\mu_k,\sigma_k^2|X_k)$`, then take the weighted average to get estimates of `$\mu_k$` and `$\sigma_k^2$`. Then plug the data point `$x_i$` into the `$\mathcal{N}(\mu, \sigma^2)$` probability density function (PDF) to get an estimate of `$P(x_i|\mu_k, \sigma_k^2)$`. This process needs to be done for each `$k$` to get the discrete distribution we want.

## Code It Up

Talk is cheap, show me the code.


{{< codeCollapse >}}

import numpy as np 
import matplotlib.pyplot as plt 
from scipy.stats import norm
from scipy.stats import invgamma
import time

np.random.seed(42)
n_female = 600
mu_female = 162
sd_female = 6
n_male = 1000 - n_female
mu_male = 175
sd_male = 7

female_heights = np.random.normal(mu_female, sd_female, n_female)
male_heights = np.random.normal(mu_male, sd_male, n_male)
heights = np.concatenate([female_heights, male_heights])
np.random.shuffle(heights)
data = heights.copy()

plt.figure(figsize=(12, 6))
plt.hist(data, bins = 30, density=True, alpha=0.7, color='gray')
plt.title('Histogram of Heights')
plt.xlabel("Height (cm)")
plt.ylabel("Density")
plt.show()
{{< /codeCollapse >}}


{{< indentedblock >}}
    
{{< /indentedblock >}}
![png](/cn/blog/2024-12-12-igmm_files/2024-12-12-igmm_4_0.png)
{{< indentedblock >}}
    



{{< /indentedblock >}}
```python
from collections import defaultdict

# setting
iterations = 30
nclusters = 10
nsampling = 10

# history of P(z_i=k|x_i)
p1s = []
# history of P(mu_k, sigma_k|X_k)
params = []

# Initialize parameters for each cluster
cluster_params = []
data_min, data_max = min(data), max(data)
for _ in range(nclusters):
    mu0 = np.random.uniform(data_min, data_max)  
    kappa0 = 1
    alpha0 = 2  
    beta0 = 1
    cluster_params.append({
        'mu': mu0,
        'kappa': kappa0,
        'alpha': alpha0,
        'beta': beta0
    })

start_time = time.time()
for it in range(iterations):
    # length will be the length of data
    # if assignments = [0] means the first data point belongs to the first cluster
    assignments = []

    for d in data:
        # Step 1: Calculate P(z_i = k | x_i)
        p1 = []
        # for each cluster, sample and 
        # get weighted average of P(x_i|mu_k, sigma_k^2)
        for k in range(nclusters):
            mus = []
            sigmas = []
            for _ in range(nsampling):
                sigma2 = invgamma.rvs(
                    cluster_params[k]['alpha'], 
                    scale = cluster_params[k]['beta']
                )
                sigma = np.sqrt(sigma2)
                mu = norm.rvs(
                    cluster_params[k]["mu"], 
                    sigma/np.sqrt(cluster_params[k]['kappa'])
                )
                mus.append(mu)
                sigmas.append(sigma)
            
            # norm.pdf() result is too small, use LogSumExp to escape underflow
            log_likelihoods = [norm.logpdf(
                d, mu, sigma) for mu, sigma in zip(mus, sigmas)]
            max_log_likelihood = np.max(log_likelihoods)
            log_sum_exp = max_log_likelihood + np.log(
                np.sum(np.exp(log_likelihoods - max_log_likelihood)))
            average_likelihood = np.exp(log_sum_exp)/len(log_likelihoods)
            p1.append(average_likelihood)

        # Step 2: Assign data point to a cluster
        probs = np.array(p1)
        if np.sum(probs) == 0:
            probs = np.ones(nclusters)/nclusters
        else:
            probs = probs / np.sum(probs)
        
        # 0-9
        choice = np.random.choice(nclusters, p=probs)
        assignments.append(choice)

    # Step 3: Batch update cluster parameters of P(mu_k, sigma_k^2|X_k)
    for k in range(nclusters):
        cluster_data_idx = [i for i, val in enumerate(assignments) if val == k]
        cluster_data = data[cluster_data_idx]
        n = len(cluster_data)
        if n == 0:
            # if cluster is empty, no need to calculate below
            # move on to the next k
            continue
        x_bar = np.mean(cluster_data)
        k_params = cluster_params[k]
        kappa_n = k_params['kappa'] + n
        mu_n = (k_params['kappa'] * k_params['mu'] + n * x_bar) / kappa_n
        alpha_n = k_params['alpha'] + 0.5 * n
        beta_n = k_params['beta'] + 0.5 * np.sum((cluster_data - x_bar)**2) + (
            k_params['kappa'] * n * (x_bar - k_params['mu'])**2) / (2 * kappa_n)
            
        cluster_params[k] = {
            'mu': mu_n,
            "kappa": kappa_n,
            "alpha": alpha_n,
            "beta": beta_n
        }
    
    # keep data from this iteration 
    if (it + 1)%10 == 0:
        elapsed = time.time() - start_time
        print(f"Iteration {it+1} completed! Time elapsed: {elapsed:.2f}s")
    p1s.append(p1.copy())
    params.append([cluster_params[k] for k in range(nclusters)])
```

{{< indentedblock >}}
    Iteration 10 completed! Time elapsed: 59.77s
    Iteration 20 completed! Time elapsed: 119.62s
    Iteration 30 completed! Time elapsed: 179.75s



{{< /indentedblock >}}
{{< codeCollapse >}}

def extract_cluster_means_and_variances(params, nclusters):
    """
    Extract cluster means and variances over iterations from parameter history.
    
    Args:
        params (list): A list of cluster parameters for each iteration.
        nclusters (int): Number of clusters.

    Returns:
        cluster_means (dict): Mean values for each cluster across iterations.
        cluster_variances (dict): Variance values for each cluster across iterations.
    """
    iterations = len(params)  # Number of iterations
    cluster_means = {k: [] for k in range(nclusters)}
    cluster_variances = {k: [] for k in range(nclusters)}

    # Extract means and variances for each cluster
    for it in range(iterations):
        for k in range(nclusters):
            cluster_means[k].append(params[it][k]['mu'])
            cluster_variances[k].append(1 / params[it][k]['alpha'])  # Approximation for variance

    return cluster_means, cluster_variances

def plot_cluster_means(cluster_means, iterations):
    """
    Plot the convergence of cluster means over iterations.

    Args:
        cluster_means (dict): Mean values for each cluster across iterations.
        iterations (int): Total number of iterations.
    """
    plt.figure(figsize=(12, 6))
    for k, means in cluster_means.items():
        plt.plot(range(iterations), means, label=f"Cluster {k} Mean")
    plt.xlabel("Iteration")
    plt.ylabel("Cluster Mean (μ)")
    plt.title("Cluster Mean Convergence")
    plt.legend()
    plt.show()

def plot_cluster_variances(cluster_variances, iterations):
    """
    Plot the convergence of cluster variances over iterations.

    Args:
        cluster_variances (dict): Variance values for each cluster across iterations.
        iterations (int): Total number of iterations.
    """
    plt.figure(figsize=(12, 6))
    for k, variances in cluster_variances.items():
        plt.plot(range(iterations), variances, label=f"Cluster {k} Variance")
    plt.xlabel("Iteration")
    plt.ylabel("Cluster Variance (σ²)")
    plt.title("Cluster Variance Convergence")
    plt.legend()
    plt.show()

# Step 2: Bar chart for cluster sizes
def plot_cluster_sizes(assignments):
    cluster_sizes = np.bincount(assignments)
    plt.figure(figsize=(10, 5))
    plt.bar(range(len(cluster_sizes)), cluster_sizes, color='skyblue')
    plt.title("Cluster Sizes")
    plt.xlabel("Cluster Index")
    plt.ylabel("Number of Points")
    plt.show()

# Step 3: Line plot for likelihood progression
def plot_likelihood_progression(p1s):
    avg_likelihood = [np.mean(p) for p in p1s]
    plt.figure(figsize=(10, 5))
    plt.plot(avg_likelihood, marker='o')
    plt.title("Likelihood Progression")
    plt.xlabel("Iteration")
    plt.ylabel("Average Likelihood")
    plt.grid()
    plt.show()
{{< /codeCollapse >}}


```python
plot_cluster_sizes(assignments)

# Extract cluster means and variances
cluster_means, cluster_variances = extract_cluster_means_and_variances(params, nclusters)
# Plot cluster means
plot_cluster_means(cluster_means, len(params))
# Plot cluster variances
plot_cluster_variances(cluster_variances, len(params))

plot_likelihood_progression(p1s)
```


{{< indentedblock >}}
    
{{< /indentedblock >}}
![png](/cn/blog/2024-12-12-igmm_files/2024-12-12-igmm_7_0.png)
{{< indentedblock >}}
    



    
{{< /indentedblock >}}
![png](/cn/blog/2024-12-12-igmm_files/2024-12-12-igmm_7_1.png)
{{< indentedblock >}}
    



    
{{< /indentedblock >}}
![png](/cn/blog/2024-12-12-igmm_files/2024-12-12-igmm_7_2.png)
{{< indentedblock >}}
    



    
{{< /indentedblock >}}
![png](/cn/blog/2024-12-12-igmm_files/2024-12-12-igmm_7_3.png)
    


## Using Dirichlet Process as Prior

We used a uniform distribution as our prior above, but we can see the results weren't very good. After 30 iterations, we didn't achieve our desired outcome of having most data points fall into two clusters. This is due to two reasons: First, we didn't have enough iterations. With more iterations, the choice of prior would have less impact on the results. Second, our choice of prior wasn't good - with limited iterations, we couldn't obtain a realistic posterior distribution.

So what would be a more appropriate prior? The Dirichlet Process. Why? Because if a cluster is large, when we have a new data point, the probability of that data point belonging to this cluster should be greater than it belonging to any of infinite tiny clusters. This aligns with the posterior update logic in Bayesian statistics. One obvious advantage of choosing the Dirichlet Process as a prior is that we don't need to specify the number of clusters in advance. Instead, we use the `alpha` parameter to determine the probability of creating a new cluster.

The complete code is as follows:


```python
# setting
iterations = 30
nsampling = 10
# alpha for dp
dp_alpha = 2

# Initialize cluster parameters 
default_params = {
    'mu': np.mean(data),
    'kappa': 1,
    'alpha': 2,
    'beta': 1
}

cluster_params = dict()
cluster_params[0] = default_params

# history of P(z_i=k|x_i)
p1s = []
# history of P(mu_k, sigma_k|X_k)
params_history = []

start_time = time.time()
for it in range(iterations):
    # length will be the length of data
    # if assignments = [0] means the first data point belongs to the first cluster
    assignments = []

    for idx, d in enumerate(data):
        # Step 1: Calculate P(z_i = k | x_i)
        p1 = []
        # for each cluster, sample and get weighted average of P(x_i|mu_k, sigma_k^2)

        # we assume the first data point belongs to cluster 0
        for k, k_params in cluster_params.items():
            mus = []
            sigmas = []
            for _ in range(nsampling):
                sigma2 = invgamma.rvs(
                    k_params['alpha'], 
                    scale = k_params['beta']
                )
                sigma = np.sqrt(sigma2)
                mu = norm.rvs(
                    k_params["mu"], 
                    sigma/np.sqrt(k_params['kappa'])
                )
                mus.append(mu)
                sigmas.append(sigma)
            
            # norm.pdf() result is too small, use LogSumExp to escape underflow
            log_likelihoods = [norm.logpdf(
                d, mu, sigma) for mu, sigma in zip(mus, sigmas)]
            max_log_likelihood = np.max(log_likelihoods)
            log_sum_exp = max_log_likelihood + np.log(
                np.sum(np.exp(log_likelihoods - max_log_likelihood)))
            average_likelihood = np.exp(log_sum_exp)/len(log_likelihoods)
            p1.append(average_likelihood)

        # Step 2: Assign data point to a cluster
        if idx == 0:
            assignments.append(0)
        else:
            choices = np.arange(0, max(assignments) + 2)
            n_k = np.bincount(assignments)
            curr_total = np.sum(n_k) + dp_alpha
            probs = np.append(n_k/curr_total, dp_alpha/curr_total)
            cluster_assigned = np.random.choice(choices, p = probs)
            assignments.append(cluster_assigned)

            # if cluster is new, initiate params for this cluster
            if cluster_assigned > len(cluster_params):
                cluster_params[cluster_assigned] = default_params.copy()

    
    # Remove empty clusters from cluster_params
    empty_k = [x for x in cluster_params.keys() if x not in np.unique(assignments)]
    for k in empty_k:
        del cluster_params[k]

    # Step 3: Batch update cluster parameters of P(mu_k, sigma_k^2|X_k)
    for k, k_params in cluster_params.items():
        cluster_data_idx = [i for i, val in enumerate(assignments) if val == k]
        cluster_data = data[cluster_data_idx]
        n = len(cluster_data)
        x_bar = np.mean(cluster_data)

        kappa_n = k_params['kappa'] + n
        mu_n = (k_params['kappa'] * k_params['mu'] + n * x_bar) / kappa_n
        alpha_n = k_params['alpha'] + 0.5 * n
        beta_n = k_params['beta'] + 0.5 * np.sum((cluster_data - x_bar)**2) + (
            k_params['kappa'] * n * (x_bar - k_params['mu'])**2) / (2 * kappa_n)
            
        cluster_params[k] = {
            'mu': mu_n,
            "kappa": kappa_n,
            "alpha": alpha_n,
            "beta": beta_n
        }
        
    # keep data from this iteration 
    if (it + 1)%10 == 0:
        elapsed = time.time() - start_time
        print(f"Iteration {it+1} completed! Time elapsed: {elapsed:.2f}s")
    p1s.append(p1.copy())
    params_history.append(cluster_params)
```

{{< indentedblock >}}
    Iteration 10 completed! Time elapsed: 79.61s
    Iteration 20 completed! Time elapsed: 159.91s
    Iteration 30 completed! Time elapsed: 242.62s



{{< /indentedblock >}}
{{< codeCollapse >}}

def visualize_dpmm_results(data, assignments, params, p1s):
    """
    Comprehensive visualization of DPMM results.
    
    Args:
        data: Original data points
        assignments: Final cluster assignments
        params: History of cluster parameters
        p1s: History of likelihoods
    """
    # Create a figure with 2x2 subplots
    fig = plt.figure(figsize=(20, 15))
    
    # Cluster Sizes
    ax1 = plt.subplot(221)
    plot_cluster_sizes(assignments, ax=ax1)
    
    # Parameter Evolution
    ax2 = plt.subplot(222)
    plot_mu_evolution(params, ax=ax2)

    ax3 = plt.subplot(223)
    plot_sigma_evolution(params, ax=ax3)

    # Likelihood Progression
    ax4 = plt.subplot(224)
    plot_likelihood_progression(p1s, ax=ax4)
    
    plt.tight_layout()
    plt.show()

def plot_cluster_sizes(assignments, ax=None):
    """Plot the distribution of cluster sizes."""
    if ax is None:
        fig, ax = plt.subplots(figsize=(10, 6))
    
    unique_clusters, counts = np.unique(assignments, return_counts=True)
    ax.bar(unique_clusters, counts, alpha=0.8, color='skyblue')
    ax.set_title('Cluster Sizes')
    ax.set_xlabel('Cluster ID')
    ax.set_ylabel('Number of Data Points')
    ax.grid(True, alpha=0.3)

def plot_mu_evolution(params_history, ax=None):
    """Plot the evolution of mu (mean) over iterations."""
    if ax is None:
        fig, ax = plt.subplots(figsize=(10, 6))
    
    # Track mean parameters for each cluster
    cluster_means = {}
    total_iterations = len(params_history)
    
    for i, params in enumerate(params_history):
        for cluster_id in params.keys():  # Use actual cluster IDs
            if cluster_id not in cluster_means:
                cluster_means[cluster_id] = [None] * total_iterations  # Pre-fill with None
            cluster_means[cluster_id][i] = params[cluster_id]['mu']
    
    # Plot mean evolution for each cluster
    for cluster_id, means in cluster_means.items():
        iterations = range(total_iterations)
        ax.plot(iterations, means, '-o', label=f'Cluster {cluster_id}', markersize=3)
    
    ax.set_title('Evolution of Cluster Means')
    ax.set_xlabel('Iteration')
    ax.set_ylabel('Mean Value (μ)')
    ax.grid(True, alpha=0.3)
    ax.legend()

def plot_sigma_evolution(params_history, ax=None):
    """
    Plot the evolution of cluster standard deviations (σ) over iterations.
    
    Args:
        params_history (list): List of dictionaries with cluster parameters for each iteration.
        ax (matplotlib.axes.Axes, optional): Matplotlib axis object for plotting.
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(10, 6))

    # Track standard deviation (sigma) for each cluster
    cluster_sigmas = {}
    total_iterations = len(params_history)
    
    for i, params in enumerate(params_history):
        for cluster_id in params.keys():  # Use actual cluster IDs
            if cluster_id not in cluster_sigmas:
                cluster_sigmas[cluster_id] = [None] * total_iterations  # Pre-fill with None
            beta_k = params[cluster_id]['beta']
            alpha_k = params[cluster_id]['alpha']
            sigma_k = np.sqrt(beta_k / alpha_k)  # Compute standard deviation
            cluster_sigmas[cluster_id][i] = sigma_k
    
    # Plot sigma evolution for each cluster
    for cluster_id, sigmas in cluster_sigmas.items():
        iterations = range(total_iterations)
        ax.plot(iterations, sigmas, '-o', label=f'Cluster {cluster_id}', markersize=3)
    
    ax.set_title('Evolution of Cluster Standard Deviations (σ)')
    ax.set_xlabel('Iteration')
    ax.set_ylabel('Standard Deviation (σ)')
    ax.grid(True, alpha=0.3)
    ax.legend()

def plot_likelihood_progression(p1s, ax=None):
    """Plot the progression of average likelihood."""
    if ax is None:
        fig, ax = plt.subplots(figsize=(10, 6))
    
    # Calculate average likelihood for each iteration
    avg_likelihoods = [np.mean(p1) for p1 in p1s]
    iterations = range(len(avg_likelihoods))
    
    ax.plot(iterations, avg_likelihoods, '-o', markersize=3)
    ax.set_title('Average Likelihood Progression')
    ax.set_xlabel('Iteration')
    ax.set_ylabel('Average Likelihood')
    ax.grid(True, alpha=0.3)

{{< /codeCollapse >}}


```python
visualize_dpmm_results(data, assignments, params_history, p1s)
```


{{< indentedblock >}}
    
{{< /indentedblock >}}
![png](/cn/blog/2024-12-12-igmm_files/2024-12-12-igmm_11_0.png)
    


We can see that our algorithm is very slow. Below, I'll try to optimize it through vectorization.
There are two main changes:

1. Replace `[norm.logpdf(d, mu, sigma) for mu, sigma in zip(mus, sigmas)]` with `calculate_log_likelihood(d, mus, sigmas)` below. This way we avoid using a for loop.

2. We calculate `$P(\mu_k, \sigma^2_k|X_k)$` point by point, which is fine, but we don't need to sample the parameters of each cluster (i.e., `$\mu$` and `$\sigma^2_k$`) for each point. This is because the sampling is determined by each cluster's parameter posterior, which is only updated at the end of each iteration, not point by point. Therefore, we can sample and estimate `$\mu$` and `$\sigma^2_k$` at the beginning of each iteration, before iterating through each data point.


```python
from numba import jit
import time

@jit(nopython=True)

def calculate_log_likelihood(d, mus, sigmas):
    """Vectorized log likelihood calculation with numerical stability.
    log(exp(-(x-μ)²/(2σ²)) / sqrt(2πσ²)) 
        = -(x-μ)²/(2σ²) - log(sqrt(2πσ²))
        = -(x-μ)²/(2σ²) - 0.5*log(2π) - log(σ)
    """
    log_likelihoods = -0.5 * np.log(2 * np.pi) - np.log(sigmas) - \
                      0.5 * ((d - mus) / sigmas) ** 2
    max_log_likelihood = np.max(log_likelihoods)
    log_sum_exp = max_log_likelihood + np.log(
        np.sum(np.exp(log_likelihoods - max_log_likelihood)))
    return np.exp(log_sum_exp) / len(mus)
```


```python
# setting
iterations = 1000
nsampling = 10
# alpha for dp
dp_alpha = 2

# Initialize cluster parameters 
default_params = {
    'mu': np.mean(data),
    'kappa': 1,
    'alpha': 2,
    'beta': 1
}

cluster_params = {0:default_params.copy()}

# history of P(z_i=k|x_i)
p1s = []
# history of P(mu_k, sigma_k|X_k)
params_history = []
n_data = len(data)

start_time = time.time()
for it in range(iterations):
    # length will be the length of data
    # if assignments = [0] means the first data point belongs to the first cluster
    assignments = np.zeros(n_data, dtype=np.int32)

    # for each cluster, sample and get weighted average of P(x_i|mu_k, sigma_k^2)
    # we assume the first data point belongs to cluster 0
    all_mus = {}
    all_sigmas = {}
    for k, k_params in cluster_params.items():
        sigma2_samples = invgamma.rvs(
            k_params['alpha'], 
            scale = k_params['beta'],
            size = nsampling
        )
        sigma_samples = np.sqrt(sigma2_samples)
        mu_samples = norm.rvs(
            k_params["mu"], 
            sigma_samples/np.sqrt(k_params['kappa'])
        )
        all_mus[k] = mu_samples
        all_sigmas[k] = sigma_samples

    for idx, d in enumerate(data):
        # Step 1: Calculate P(z_i = k | x_i) for all existing clusters
        p1 = np.zeros(len(cluster_params))
        for k_idx, k in enumerate(cluster_params.keys()):
            p1[k_idx] = calculate_log_likelihood(d, all_mus[k], all_sigmas[k])
        
        # Step 2: Assign data point to a cluster
        if idx == 0:
            assignments[idx] = 0
            continue

        curr_assignments = assignments[:idx] # excluding assignments[idx]
        choices = np.arange(0, max(curr_assignments) + 2)
        n_k = np.bincount(curr_assignments)
        curr_total = idx + dp_alpha
        probs = np.append(n_k/curr_total, dp_alpha/curr_total)
        cluster_assigned = np.random.choice(choices, p = probs)
        assignments[idx] = cluster_assigned

        # if cluster is new, initiate params for this cluster
        if cluster_assigned not in cluster_params:
            cluster_params[cluster_assigned] = default_params.copy()
            sigma2_samples = invgamma.rvs(
                default_params['alpha'], 
                scale = default_params['beta'],
                size = nsampling
            )
            sigma_samples = np.sqrt(sigma2_samples)
            mu_samples = norm.rvs(
                default_params["mu"], 
                sigma_samples/np.sqrt(default_params['kappa'])
            )
            all_mus[cluster_assigned] = mu_samples
            all_sigmas[cluster_assigned] = sigma_samples

    
    # Remove empty clusters from cluster_params
    empty_k = [x for x in cluster_params.keys() if x not in np.unique(assignments)]
    for k in empty_k:
        del cluster_params[k]

    # Step 3: Batch update cluster parameters of P(mu_k, sigma_k^2|X_k)
    for k, k_params in cluster_params.items():
        cluster_data = data[assignments == k]
        n = len(cluster_data)

        if n> 0:
            x_bar = np.mean(cluster_data)

            kappa_n = k_params['kappa'] + n
            mu_n = (k_params['kappa'] * k_params['mu'] + n * x_bar) / kappa_n
            alpha_n = k_params['alpha'] + 0.5 * n
            beta_n = k_params['beta'] + 0.5 * np.sum((cluster_data - x_bar)**2) + (
                k_params['kappa'] * n * (x_bar - k_params['mu'])**2) / (2 * kappa_n)
                
            cluster_params[k] = {
                'mu': mu_n,
                "kappa": kappa_n,
                "alpha": alpha_n,
                "beta": beta_n
            }
        
    # keep data from this iteration 
    if (it + 1) % 100 == 0:
        elapsed = time.time() - start_time
        print(f"Iteration {it+1} completed! Time elapsed: {elapsed:.2f}s")
    p1s.append(list(p1))
    params_history.append({k: v.copy() for k, v in cluster_params.items()})
```

{{< indentedblock >}}
    Iteration 100 completed! Time elapsed: 4.12s
    Iteration 200 completed! Time elapsed: 8.21s
    Iteration 300 completed! Time elapsed: 12.22s
    Iteration 400 completed! Time elapsed: 16.25s
    Iteration 500 completed! Time elapsed: 20.29s
    Iteration 600 completed! Time elapsed: 24.26s
    Iteration 700 completed! Time elapsed: 28.19s
    Iteration 800 completed! Time elapsed: 32.13s
    Iteration 900 completed! Time elapsed: 36.13s
    Iteration 1000 completed! Time elapsed: 40.25s


{{< /indentedblock >}}

It's much faster!


```python
# Create all visualizations at once
visualize_dpmm_results(data, assignments, params_history, p1s)
```


{{< indentedblock >}}
    
{{< /indentedblock >}}
![png](/cn/blog/2024-12-12-igmm_files/2024-12-12-igmm_16_0.png)
    

