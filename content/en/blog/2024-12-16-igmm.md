---
title: "Infinite Gaussian Mixture Model"
date: 2024-12-12T14:00:00-06:00
author: Hongtao Hao
slug: igmm
draft: false
toc: true
tags: statistics
translate: false
---

## Concept Explanation

The Infinite Gaussian Mixture Model (IGMM) is an extension of the [Gaussian Mixture Model](/blog/2024/11/21/gmm/). It handles situations where we don't know how many clusters exist.

There are generally two approaches to solve this problem. The first is to test different numbers of clusters and compare which result is better, using common methods like the [Elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) (using Sum of Squared Errors), [Akaike information criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion), [Bayesian information criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion), etc.

The second approach, which we'll discuss in this article, assumes an infinite number of clusters. Let's say a data point `$x_i$` belongs to cluster `$z_i = k$`, where `$k$` represents a specific cluster. We assume the data in each cluster follows a normal distribution, `$\mathcal{N}(\mu_k, \sigma_k^2)$`. We ultimately want to obtain two probability distributions:

1. For a data point `$x_i$`, we want to know the probability that this data point belongs to a certain cluster, `$P(z_i = k|x_i)$`, which is a discrete distribution.
2. Given a set of data points belonging to a certain cluster `$X_k = \{x_i|z_i = k\}$`, we want to know the posterior distribution of that cluster's parameters, i.e., the probability distribution of different `$\mu_k$` and `$\sigma_k^2$`, `$P(\mu_k, \sigma_k^2|X_k)$`, which is a continuous distribution.

To calculate these two probabilities, we need to use our "trump card" - Bayes' theorem:

\begin{equation}
P(z_i = k|x_i) \propto P(z_i = k) \cdot P(x_i|z_i = k) =  P(z_i = k) \cdot P(x_i|\mu_k, \sigma_k^2) \tag{1}
\end{equation}

\begin{equation}
P(\mu_k, \sigma_k^2|X_k) \propto P(\mu_k, \sigma_k^2) \cdot \prod_{x_i \in X_k} P(x_i|\mu_k, \sigma_k^2)\tag{2}
\end{equation}

For equation (1):

`$P(z_i = k)$` is the prior probability of the cluster. In other words, how we infer which cluster a data point `$x_i$` belongs to without any information.

`$P(x_i|z_i = k)$` represents the likelihood of the data point given that `$x_i$` belongs to cluster `$k$`. For a Gaussian distribution, this likelihood probability is the density value (PDF) corresponding to a particular `$x$` on the x-axis. That is, `$P(x_i|z_i = k) = P(x_i|\mu_k, \sigma_k^2)$`

For equation (2):

The final `$P(\mu_k, \sigma_k^2|X_k)$` represents our understanding of the cluster parameters given the information `$X_k$`. So what distribution should we choose to describe our understanding of the parameters?

Let's first look at the cluster mean `$\mu_k$`. There are many possible distributions, such as normal distribution, T distribution, Laplace distribution, etc. For `$\sigma_k^2$`, we need to ensure `$\sigma_k^2 \ge 0$`, and there are also many possible distributions, such as gamma distribution, inverse gamma distribution, chi-square distribution, log-normal distribution, exponential distribution, etc. All of these can ensure non-negative results (distribution random variables).

Additionally, we need to note that `$P(\mu_k, \sigma_k^2|X_k)$` is a joint probability, as `$\mu_k$` and `$\sigma_k^2$` are related. When `$\sigma_k^2$` is relatively small, our uncertainty about the mean is also small. Therefore, the posterior distribution needs to describe their relationship.

`$\prod_{x_i \in X_k} P(x_i|\mu_k, \sigma_k^2)$` is the product of multiple normal distribution density functions. However, in actual computation, we calculate point by point:

\begin{equation}
P(\mu_k, \sigma_k^2|x_i, z_i = k) \propto P(\mu_k, \sigma_k^2) \cdot P(x_i|\mu_k, \sigma_k^2)\tag{3}
\end{equation}

After calculating for all points in `$X_k$`, the final result is `$P(\mu_k, \sigma_k^2|X_k)$`

Why does calculating point by point give the same result as multiplying all together? Consider that for the first data point in set `$X_k$`, the posterior from `$P(\mu_k, \sigma_k^2) \cdot P(x_1|\mu_k, \sigma_k^2)$` becomes the prior when calculating `$x_2$`. So calculating point by point gives the same result as calculating with all points in the cluster together.

Now the question is, how do we choose distributions for the mean and variance to satisfy the above two requirements: (1) Match reality (choose from the possible distributions listed above) (2) Be a joint probability. Additionally, it would be best to satisfy a third requirement (3) Be computationally convenient.

Scientists discovered something very clever: if we choose to use a normal distribution to describe the mean and an inverse gamma distribution to describe the variance as our prior probability, then after multiplying with `$\prod_{x_i \in X_k} P(x_i|\mu_k, \sigma_k^2)$`, the final distribution has the same form as the prior distribution, also being a "Normal-Inverse-Gamma Distribution". This greatly simplifies the computation, so we choose to use a normal distribution to describe the mean and an inverse gamma distribution to describe the variance. Here, the data determines the inverse gamma function, and the inverse gamma function determines the normal distribution. This way, all three requirements above are satisfied.

For more examples of such clever cases, refer to "[Conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior)".

## Specific Calculation Process

Let's talk about how to perform the specific calculations. We calculate point by point, that is, we iterate through each data point.

### Equation (3)

Let's first look at equation (3).

Assume the current prior `$P(\mu_k, \sigma_k^2)$` is a Normal-Inverse-Gamma distribution:

`$$\sigma^2 \sim \text{InvGamma}(\alpha_0, \beta_0)$$`

`$$\mu|\sigma^2 \sim \mathcal{N}(\mu_0, \frac{\sigma^2}{\kappa_0})$$`

Where:

- `$\alpha_0, \beta_0$`: control the shape of the variance distribution
- `$\mu_0$`: prior mean of the mean
- `$\kappa_0$`: degree of certainty in the prior mean

The likelihood function for a new data point `$x_i$` belonging to cluster `$k$`:

`$$P(x_i|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi\sigma_k^2}}\exp\left( -\frac{(x_i - \mu_k)^2}{2\sigma_k^2} \right)$$`

Note that `$\mu_k$` and `$\sigma_k^2$` are the current posterior distribution of that cluster's parameters, i.e., `$P(\mu_k, \sigma_k^2|X_k)$`. In other words, they themselves are distributions, not specific values. For the first data point in that cluster, `$\mu_k$` and `$\sigma_k^2$` are the prior distributions.

`$\mu_k$` and `$\sigma_k^2$` are distributions, not specific values, but `$P(x_i|\mu_k, \sigma_k^2)$` is indeed a specific value - how do we handle this? We need to consider all `$\mu$` and `$\sigma^2$`, then take a weighted average. The weights are the density values of each `$\mu$` and `$\sigma^2$` in `$P(\mu_k, \sigma_k^2|X_k)$`:

`$$P(x_i|\mu_k, \sigma_k^2) = \int_{\mu} \int_{\sigma^2} P(x_i|\mu, \sigma^2) \cdot P(\mu, \sigma^2|X_k) \cdot d\mu \cdot d\sigma^2$$`

According to equation (3), this double integral still needs to be multiplied by the prior distribution `$P(\mu_k, \sigma_k^2)$`, which would be "extremely difficult to calculate".

However, mathematicians have already worked this out: if `$P(\mu_k, \sigma_k^2)$` uses a Normal-Inverse-Gamma distribution, plugging into the above formula results in another Normal-Inverse-Gamma distribution - this is the "conjugate prior".

This way, we can directly update the parameters of the Normal-Inverse-Gamma distribution based on these properties of the "conjugate prior" derived by our predecessors:

- `$\kappa_n = \kappa_0 + n$`
- `$\mu_n = \frac{\kappa_0\mu_0 + n \bar{x}}{\kappa_n}$`
- `$\alpha_n = \alpha_0 + \frac{n}{2}$`
- `$\beta_n = \beta_0 + \frac{1}{2}\sum(x_i - \bar{x})^2 + \frac{\kappa_0n(\bar{x}-\mu_0)^2}{2\kappa_n}$`

That is, our posterior distribution is:

`$$\sigma_\text{post}^2 \sim \text{InvGamma}(\alpha_n, \beta_n)$$`

`$$\mu_\text{post}|\sigma_\text{post}^2 \sim \mathcal{N}(\mu_n, \frac{\sigma_\text{post}^2}{\kappa_n})$$`

There are two points that need special attention. First, in the formulas given above, if we're updating the posterior distribution parameters incrementally (incremental updating), then `$n = 1, \bar{x} = x_i$`. If we're doing batch updating, meaning we calculate all data points first and then update the parameters together after knowing all points contained in each cluster, then `$n$` is how many total data points are in that cluster, and `$\bar{x}$` is the mean of these data. Usually, for computational convenience, we prefer to use batch updating.

If using batch updating, we need to finally get the `$n$` and `$\bar{x}$` for each cluster. So when we calculate point by point, we need to update `$X_k = \{x_i|z_i = k\}$`, that is, assign each data point `$x_i$` to a specific cluster. But according to equation (1), the cluster membership of any data point is a distribution, i.e., `$P(z_i = k|x_i)$`, not a definite value.

We mainly have two methods to handle this:

1. Maximum A Posteriori (MAP) estimation. That is, find `$k^*$` corresponding to the maximum of `$P(z_i = k|x_i)$`.

2. Random sampling. Randomly sample from `$P(z_i = k|x_i)$` according to weights to get a specific `$k$`.

We usually choose the second method, as this preserves uncertainty.

### Equation (1)

But when calculating equation (1), we're not so lucky. If calculating exactly, we must use the double integral formula above to calculate `$P(x_i|\mu_k, \sigma_k^2)$`. This calculation is very complex, and to simplify it, we'd prefer to have `$\mu_k$` and `$\sigma_k^2$` be specific values rather than distributions, so we can just plug them into the normal distribution density function. So how do we find these two specific values?

We know `$\mu_k, \sigma_k^2 \sim P(\mu_k, \sigma_k^2|X_k)$`, so we can:

1. Random sampling. From this posterior distribution, randomly sample multiple values, calculate `$P(x_i|\mu_k, \sigma_k^2)$`, then take the average:

`$$P(x_i|\mu_k, \sigma_k^2) = \frac{1}{S} \sum_{s=1}^S P(x_i|\mu_k^{(s)}, \sigma_k^{2(s)}) \tag{4}$$`

where `$S$` is the number of samples. This method is usually implemented using Gibbs sampling or MCMC methods. We won't expand on this here.

2. Maximum A Posteriori (MAP) estimation, that is, take the value with the highest density.

Generally, we choose random sampling to preserve uncertainty.

For `$P(z_i = k)$`, general textbooks directly use Dirichlet Process or Chinese Restaurant Process, but for simplicity, we won't assume infinite clusters. Instead, we'll say: we know there might be two clusters, we're uncertain, but there definitely won't be more than 10 clusters. Then we'll use a Uniform Distribution. Of course, we could also use a Multinomial Distribution, etc. Since with enough data, the prior distribution isn't so important, we'll still use a uniform distribution for simple calculation.

### Summary

Let's summarize the calculation process, that is, the specific algorithm.

We calculate point by point: iterate through each data point. For equation (2), based on conjugate priors, we can directly update the cluster parameter posterior distribution `$P(\mu_k,\sigma_k^2|X_k)$` using the number of data points `$n$` contained in that cluster and the mean `$\bar{x}$` of these data points. However, since the cluster distribution `$P(z_i=k|x_i)$` given a data point is not a specific value, we choose single random sampling to get a specific `$k$`. This way, after iterating through all data points, we know the `$n$` and `$\bar{x}$` for each cluster.

In equation (1), to avoid the intensive computation of double integrals, we choose to perform multiple random samplings from the parameter posterior distribution `$P(\mu_k,\sigma_k^2|X_k)$`, then take a weighted average to get estimated values for `$\mu_k$` and `$\sigma_k^2$`. Then we plug the data point `$x_i$` into the `$\mathcal{N}(\mu, \sigma^2)$` probability density function (PDF) to get an estimated value for `$P(x_i|\mu_k, \sigma_k^2)$`. This process needs to be done for each `$k$` to get the discrete distribution we want.

## Let's Get Started

Talk is cheap, show me the code.

```python
import numpy as np 
import matplotlib.pyplot as plt 
from scipy.stats import norm
from scipy.stats import invgamma
import time
from collections import defaultdict
from numba import jit
```

```python
np.random.seed(42)
n_female = 600
mu_female = 162
sd_female = 6
n_male = 1000 - n_female
mu_male = 175
sd_male = 7

female_heights = np.random.normal(mu_female, sd_female, n_female)
male_heights = np.random.normal(mu_male, sd_male, n_male)
heights = np.concatenate([female_heights, male_heights])
np.random.shuffle(heights)
data = heights.copy()

plt.figure(figsize=(12, 6))
plt.hist(data, bins = 30, density=True, alpha=0.7, color='gray')
plt.title('Histogram of Heights')
plt.xlabel("Height (cm)")
plt.ylabel("Density")
plt.show()
```

```python
# setting
iterations = 10
nclusters = 10
nsampling = 10

# history of P(z_i=k|x_i)
p1s = []
# history of P(mu_k, sigma_k|X_k)
params = []

# Initialize parameters for each cluster
cluster_params = []
data_min, data_max = min(data), max(data)
for _ in range(nclusters):
    mu0 = np.random.uniform(data_min, data_max)  
    kappa0 = 1
    alpha0 = 2  
    beta0 = 1
    cluster_params.append({
        'mu': mu0,
        'kappa': kappa0,
        'alpha': alpha0,
        'beta': beta0
    })

start_time = time.time()
for it in range(iterations):
    # length will be the length of data
    # if assignments = [0] means the first data point belongs to the first cluster
    assignments = []

    for d in data:
        # Step 1: Calculate P(z_i = k | x_i)
        p1 = []
        # for each cluster, sample and 
        # get weighted average of P(x_i|mu_k, sigma_k^2)
        for k in range(nclusters):
            mus = []
            sigmas = []
            for _ in range(nsampling):
                sigma2 = invgamma.rvs(
                    cluster_params[k]['alpha'], 
                    scale = cluster_params[k]['beta']
                )
                sigma = np.sqrt(sigma2)
                mu = norm.rvs(
                    cluster_params[k]["mu"], 
                    sigma/np.sqrt(cluster_params[k]['kappa'])
                )
                mus.append(mu)
                sigmas.append(sigma)
            
            # norm.pdf() result is too small, use LogSumExp to escape underflow
            log_likelihoods = [norm.logpdf(
                d, mu, sigma) for mu, sigma in zip(mus, sigmas)]
            max_log_likelihood = np.max(log_likelihoods)
            log_sum_exp = max_log_likelihood + np.log(
                np.sum(np.exp(log_likelihoods - max_log_likelihood)))
            average_likelihood = np.exp(log_sum_exp)/len(log_likelihoods)
            p1.append(average_likelihood)

        # Step 2: Assign data point to a cluster
        probs = np.array(p1)
        if np.sum(probs) == 0:
            probs = np.ones(nclusters)/nclusters
        else:
            probs = probs / np.sum(probs)
        
        # 0-9
        choice = np.random.choice(nclusters, p=probs)
        assignments.append(choice)

    # Step 3: Batch update cluster parameters of P(mu_k, sigma_k^2|X_k)
    for k in range(nclusters):
        cluster_data_idx = [i for i, val in enumerate(assignments) if val == k]
        cluster_data = data[cluster_data_idx]
        n = len(cluster_data)
        if n == 0:
            # if cluster is empty, no need to calculate below
            # move on to the next k
            continue
        x_bar = np.mean(cluster_data)
        k_params = cluster_params[k]
        kappa_n = k_params['kappa'] + n
        mu_n = (k_params['kappa'] * k_params['mu'] + n * x_bar) / kappa_n
        alpha_n = k_params['alpha'] + 0.5 * n
        beta_n = k_params['beta'] + 0.5 * np.sum((cluster_data - x_bar)**2) + (
            k_params['kappa'] * n * (x_bar - k_params['mu'])**2) / (2 * kappa_n)
            
        cluster_params[k] = {
            'mu': mu_n,
            "kappa": kappa_n,
            "alpha": alpha_n,
            "beta": beta_n
        }
    
    # keep data from this iteration 
    if (it + 1)%1 == 0:
        elapsed = time.time() - start_time
        print(f"Iteration {it+1} completed! Time elapsed: {elapsed:.2f}s")
    p1s.append(p1.copy())
    params.append([cluster_params[k] for k in range(nclusters)])
```

### Optimization

We can see that our algorithm is very slow. Below, I'll try to optimize it through "vector operations":

1. Change `[norm.logpdf(d, mu, sigma) for mu, sigma in zip(mus, sigmas)]` to `calculate_average_likelihood(d, mus, sigmas)` below. This way we avoid the for loop.

2. We calculate `$P(\mu_k, \sigma^2_k|X_k)$` point by point, which is fine, but we don't need to sample `$\mu$` and `$\sigma^2_k$` for each cluster point by point. This is because sampling is determined by each cluster's parameter posterior, and the parameter posterior is only updated at the end of each iteration, not point by point. So we can sample `$\mu$` and `$\sigma^2_k$` at the beginning of each iteration, before iterating through each data point.

```python
@jit(nopython=True)
def calculate_average_likelihood(d, mus, sigmas):
    """Vectorized log likelihood calculation with numerical stability.
    log(exp(-(x-μ)²/(2σ²)) / sqrt(2πσ²)) 
        = -(x-μ)²/(2σ²) - log(sqrt(2πσ²))
        = -(x-μ)²/(2σ²) - 0.5*log(2π) - log(σ)
    """
    log_likelihoods = -0.5 * np.log(2 * np.pi) - np.log(sigmas) - \
                      0.5 * ((d - mus) / sigmas) ** 2
    max_log_likelihood = np.max(log_likelihoods)
    log_sum_exp = max_log_likelihood + np.log(
        np.sum(np.exp(log_likelihoods - max_log_likelihood)))
    return np.exp(log_sum_exp) / len(mus)

# setting
iterations = 1000
nclusters = 10
nsampling = 10

# history of P(z_i=k|x_i)
p1s = []
# history of P(mu_k, sigma_k|X_k)
params_history = []
n_data = len(data)

# Initialize parameters for each cluster
cluster_params = {}
data_min, data_max = min(data), max(data)
for k in range(nclusters):
    mu0 = np.random.uniform(data_min, data_max)  
    kappa0 = 1
    alpha0 = 2  
    beta0 = 1
    cluster_params[k]={
        'mu': mu0,
        'kappa': kappa0,
        'alpha': alpha0,
        'beta': beta0
    }

start_time = time.time()
for it in range(iterations):
    # length will be the length of data
    # if assignments = [0] means the first data point belongs to the first cluster
    assignments = np.zeros(n_data, dtype=np.int32)

    # for each cluster, sample and get weighted average of P(x_i|mu_k, sigma_k^2)
    # we assume the first data point belongs to cluster 0
    all_mus = {}
    all_sigmas = {}
    for k, k_params in cluster_params.items():
        sigma2_samples = invgamma.rvs(
            k_params['alpha'], 
            scale = k_params['beta'],
            size = nsampling
        )
        sigma_samples = np.sqrt(sigma2_samples)
        mu_samples = norm.rvs(
            k_params["mu"], 
            sigma_samples/np.sqrt(k_params['kappa'])
        )
        all_mus[k] = mu_samples
        all_sigmas[k] = sigma_samples

    for idx, d in enumerate(data):
        # Step 1: Calculate P(z_i = k | x_i) for all existing clusters
        p1 = np.zeros(len(cluster_params))
        for k_idx, k in enumerate(cluster_params.keys()):
            p1[k_idx] = calculate_average_likelihood(d, all_mus[k], all_sigmas[k])
        
        # Step 2: Assign data point to a cluster
        if idx == 0:
            assignments[idx] = 0
            continue

        probs = np.array(p1)
        if np.sum(probs) == 0:
            probs = np.ones(nclusters)/nclusters
        else:
            probs = probs / np.sum(probs)
        
        # 0-9
        choice = np.random.choice(nclusters, p=probs)
        assignments[idx] = choice

    # Step 3: Batch update cluster parameters of P(mu_k, sigma_k^2|X_k)
    for k, k_params in cluster_params.items():
        cluster_data = data[assignments == k]
        n = len(cluster_data)

        if n> 0:
            x_bar = np.mean(cluster_data)

            kappa_n = k_params['kappa'] + n
            mu_n = (k_params['kappa'] * k_params['mu'] + n * x_bar) / kappa_n
            alpha_n = k_params['alpha'] + 0.5 * n
            beta_n = k_params['beta'] + 0.5 * np.sum((cluster_data - x_bar)**2) + (
                k_params['kappa'] * n * (x_bar - k_params['mu'])**2) / (2 * kappa_n)
                
            cluster_params[k] = {
                'mu': mu_n,
                "kappa": kappa_n,
                "alpha": alpha_n,
                "beta": beta_n
            }
    
    # keep data from this iteration 
    if (it + 1)%100 == 0:
        elapsed = time.time() - start_time
        print(f"Iteration {it+1} completed! Time elapsed: {elapsed:.2f}s")
    p1s.append(p1.copy())
    params_history.append([cluster_params[k] for k in range(nclusters)])
```

Much faster now. Now 1000 iterations take less than 20 seconds.

## Posterior Predictive Distribution 

We can use the posterior predictive distribution to check if our model is correct.

Above, our code finally has two results: `assignments` and `cluster_params`. The former is the cluster membership of each data point, and the latter is the posterior distribution parameters of each cluster.

So if I ask you, between two new data points, `$166$` and `$176$`, which one has a higher probability of occurring? For a more "once and for all" way to answer this question, we should calculate the probability density (Probability Density) for each data point from negative infinity to positive infinity in this interval. This is the posterior predictive distribution.

Mathematically, we are calculating:

`$$P(x_\text{new}) = \sum_{k=1}^K P(z_\text{new}=k)\cdot P(x_\text{new}|\mu_k, \sigma_k)$$`

Let's prove that the integral of this distribution equals 1:

`$$
\begin{aligned} 
\int_{-\infty}^{\infty} P(x_\text{new}) dx_\text{new} &
= \int_{-\infty}^{\infty} \sum_{k=1}^K P(z_\text{new}=k)\cdot P(x_\text{new}|\mu_k, \sigma_k) dx_\text{new}\\ &
= \sum_{k=1}^K P(z_\text{new}=k) \int_{-\infty}^{\infty} P(x_\text{new}|\mu_k, \sigma_k) dx_\text{new} \\ &
= 1 \cdot 1 \\ &
= 1
\end{aligned}
$$`

In actual calculation, there are two issues to consider. First, we don't need to calculate `$x_\text{new}$` for the entire interval `$(-\infty, \infty)$`. To verify the model's accuracy, we only need to calculate the interval of the original data.

Second, `$P(x_\text{new}|\mu_k, \sigma_k)$` cannot be calculated directly because `$\mu_k, \sigma_k$` are distributions, not specific values. We use random sampling and then take the average value. We mentioned this in equation (4).

```python
def plot_predictive_distribution(
    data, 
    all_mus, 
    all_sigmas, 
    assignments, 
    bins=30
):
    # Calculate cluster proportions
    n_data = len(data)
    cluster_ids, counts = np.unique(assignments, return_counts=True)
    proportions = counts / n_data

    # Generate predictive distribution
    x_vals = np.linspace(min(data), max(data), 1000)
    predictive_pdf = np.zeros_like(x_vals)

    # Compute the contribution of each cluster to the predictive PDF
    for cluster_id, proportion in zip(cluster_ids, proportions):
        cluster_likelihoods = np.array([
            calculate_average_likelihood(x, all_mus[cluster_id], all_sigmas[cluster_id])
            for x in x_vals
        ])
        predictive_pdf += proportion * cluster_likelihoods

    # Plot results
    plt.figure(figsize=(12, 6))
    # Plot histogram of data
    plt.hist(data, bins=bins, density=True, alpha=0.6, color='gray', label='Original Data')

    # Plot predictive distribution
    plt.plot(x_vals, predictive_pdf, label='Predictive Distribution', color='red')

    plt.title('Predictive Distribution vs Original Data')
    plt.xlabel('Data')
    plt.ylabel('Density')
    plt.legend()
    plt.show()
```

```python
all_mus = {}
all_sigmas = {}
for k, k_params in cluster_params.items():
    sigma2_samples = invgamma.rvs(
        k_params['alpha'], 
        scale = k_params['beta'],
        size = nsampling
    )
    sigma_samples = np.sqrt(sigma2_samples)
    mu_samples = norm.rvs(
        k_params["mu"], 
        sigma_samples/np.sqrt(k_params['kappa'])
    )
    all_mus[k] = mu_samples
    all_sigmas[k] = sigma_samples

plot_predictive_distribution(data, all_mus, all_sigmas, assignments)
```

## Using Dirichlet Process as Prior

We used a uniform distribution as prior above, but we can see that the results aren't very good. After 30 iterations, the result we wanted - most of the data in two clusters - didn't appear. This is because, first, we didn't have enough iterations. With enough iterations, the choice of prior would have little effect on the results. Second, we didn't choose a good prior, so with limited iterations, we couldn't get a posterior distribution that matches reality.

So what would be a more suitable prior? The Dirichlet Process. Why? Because if a cluster is very large, then when we have a new data point, the probability of this data point belonging to this cluster should be greater than the probability of it belonging to infinitely many tiny clusters, which also aligns with the Bayesian statistical posterior update logic. An obvious benefit of choosing the Dirichlet Process as prior is that we don't need to set in advance how many clusters there are, but instead decide the probability of starting a new cluster by setting `alpha`.

Here's the complete code:

```python
# Lower DP concentration
dp_alpha = 2

# More iterations
iterations = 2000

# More dispersed default params
default_params = {
    'mu': np.mean(data),
    'kappa': 0.5,  # Lower = more spread in means
    'alpha': 3,    # Lower = more variance in cluster widths
    'beta': 1.0    # Higher = wider clusters
}

# More samples for better posterior estimation
nsampling = 100

cluster_params = {0:default_params.copy()}

# history of P(z_i=k|x_i)
p1s = []
# history of P(mu_k, sigma_k|X_k)
params_history = []
n_data = len(data)

start_time = time.time()
for it in range(iterations):
    # length will be the length of data
    # if assignments = [0] means the first data point belongs to the first cluster
    assignments = np.zeros(n_data, dtype=np.int32)

    # for each cluster, sample and get weighted average of P(x_i|mu_k, sigma_k^2)
    # we assume the first data point belongs to cluster 0
    all_mus = {}
    all_sigmas = {}
    for k, k_params in cluster_params.items():
        sigma2_samples = invgamma.rvs(
            k_params['alpha'], 
            scale = k_params['beta'],
            size = nsampling
        )
        sigma_samples = np.sqrt(sigma2_samples)
        mu_samples = norm.rvs(
            k_params["mu"], 
            sigma_samples/np.sqrt(k_params['kappa'])
        )
        all_mus[k] = mu_samples
        all_sigmas[k] = sigma_samples

    for idx, d in enumerate(data):
        # Step 1: Calculate P(z_i = k | x_i) for all existing clusters
        p1 = np.zeros(len(cluster_params))
        for k_idx, k in enumerate(cluster_params.keys()):
            p1[k_idx] = calculate_average_likelihood(d, all_mus[k], all_sigmas[k])
        
        # Step 2: Assign data point to a cluster
        if idx == 0:
            assignments[idx] = 0
            continue

        curr_assignments = assignments[:idx] # excluding assignments[idx]
        choices = np.arange(0, max(curr_assignments) + 2)
        n_k = np.bincount(curr_assignments)
        curr_total = idx + dp_alpha
        probs = np.append(n_k/curr_total, dp_alpha/curr_total)
        cluster_assigned = np.random.choice(choices, p = probs)
        assignments[idx] = cluster_assigned

        # if cluster is new, initiate params for this cluster
        if cluster_assigned not in cluster_params:
            cluster_params[cluster_assigned] = default_params.copy()
            sigma2_samples = invgamma.rvs(
                default_params['alpha'], 
                scale = default_params['beta'],
                size = nsampling
            )
            sigma_samples = np.sqrt(sigma2_samples)
            mu_samples = norm.rvs(
                default_params["mu"], 
                sigma_samples/np.sqrt(default_params['kappa'])
            )
            all_mus[cluster_assigned] = mu_samples
            all_sigmas[cluster_assigned] = sigma_samples
    
    # Remove empty clusters from cluster_params
    empty_k = [x for x in cluster_params.keys() if x not in np.unique(assignments)]
    for k in empty_k:
        del cluster_params[k]

    # Step 3: Batch update cluster parameters of P(mu_k, sigma_k^2|X_k)
    for k, k_params in cluster_params.items():
        cluster_data = data[assignments == k]
        n = len(cluster_data)

        if n> 0:
            x_bar = np.mean(cluster_data)

            kappa_n = k_params['kappa'] + n
            mu_n = (k_params['kappa'] * k_params['mu'] + n * x_bar) / kappa_n
            alpha_n = k_params['alpha'] + 0.5 * n
            beta_n = k_params['beta'] + 0.5 * np.sum((cluster_data - x_bar)**2) + (
                k_params['kappa'] * n * (x_bar - k_params['mu'])**2) / (2 * kappa_n)
                
            cluster_params[k] = {
                'mu': mu_n,
                "kappa": kappa_n,
                "alpha": alpha_n,
                "beta": beta_n
            }
        
    # keep data from this iteration 
    if (it + 1) % 100 == 0:
        elapsed = time.time() - start_time
        print(f"Iteration {it+1} completed! Time elapsed: {elapsed:.2f}s")
    p1s.append(list(p1))
    params_history.append({k: v.copy() for k, v in cluster_params.items()})
```

```python
plot_cluster_sizes(assignments)
```

```python
plot_likelihood_progression(p1s)
```

```python
all_mus = {}
all_sigmas = {}
for k, k_params in cluster_params.items():
    sigma2_samples = invgamma.rvs(
        k_params['alpha'], 
        scale = k_params['beta'],
        size = nsampling
    )
    sigma_samples = np.sqrt(sigma2_samples)
    mu_samples = norm.rvs(
        k_params["mu"], 
        sigma_samples/np.sqrt(k_params['kappa'])
    )
    all_mus[k] = mu_samples
    all_sigmas[k] = sigma_samples

plot_predictive_distribution(data, all_mus, all_sigmas, assignments)
```