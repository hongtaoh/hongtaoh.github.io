<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>How Much Knowledge Can You Pack Into the Parameters of a Language Model?</title>
    <meta charset="utf-8" />
    <meta name="author" content="Hongtao Hao" />
    <meta name="date" content="2022-06-18" />
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# How Much Knowledge Can You Pack Into the Parameters of a Language Model?
]
.subtitle[
## Prof. Junjie Hu’s NLP Reading Group
]
.author[
### Hongtao Hao
]
.date[
### 2022-06-18
]

---

background-image: url(https://raw.githubusercontent.com/hongtaoh/hongtaoh.github.io/sources/static/media/slides/nlp-reading/author.png)
background-size: 700px
background-position: 50% 50%

## Authors
---

## Before You Fall Asleep

--
### Goal
Fine-tune language models pre-trained on **unstructured text** and see how well they perform on question answering tasks **without any external resources**.

--
### Result
  1. The performance is competitive with open-domain question answering systems that utilize external knowledge resources.

--

  2. The performance increases as the model gets bigger.

---
## Motivation

Neural language models trained on unstructured and unlabeled text are able to store implicit knowledge. 

Okay, but so what?

--
1. We have much more unstructured and unlabeled text than labeled text data. 

2. These models are better at information retrievel when queried with informal queries (Don't forget that they are language models!)

---

## How is this work different from others?

- No relevant information, or external resources allowed

- Purpose: see how much knowledge is stored in the model's parameters. 
- Also looks at whether more parameters mean more knowledge stored. 

---
class: inverse, middle, center

# Background

---
## Question answering

- Definition: select or output the correct answer when asked a question

--

- Types: 

  1. Reading comprehension: providing context where answer is stored.
  
--

  2. Open-domain question answering: allowing access to external knowledge bases. 

---
## Transfer learning with language models

- Transfer learning in NLP: train a model with unstructured text before fine-tuning it on downstream tasks. 

- Advantages: 
  - word meaning
  - syntax
  - world knowledge

--

- However, the most popular model in NLP transfer learning--BERT--is not applicable in this study because as an encoder-only model, it is not able to answer question without a context. 

--

- Encoder-decoder Transformer, i.e., “Text-to-Text Transfer Transformer” (T5)

---
class: inverse, middle, center

# Experiment

---
## Datasets

- Natural Questions

- WebQuestions

- TriviaQA

--

Each of these datasets contains both questions and the associated context. In this study, only questions are used. 

---
## Training

- Original T5 models were trained on a multi-task mixture: reading comprehension, translation, classification, question answering, etc.

   - Datasets used in original T5 models training are different from the evaluation datasets in this study. 
  
   - Because original T5 was trained on question answering tasks, this study also reports the performance of T5.1.1 checkpoints that were only trained on unstructured text. 
  
  
- Model size
  1. Base
  2. Large
  3. 3 Billion
  4. 11 Billion

- Salient Span Masking

---
class: inverse, middle, center

# Results

---
background-image: url(https://raw.githubusercontent.com/hongtaoh/hongtaoh.github.io/sources/static/media/slides/nlp-reading/result.png)
background-size: 400px
background-position: 50% 50%

---
## Major results 

1. Performance is competitive with open-domain QA systems

2. Performance increases as the model gets bigger

3. SSM improves performance

---
## Are large models with the compuatio cost?

Probably yes. Open-domain systems are computational expensive as well. 

---
background-image: url(https://raw.githubusercontent.com/hongtaoh/hongtaoh.github.io/sources/static/media/slides/nlp-reading/false-negatives.png)
background-size: 800px
background-position: 50% 50%

---
## False negatives

The performance of closed-book QA systems may be underestimated by the evaluation procedure of benchmarks. 

Among 150 hand coded answers (to questions in validation set of Natural Questions), only 62% are true negatives. 

If we consider this, the score will be 57.8


---
class: inverse, center, middle

# Thank you!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

Slides source codes can be found  [**here**](https://raw.githubusercontent.com/hongtaoh/hongtaoh.github.io/sources/static/slides/knowledge-in-LM.Rmd).
  

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
