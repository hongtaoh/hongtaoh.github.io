{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"从零开始训练简易版 Bert\"\n",
    "date: 2026-01-17\n",
    "author: \"郝鸿涛\"\n",
    "slug: llm\n",
    "draft: false\n",
    "toc: false\n",
    "tags: llm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert 的作用是得到上下文相关的向量表示。举一个例子，\n",
    "\n",
    "- 「你来就来，还带礼物。你几个意思啊？」\n",
    "- 「别多想。我就意思一下」\n",
    "- 「我看不懂这句话里这个词是什么意思」\n",
    "- 「你这么说就没意思了啊」\n",
    "\n",
    "你看，这几句话里的「意思」，它的含义是不同的。向量表示的时候我们肯定不能静态地给同一个词相同的向量表示。\n",
    "\n",
    "上下文相关的向量表示，就是说即使是同一个词，在不同的语境下，我们希望它的向量表示是不同的。如何实现这一点呢？这就需要「注意力机制」，就是那篇赫赫有名的 Attention is all you need 的论文。这篇论文提到的 Transformer 架构，包括了 Encoder 和 Decoder，但 Bert 只涉及 Encoder。\n",
    "\n",
    "在训练之前，我们要做一个「字-向量」查找表。就需要把所有的字整合在一起。最开始，我们可以随机分配向量表示。比方说，「你」的向量表示为 `[0.1, -0.5, 0.9]` 之类。当然，维度会更高。这里需要注意的是，同一个词的向量表示是一样的。另外，我们把 MASK 当作一个具体的字。\n",
    "\n",
    "Bert 的训练核心是这样子。先随机找一个句子，比如 「你这么说就没意思了啊」，然后遮住一个字，比如 「说」:「你这么 MASK 就没意思了啊」。然后：\n",
    "\n",
    "1. 每个字通过「字-向量」查找表来得到向量表示。\n",
    "2. 加上位置表示，也就是说要告诉模型，每个字的位置。\n",
    "3. 经过多层的 Self-Attention (Multi-Head Attention)，得到每个词新的向量表示。这一点我们接下来讲。\n",
    "4. Mask 这个词的输出向量经过分类器之后，我们得到上面词表中每一个词是这个被遮住词的概率。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
