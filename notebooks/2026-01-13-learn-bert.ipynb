{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"ä»é›¶å¼€å§‹è®­ç»ƒç®€æ˜“ç‰ˆ BERT\"\n",
    "date: 2026-02-01\n",
    "author: \"éƒé¸¿æ¶›\"\n",
    "slug: bert\n",
    "draft: false\n",
    "toc: true\n",
    "tags: llm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert çš„ä½œç”¨æ˜¯å¾—åˆ°ä¸Šä¸‹æ–‡ç›¸å…³çš„å‘é‡è¡¨ç¤ºã€‚ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œ\n",
    "\n",
    "- ã€Œä½ æ¥å°±æ¥ï¼Œè¿˜å¸¦ç¤¼ç‰©ã€‚ä½ å‡ ä¸ªæ„æ€å•Šï¼Ÿã€\n",
    "- ã€Œåˆ«å¤šæƒ³ã€‚æˆ‘å°±æ„æ€ä¸€ä¸‹ã€\n",
    "- ã€Œæˆ‘çœ‹ä¸æ‡‚è¿™å¥è¯é‡Œè¿™ä¸ªè¯æ˜¯ä»€ä¹ˆæ„æ€ã€\n",
    "- ã€Œä½ è¿™ä¹ˆè¯´å°±æ²¡æ„æ€äº†å•Šã€\n",
    "\n",
    "ä½ çœ‹ï¼Œè¿™å‡ å¥è¯é‡Œçš„ã€Œæ„æ€ã€ï¼Œå®ƒçš„å«ä¹‰æ˜¯ä¸åŒçš„ã€‚å‘é‡è¡¨ç¤ºçš„æ—¶å€™æˆ‘ä»¬è‚¯å®šä¸èƒ½é™æ€åœ°ç»™åŒä¸€ä¸ªè¯ç›¸åŒçš„å‘é‡è¡¨ç¤ºã€‚\n",
    "\n",
    "ä¸Šä¸‹æ–‡ç›¸å…³çš„å‘é‡è¡¨ç¤ºï¼Œå°±æ˜¯è¯´å³ä½¿æ˜¯åŒä¸€ä¸ªè¯ï¼Œåœ¨ä¸åŒçš„è¯­å¢ƒä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒçš„å‘é‡è¡¨ç¤ºæ˜¯ä¸åŒçš„ã€‚å¦‚ä½•å®ç°è¿™ä¸€ç‚¹å‘¢ï¼Ÿè¿™å°±éœ€è¦ã€Œæ³¨æ„åŠ›æœºåˆ¶ã€ï¼Œå°±æ˜¯é‚£ç¯‡å¤§åé¼é¼çš„ [Attention is all you need](https://arxiv.org/abs/1706.03762) çš„è®ºæ–‡ã€‚è¿™ç¯‡è®ºæ–‡æåˆ°çš„ Transformer æ¶æ„ï¼ŒåŒ…æ‹¬äº† Encoder å’Œ Decoderï¼Œä½† Bert åªæ¶‰åŠ Encoderã€‚\n",
    "\n",
    "åœ¨è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬è¦åšä¸€ä¸ªã€Œå­—-å‘é‡ã€æŸ¥æ‰¾è¡¨ï¼Œä»¥ä¸‹ç®€ç§°ã€Œè¯è¡¨ã€ã€‚æˆ‘ä»¬éœ€è¦æŠŠæ‰€æœ‰çš„å­—æ•´åˆåœ¨ä¸€èµ·ã€‚æœ€å¼€å§‹ï¼Œæˆ‘ä»¬å¯ä»¥éšæœºåˆ†é…å‘é‡è¡¨ç¤ºã€‚æ¯”æ–¹è¯´ï¼Œã€Œä½ ã€çš„å‘é‡è¡¨ç¤ºä¸º `[0.1, -0.5, 0.9]` ä¹‹ç±»ã€‚å½“ç„¶ï¼ŒçœŸæ­£çš„è®­ç»ƒä¸­ï¼Œç»´åº¦ä¼šæ›´é«˜ï¼Œä½†æˆ‘ä»¬è¿™é‡Œåªæ˜¯ä¸ºäº†æ¼”ç¤ºï¼Œæ‰€ä»¥é‡åœ¨ç®€å•ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒåŒä¸€ä¸ªè¯çš„å‘é‡è¡¨ç¤ºæ˜¯ä¸€æ ·çš„ã€‚å¦å¤–ï¼Œ\n",
    "\n",
    "Bert çš„è®­ç»ƒæ ¸å¿ƒæ˜¯è¿™æ ·å­ã€‚å…ˆéšæœºæ‰¾ä¸€ä¸ªå¥å­ï¼Œæ¯”å¦‚ ã€Œä½ è¿™ä¹ˆè¯´å°±æ²¡æ„æ€äº†å•Šã€ï¼Œç„¶åé®ä½ä¸€ä¸ªå­—ï¼Œæ¯”å¦‚ ã€Œè¯´ã€:ã€Œä½ è¿™ä¹ˆ MASK å°±æ²¡æ„æ€äº†å•Šã€ã€‚æˆ‘ä»¬æŠŠ MASK å½“ä½œä¸€ä¸ªå…·ä½“çš„å­—ã€‚\n",
    "\n",
    "1. æ¯ä¸ªå­—é€šè¿‡è¯è¡¨æŸ¥æ‰¾è¡¨æ¥å¾—åˆ°å‘é‡è¡¨ç¤ºã€‚\n",
    "2. åŠ ä¸Šä½ç½®è¡¨ç¤ºï¼Œä¹Ÿå°±æ˜¯è¯´è¦å‘Šè¯‰æ¨¡å‹ï¼Œæ¯ä¸ªå­—çš„ä½ç½®ã€‚\n",
    "3. ç»è¿‡å¤šå±‚çš„ Self-Attentionï¼Œå¾—åˆ°æ¯ä¸ªå­—æ–°çš„å‘é‡è¡¨ç¤ºã€‚è¿™ä¸€ç‚¹æˆ‘ä»¬æ¥ä¸‹æ¥è®²ã€‚\n",
    "4. Mask è¿™ä¸ªå­—çš„è¾“å‡ºå‘é‡ç»è¿‡åˆ†ç±»å™¨ä¹‹åï¼Œæˆ‘ä»¬å¾—åˆ°ä¸Šé¢è¯è¡¨ä¸­æ¯ä¸€ä¸ªå­—æ˜¯è¿™ä¸ªè¢«é®ä½è¯çš„æ¦‚ç‡ã€‚\n",
    "\n",
    "è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶ã€Œæ„æ€ã€æ˜¯ä¸€ä¸ªè¯ï¼Œä¸åº”è¯¥åˆ†å¼€ï¼Œä½†æ˜¯ä¸­æ–‡åˆ†è¯å¾ˆéš¾ï¼Œè€Œä¸”å®¹æ˜“å‡ºé”™ã€‚BERT çš„å¼ºå¤§ä¹‹å¤„åœ¨äºï¼šå³ä½¿è¾“å…¥çš„æ˜¯å•ç‹¬çš„ã€Œæ„ã€å’Œã€Œæ€ã€ï¼Œç»è¿‡ Attention æœºåˆ¶åï¼Œã€Œæ„ã€çš„å‘é‡ä¼šå¸æ”¶ã€Œæ€ã€çš„ä¿¡æ¯ï¼Œä»è€Œæ¥è¿‘ã€Œæ„æ€ã€è¿™ä¸ªæ¦‚å¿µçš„å‘é‡è¡¨ç¤ºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ‰‹ç®— Self-Attention \n",
    "\n",
    "ä¸ºäº†ææ‡‚ Self-Attention åˆ°åº•æ˜¯æ€ä¹ˆç®—çš„ï¼Œæˆ‘æŠ›å¼€å¤æ‚çš„å…¬å¼ï¼Œä¸¾ä¸€ä¸ªæœ€ç®€å•çš„ä¾‹å­ã€‚\n",
    "\n",
    "**åœºæ™¯è®¾å®šï¼š**\n",
    "* è¾“å…¥åªæœ‰ä¸¤ä¸ªè‹±æ–‡è¯ï¼š**one cat**ã€‚\n",
    "* å‡è®¾å‘é‡ç»´åº¦ ($d_{model}$) ä¸º **3**ã€‚\n",
    "* è¾“å…¥å‘é‡ ($X$) å¦‚ä¸‹ï¼š\n",
    "    - one: `[1, 0, 2]`\n",
    "    - cat: `[-1, 2, 0]`\n",
    "  \n",
    "å³ï¼š\n",
    "\n",
    "$$ X = \\begin{bmatrix} 1 & 0 & 2 \\\\ -1 &2 & 0 \\end{bmatrix} $$\n",
    "\n",
    "### ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡æƒé‡çŸ©é˜µ (Q, K, V)\n",
    "\n",
    "Self-Attention éœ€è¦ä¸‰ä¸ªæƒé‡çŸ©é˜µ $W^Q, W^K, W^V$ã€‚ä¸ºäº†æ–¹ä¾¿æ‰‹ç®—ï¼Œæˆ‘å‡è®¾æ¨¡å‹ç›®å‰å­¦åˆ°çš„æƒé‡çŸ©é˜µå¦‚ä¸‹ã€‚ä¸ºäº†æ¼”ç¤ºæ•ˆæœï¼Œæˆ‘ç‰¹æ„è®¾è®¡äº†ç®€å•çš„æ•´æ•°çŸ©é˜µï¼š\n",
    "\n",
    "$$\n",
    "W^Q = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad\n",
    "W^K = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}, \\quad\n",
    "W^V = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### ç¬¬äºŒæ­¥ï¼šè®¡ç®— Q, K, V å‘é‡\n",
    "\n",
    "è®¡ç®—å…¬å¼ä¸ºï¼šå‘é‡ $\\times$ çŸ©é˜µã€‚\n",
    "\n",
    "**1. è®¡ç®— Query (Q)**\n",
    "$$Q = X \\times W^Q = \\begin{bmatrix} 1 & 0 & 2 \\\\ -1 & 2 & 0 \\end{bmatrix}$$\n",
    "* $q_{\\text{one}} = [1, 0, 2]$\n",
    "* $q_{\\text{cat}} = [-1, 2, 0]$\n",
    "\n",
    "**2. è®¡ç®— Key (K)**\n",
    "$$K = X \\times W^K = \\begin{bmatrix} 1 & 2 & 0 \\\\ -1 & 0 & 2 \\end{bmatrix}$$\n",
    "* $k_{\\text{one}} = [1, 2, 0]$  \n",
    "* $k_{\\text{cat}} = [-1, 0, 2]$\n",
    "\n",
    "**3. è®¡ç®— Value (V)**\n",
    "$$V = X \\times W^V = \\begin{bmatrix} 2 & 0 & 4 \\\\ -2 & 4 & 0 \\end{bmatrix}$$\n",
    "* $v_{\\text{one}} = [2, 0, 4]$\n",
    "* $v_{\\text{cat}} = [-2, 4, 0]$\n",
    "\n",
    "### ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (Attention Score)\n",
    "\n",
    "è¿™ä¸€æ­¥æ˜¯è®¡ç®—ç›¸ä¼¼åº¦ã€‚æˆ‘ä»¬æ‹¿ **Query** å»ç‚¹ç§¯ **Key** çš„è½¬ç½® ($Q \\cdot K^T$)ã€‚\n",
    "\n",
    "$$\n",
    "\\text{Score} = \\begin{bmatrix}\n",
    "q_{\\text{one}} \\cdot k_{\\text{one}} & q_{\\text{one}} \\cdot k_{\\text{cat}} \\\\\n",
    "q_{\\text{cat}} \\cdot k_{\\text{one}} & q_{\\text{cat}} \\cdot k_{\\text{cat}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**å…·ä½“è®¡ç®—è¿‡ç¨‹ï¼š**\n",
    "\n",
    "* **One å¯¹ One**: $1\\times1 + 0\\times2 + 2\\times0 = \\mathbf{1}$\n",
    "* **One å¯¹ Cat**: $1\\times(-1) + 0\\times0 + 2\\times2 = -1 + 4 = \\mathbf{3}$\n",
    "* **Cat å¯¹ One**: $(-1)\\times1 + 2\\times2 + 0\\times0 = -1 + 4 = \\mathbf{3}$\n",
    "* **Cat å¯¹ Cat**: $(-1)\\times(-1) + 2\\times0 + 0\\times2 = \\mathbf{1}$\n",
    "\n",
    "**å¾—åˆ°åˆ†æ•°çŸ©é˜µï¼š**\n",
    "$$\n",
    "\\text{Scores} = \\begin{bmatrix} 1 & 3 \\\\ 3 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### ç¬¬å››æ­¥ï¼šç¼©æ”¾ (Scaling)\n",
    "\n",
    "ä¸ºäº†é˜²æ­¢æ•°å€¼è¿‡å¤§ï¼Œæˆ‘ä»¬éœ€è¦é™¤ä»¥ $\\sqrt{d_k}$ã€‚è¿™é‡Œ $\\sqrt{3} \\approx 1.732$ã€‚\n",
    "\n",
    "$$\n",
    "\\text{Scaled Scores} = \\begin{bmatrix} 1/1.732 & 3/1.732 \\\\ 3/1.732 & 1/1.732 \\end{bmatrix} \\approx \\begin{bmatrix} 0.58 & 1.73 \\\\ 1.73 & 0.58 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### ç¬¬äº”æ­¥ï¼šå½’ä¸€åŒ– (Softmax)\n",
    "\n",
    "æˆ‘ä»¬è¦æŠŠåˆ†æ•°å˜æˆæ¦‚ç‡ï¼Œå³å¯¹æ¯ä¸€è¡Œåš Softmaxï¼š$\\frac{e^x}{\\sum e^x}$ã€‚\n",
    "\n",
    "**ä»¥ç¬¬ä¸€è¡Œ (one) ä¸ºä¾‹ï¼š**\n",
    "* $e^{0.58} \\approx 1.79$\n",
    "* $e^{1.73} \\approx 5.64$\n",
    "* æ€»å’Œ $= 7.43$\n",
    "* **å¯¹è‡ªå·±çš„æƒé‡**: $1.79 / 7.43 \\approx \\mathbf{0.24}$\n",
    "* **å¯¹ cat çš„æƒé‡**: $5.64 / 7.43 \\approx \\mathbf{0.76}$\n",
    "\n",
    "åŒç†è®¡ç®—ç¬¬äºŒè¡Œï¼Œæœ€ç»ˆå¾—åˆ° **Attention Matrix**ï¼š\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "0.24 & 0.76 \\\\\n",
    "0.76 & 0.24\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "> **å«ä¹‰è§£è¯»**ï¼šè®¡ç®— \"one\" çš„æ–°å‘é‡æ—¶ï¼Œæ¨¡å‹è®¤ä¸ºåº”è¯¥ä¿ç•™ 24% çš„åŸæ„ï¼Œå¹¶å¸æ”¶ 76% å…³äº \"cat\" çš„ä¿¡æ¯ã€‚\n",
    "\n",
    "### ç¬¬å…­æ­¥ï¼šåŠ æƒæ±‚å’Œ (Multiply by V)\n",
    "\n",
    "æœ€åï¼Œç”¨è®¡ç®—å‡ºçš„æƒé‡å»æ··åˆ $V$ å‘é‡ã€‚\n",
    "\n",
    "**1. è®¡ç®— one çš„æ–°å‘é‡ ($Z_{\\text{one}}$)ï¼š**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_{\\text{one}} &= 0.24 \\cdot v_{\\text{one}} + 0.76 \\cdot v_{\\text{cat}} \\\\\n",
    "&= 0.24 \\cdot [2, 0, 4] + 0.76 \\cdot [-2, 4, 0] \\\\\n",
    "&= [0.48, 0, 0.96] + [-1.52, 3.04, 0] \\\\\n",
    "&= \\mathbf{[-1.04, 3.04, 0.96]}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**2. è®¡ç®— cat çš„æ–°å‘é‡ ($Z_{\\text{cat}}$)ï¼š**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_{\\text{cat}} &= 0.76 \\cdot v_{\\text{one}} + 0.24 \\cdot v_{\\text{cat}} \\\\\n",
    "&= 0.76 \\cdot [2, 0, 4] + 0.24 \\cdot [-2, 4, 0] \\\\\n",
    "&= [1.52, 0, 3.04] + [-0.48, 0.96, 0] \\\\\n",
    "&= \\mathbf{[1.04, 0.96, 3.04]}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**æœ€ç»ˆç»“æœå¯¹æ¯”ï¼š**\n",
    "\n",
    "Self-Attention å±‚çš„è¾“å‡ºçŸ©é˜µ $Z$ ä¸ºï¼š\n",
    "$$\n",
    "Z = \\begin{bmatrix}\n",
    "-1.04 & 3.04 & 0.96 \\\\\n",
    "1.04 & 0.96 & 3.04\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* **One çš„å˜åŒ–**ï¼šä» `[1, 0, 2]` å˜ä¸º `[-1.04, 3.04, 0.96]`ã€‚å®ƒå¸æ”¶äº†å¤§é‡ cat çš„ç‰¹å¾ï¼ˆç‰¹åˆ«æ˜¯ä¸­é—´ç»´åº¦å˜æˆäº† 3.04ï¼‰ã€‚\n",
    "* **Cat çš„å˜åŒ–**ï¼šä» `[-1, 2, 0]` å˜ä¸º `[1.04, 0.96, 3.04]`ã€‚å®ƒä¹Ÿå¸æ”¶äº†å¤§é‡ one çš„ç‰¹å¾ï¼ˆç‰¹åˆ«æ˜¯ç¬¬ä¸‰ç»´å˜æˆäº† 3.04ï¼‰ã€‚\n",
    "\n",
    "è¿™å°±æ˜¯ BERT è·å–ä¸Šä¸‹æ–‡èƒ½åŠ›çš„æ•°å­¦æœ¬è´¨ï¼šé€šè¿‡äº’ç›¸å…³æ³¨ï¼Œå°†åˆ«äººçš„ç‰¹å¾èå…¥è‡ªå·±çš„å‘é‡ä¸­ã€‚\n",
    "\n",
    "## å†™ä»£ç è®¡ç®— Self Attention\n",
    "\n",
    "ä»£ç å†™èµ·æ¥ä¼šç®€å•å¾ˆå¤šï¼Œæˆ‘ä»¬é¡ºä¾¿ä¹ŸéªŒè¯ä¸€ä¸‹ä¸Šé¢ç®—çš„å¯¹ä¸å¯¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from typing import List, Dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X:np.ndarray):\n",
    "    '''å¯¹æ¯ä¸€è¡Œåš softmax\n",
    "    '''\n",
    "    # p_i  = e^x_i / sum (e^x_ij). j is all items in X_i\n",
    "    # åˆ†å­åˆ†æ¯åŒæ—¶ä¹˜ä»¥ e^-max(x_i)\n",
    "    X = np.asarray(X)\n",
    "    X_max = np.max(X, axis = -1, keepdims=True)\n",
    "    exp_X = np.exp(X - X_max)\n",
    "    return exp_X / np.sum(exp_X, axis = -1, keepdims=True)\n",
    "\n",
    "def self_attention(X: np.ndarray, w_Q: np.ndarray, w_K: np.ndarray, w_V: np.ndarray):\n",
    "    # 1. è®¡ç®— Q, K, V\n",
    "    Q = X @ w_Q\n",
    "    K = X @ w_K\n",
    "    V = X @ w_V\n",
    "    \n",
    "    # 2. è·å–ç»´åº¦ d_k ç”¨äºç¼©æ”¾\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # 3. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n",
    "    attention_scores = Q @ K.T\n",
    "    \n",
    "    # 4. ç¼©æ”¾\n",
    "    scaled_scores = attention_scores / np.sqrt(d_k)\n",
    "    \n",
    "    # 5. Softmax å½’ä¸€åŒ–å¾—åˆ°æ³¨æ„åŠ›æƒé‡çŸ©é˜µ A\n",
    "    A = softmax(scaled_scores)\n",
    "    \n",
    "    # æ‰“å°ä¸­é—´ç»“æœç”¨äºå¯¹æ¯”æ•™ç¨‹\n",
    "    print(\"--- éªŒè¯ä¸­é—´ç»“æœ ---\")\n",
    "    print(f\"Scaled Scores:\\n{np.round(scaled_scores, 2)}\")\n",
    "    print(f\"\\nAttention Matrix (A):\\n{np.round(A, 2)}\")\n",
    "    \n",
    "    # 6. åŠ æƒæ±‚å’Œ\n",
    "    return A @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- è¾“å…¥ ---\n",
      "X:\n",
      "[[ 1.  0.  2.]\n",
      " [-1.  2.  0.]]\n",
      "--- éªŒè¯ä¸­é—´ç»“æœ ---\n",
      "Scaled Scores:\n",
      "[[0.58 1.73]\n",
      " [1.73 0.58]]\n",
      "\n",
      "Attention Matrix (A):\n",
      "[[0.24 0.76]\n",
      " [0.76 0.24]]\n",
      "\n",
      "--- æœ€ç»ˆè¾“å‡º (Z) ---\n",
      "[[-1.04  3.04  0.96]\n",
      " [ 1.04  0.96  3.04]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [1, 0, 2], \n",
    "    [-1, 2, 0]\n",
    "], dtype=float)\n",
    "\n",
    "w_Q = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "w_K = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "w_V = np.array([\n",
    "    [2, 0, 0],\n",
    "    [0, 2, 0],\n",
    "    [0, 0, 2]\n",
    "])\n",
    "\n",
    "print(\"--- è¾“å…¥ ---\")\n",
    "print(f\"X:\\n{X}\")\n",
    "\n",
    "output = self_attention(X, w_Q, w_K, w_V)\n",
    "\n",
    "print(\"\\n--- æœ€ç»ˆè¾“å‡º (Z) ---\")\n",
    "print(np.round(output, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬çœ‹åˆ°ï¼Œæˆ‘ä»¬ä¸Šé¢æ‰‹ç®—çš„ç»“æœæ˜¯æ­£ç¡®çš„ã€‚\n",
    "\n",
    "## BERT æ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "BERT æ˜¯ä¸€ç§ Masked Language Model (MLM)ã€‚ä»€ä¹ˆæ„æ€ï¼Ÿ\n",
    "\n",
    "æˆ‘ä»¬è¯´æœ€é‡è¦çš„ï¼šæˆ‘ä»¬ä¸ºä»€ä¹ˆç”¨ BERTï¼Ÿæœ€æ ¸å¿ƒçš„ç†ç”±æ˜¯æˆ‘ä»¬æƒ³æŠŠä¸€æ®µè¯ç”šè‡³ä¸€æ•´ç¯‡æ–‡ç« è½¬åŒ–æˆä¸Šä¸‹æ–‡ç›¸å…³çš„ Embeddingã€‚æˆ–è€…è¯´ï¼Œæˆ‘ä»¬éœ€è¦çš„æ˜¯æœ€åæˆ‘ä»¬ç®—å‡ºæ¥çš„ $Z$ã€‚\n",
    "\n",
    "é‚£ä¸Šé¢å‘Šè¯‰æˆ‘ä»¬äº†ï¼Œä¸ºäº†å¾—åˆ° $Z$ï¼Œæˆ‘ä»¬éœ€è¦ $X$, $W^Q$, $W^K$, $W^V$ã€‚ä½†è¿™äº›å¹¶ä¸æ˜¯å¹³ç™½æ— æ•…å°±å‡ºç°äº†ã€‚å¦‚ä½•å¾—åˆ°ï¼Ÿæ¨¡å‹è®­ç»ƒã€‚\n",
    "\n",
    "æˆ‘ä»¬é¦–å…ˆå¯¹ $W^Q$, $W^K$, $W^V$ è¿›è¡Œéšæœºåˆå§‹åŒ–ã€‚é‚£æ€ä¹ˆè®­ç»ƒå‘¢ï¼Ÿ\n",
    "\n",
    "æˆ‘ç”¨ Claude ç”Ÿæˆäº† [100 ä¸ªç®€å•çš„è‹±æ–‡å¥å­]('./files/simple_sentences.txt)ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¾—åˆ°æ‰€æœ‰å•ç‹¬çš„è¯ï¼Œåˆ¶ä½œä¸€ä¸ª TOKEN-EMBEDDING æŸ¥æ‰¾è¡¨ï¼Œè¿™é‡Œè¦åŠ ä¸Šä¸€ä¸ª `[MASK]`ã€‚æˆ‘ä»¬æŠŠè¿™ä¸ªè¡¨å«åš $E$ï¼Œä¹Ÿæ˜¯éšæœºåˆå§‹åŒ–ã€‚\n",
    "\n",
    "$$E \\in \\mathbb{R}^{m \\times d}$$\n",
    "\n",
    "å…¶ä¸­ $m$ æ˜¯è¯è¡¨å¤§å°ï¼Œ$d$ æ˜¯ embedding çš„ç»´åº¦ã€‚\n",
    "\n",
    "### è®­ç»ƒæµç¨‹\n",
    "\n",
    "æˆ‘ä»¬çœ‹ç¬¬ä¸€å¥è¯ï¼š`the cat sits on the mat`ã€‚æˆ‘ä»¬æŠŠ `sits` é®ä½ï¼Œå˜æˆ `the cat [MASK] on the mat`ï¼Œç”¨æ¥è®­ç»ƒã€‚\n",
    "\n",
    "è¿™å¥è¯ç»è¿‡ Self-Attention è®¡ç®—åï¼Œå¾—åˆ° $Z$ã€‚æˆ‘ä»¬é‡ç‚¹çœ‹ $Z_{\\text{mask}}$ï¼Œå®ƒåŒ…å«äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚\n",
    "\n",
    "æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ï¼Œè®©æ¨¡å‹æ¥é¢„æµ‹è¿™ä¸ªè¢«é®ä½çš„è¯æ˜¯ä»€ä¹ˆã€‚å‡†ç¡®æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—è¯è¡¨ä¸­æ¯ä¸€ä¸ªè¯æ˜¯è¿™ä¸ªè¢«é®ä½è¯çš„æ¦‚ç‡ï¼Œç„¶åæ‰¾åˆ° `sits` çš„æ¦‚ç‡ï¼Œè®¡ç®— lossï¼Œæœ€ååå‘ä¼ æ’­ã€‚\n",
    "\n",
    "### ä» $Z_{\\text{mask}}$ åˆ°é¢„æµ‹\n",
    "\n",
    "åœ¨çœŸå®çš„ BERT è®­ç»ƒä¸­ï¼Œç›´æ¥å¾—åˆ°çš„ $Z_{\\text{mask}}$ å¹¶ä¸ä¼šç›´æ¥ç”¨æ¥åšé¢„æµ‹ï¼Œè€Œæ˜¯è¦ç»è¿‡ä¸€äº›å¤„ç†ã€‚è¿™ä¹ˆåšçš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬æœ‰æ›´å¤šçš„å‚æ•°å¯ä»¥è°ƒèŠ‚ï¼Œè¿™æ ·å¯ä»¥è®©ç»“æœæ›´å‡†ç¡®ã€‚\n",
    "\n",
    "#### ç¬¬ä¸€æ­¥ï¼šç‰¹å¾æ•´ç†ï¼ˆRefinementï¼‰\n",
    "\n",
    "å¾—åˆ° $Z_{\\text{mask}}$ ä¹‹åï¼Œæˆ‘ä»¬å…ˆåšä¸€ç‚¹ç‰¹å¾æ•´ç†ï¼š\n",
    "\n",
    "$$Z_{\\text{refined}} = \\text{Activation}(Z_{\\text{mask}} \\cdot W_{\\text{dense}} + b_{\\text{dense}})$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $Z_{\\text{mask}} \\in \\mathbb{R}^{1 \\times d}$ï¼ˆ[MASK] ä½ç½®çš„è¾“å‡ºå‘é‡ï¼‰\n",
    "- $W_{\\text{dense}} \\in \\mathbb{R}^{d \\times d}$ï¼ˆå…¨è¿æ¥å±‚æƒé‡ï¼‰\n",
    "- $b_{\\text{dense}} \\in \\mathbb{R}^{1 \\times d}$ï¼ˆå…¨è¿æ¥å±‚åç½®ï¼‰\n",
    "- Activation é€šå¸¸æ˜¯ GELU æˆ– ReLU\n",
    "\n",
    "æœ€åçš„ç»“æœ $Z_{\\text{refined}} \\in \\mathbb{R}^{1 \\times d}$ã€‚\n",
    "\n",
    "#### ç¬¬äºŒæ­¥ï¼šè®¡ç®— Logits\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬è¦è€ƒå¯Ÿï¼š$Z_{\\text{refined}}$ åˆ°åº•åƒè°ï¼Ÿ\n",
    "\n",
    "æˆ‘ä»¬æŠŠ $Z_{\\text{refined}}$ æ‹¿å»å’Œå­—å…¸ $E$ é‡Œçš„æ¯ä¸€ä¸ªè¯æ¯”å¯¹ï¼š\n",
    "\n",
    "$$\\text{Logits} = Z_{\\text{refined}} \\cdot E^T + b_{\\text{vocab}}$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $E \\in \\mathbb{R}^{m \\times d}$ï¼ˆEmbedding Matrixï¼Œå°±æ˜¯è¯è¡¨ï¼‰\n",
    "- $E^T \\in \\mathbb{R}^{d \\times m}$ï¼ˆE çš„è½¬ç½®ï¼‰\n",
    "- $b_{\\text{vocab}} \\in \\mathbb{R}^{1 \\times m}$ï¼ˆè¯è¡¨åç½®ï¼‰\n",
    "- $\\text{Logits} \\in \\mathbb{R}^{1 \\times m}$ï¼ˆæ¯ä¸ªè¯çš„åŸå§‹åˆ†æ•°ï¼‰\n",
    "\n",
    "ä¸ºä»€ä¹ˆè¦ä¹˜ä»¥ $E^T$ï¼Ÿ\n",
    "\n",
    "$E$ æ˜¯é‚£æœ¬å­—å…¸ï¼ˆæ‰€æœ‰è¯çš„å‘é‡ï¼‰ã€‚$Z_{\\text{refined}}$ æ˜¯æ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡\"çŒœ\"å‡ºæ¥çš„å‘é‡ã€‚\n",
    "\n",
    "$Z_{\\text{refined}} \\cdot E^T$ æœ¬è´¨ä¸Šæ˜¯åœ¨åšç‚¹ç§¯ï¼ˆDot Productï¼‰ï¼Œä¹Ÿå°±æ˜¯è®¡ç®—ç›¸ä¼¼åº¦ã€‚æ¨¡å‹åœ¨é—®ï¼š\"æˆ‘çŒœå‡ºæ¥çš„è¿™ä¸ªå‘é‡ï¼Œè·Ÿå­—å…¸é‡Œçš„ `cat` åƒä¸åƒï¼Ÿè·Ÿ `sits` åƒä¸åƒï¼Ÿè·Ÿ `mat` åƒä¸åƒï¼Ÿ\"\n",
    "\n",
    "$b_{\\text{vocab}}$ åœ¨è¿™é‡Œæœ‰ç”¨æ˜¯å› ä¸ºæœ‰çš„è¯æœ¬èº«å°±å¾ˆå¸¸è§ï¼Œæ¯”å¦‚ an, the, a, for ç­‰ã€‚å®ƒèµ·åˆ°çš„ä½œç”¨å°±åƒä¸€ä¸ªè´å¶æ–¯ç»Ÿè®¡é‡Œçš„å…ˆéªŒæ¦‚ç‡ã€‚\n",
    "\n",
    "#### ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—æ¦‚ç‡åˆ†å¸ƒ\n",
    "\n",
    "å¯¹ logits åš softmaxï¼Œå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒï¼š\n",
    "\n",
    "$$P = \\text{softmax}(\\text{Logits}) = \\frac{e^{\\text{Logits}_i}}{\\sum_{j=1}^{m} e^{\\text{Logits}_j}}$$\n",
    "\n",
    "ç°åœ¨ $P$ ä¸­çš„æ¯ä¸ªå€¼è¡¨ç¤ºå¯¹åº”è¯æ˜¯è¢«é®ä½è¯çš„æ¦‚ç‡ï¼Œæ‰€æœ‰æ¦‚ç‡åŠ èµ·æ¥ç­‰äº 1ã€‚\n",
    "\n",
    "#### ç¬¬å››æ­¥ï¼šè®¡ç®— Loss\n",
    "\n",
    "ä½¿ç”¨äº¤å‰ç†µæŸå¤±ã€‚å‡è®¾æ­£ç¡®ç­”æ¡ˆ `sits` åœ¨è¯è¡¨ä¸­çš„ç´¢å¼•æ˜¯ $k$ï¼š\n",
    "\n",
    "$$\\text{Loss} = -\\log(P_k)$$\n",
    "\n",
    "å°±æ˜¯è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œåªçœ‹æ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ã€‚\n",
    "\n",
    "### åå‘ä¼ æ’­ï¼šæ›´æ–°æ‰€æœ‰å‚æ•°\n",
    "\n",
    "è®¡ç®—å®Œ Loss åï¼Œæˆ‘ä»¬é€šè¿‡åå‘ä¼ æ’­æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚éœ€è¦æ›´æ–°çš„å‚æ•°æœ‰ï¼š\n",
    "\n",
    "| å‚æ•° | å½¢çŠ¶ | ä½œç”¨ |\n",
    "|------|------|------|\n",
    "| $E$ | $m \\times d$ | Embedding Matrixï¼Œè¯å‘é‡æŸ¥æ‰¾è¡¨ |\n",
    "| $W^Q$ | $d \\times d$ | è®¡ç®— Query |\n",
    "| $W^K$ | $d \\times d$ | è®¡ç®— Key |\n",
    "| $W^V$ | $d \\times d$ | è®¡ç®— Value |\n",
    "| $W_{\\text{dense}}$ | $d \\times d$ | ç‰¹å¾æ•´ç†å±‚æƒé‡ |\n",
    "| $b_{\\text{dense}}$ | $1 \\times d$ | ç‰¹å¾æ•´ç†å±‚åç½® |\n",
    "| $b_{\\text{vocab}}$ | $1 \\times m$ | è¯è¡¨åç½® |\n",
    "\n",
    "æ³¨æ„ï¼šè®¡ç®— Logits æ—¶ç”¨çš„ $E^T$ å°±æ˜¯ Embedding Matrix çš„è½¬ç½®ï¼Œä¸æ˜¯é¢å¤–çš„å‚æ•°ã€‚è¿™å«åš **weight tying**ï¼ˆæƒé‡ç»‘å®šï¼‰ï¼Œè¾“å…¥å’Œè¾“å‡ºå…±äº«åŒä¸€å¥—è¯å‘é‡ã€‚\n",
    "\n",
    "æ›´æ–°è§„åˆ™ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰ï¼š\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial \\theta}$$\n",
    "\n",
    "å…¶ä¸­ $\\theta$ ä»£è¡¨ä»»æ„å‚æ•°ï¼Œ$\\eta$ æ˜¯å­¦ä¹ ç‡ã€‚\n",
    "\n",
    "### è®­ç»ƒå¾ªç¯\n",
    "\n",
    "é‡å¤ä»¥ä¸‹æ­¥éª¤ï¼Œç›´åˆ° Loss æ”¶æ•›ï¼š\n",
    "1. éšæœºé€‰ä¸€ä¸ªå¥å­\n",
    "2. éšæœºé®ä½ä¸€ä¸ªè¯ï¼Œè®°å½•æ­£ç¡®ç­”æ¡ˆ\n",
    "3. å‰å‘ä¼ æ’­ï¼š\n",
    "   - æŸ¥ Embedding Matrix å¾—åˆ° X\n",
    "     - è®¡ç®— Self-Attention å¾—åˆ° Z\n",
    "     - å– Z_maskï¼Œåšç‰¹å¾æ•´ç†å¾—åˆ° Z_refined\n",
    "     - å’Œ E^T åšç‚¹ç§¯å¾—åˆ° Logits\n",
    "     - Softmax å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ\n",
    "4. è®¡ç®— Loss\n",
    "5. åå‘ä¼ æ’­ï¼šè®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦\n",
    "6. æ›´æ–°å‚æ•°\n",
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹å­¦ä¼šäº†æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹è¢«é®ä½çš„è¯ï¼ŒåŒæ—¶ Embedding Matrix å’Œ Self-Attention å‚æ•°éƒ½å˜å¾—æœ‰æ„ä¹‰äº†ã€‚\n",
    "\n",
    "## è®­ç»ƒä»£ç \n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å§‹å®æˆ˜å†™ä»£ç ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå› ä¸ºæ¢¯åº¦è®¡ç®—éœ€è¦ç”¨åˆ°çŸ©é˜µæ±‚å¯¼ï¼Œæˆ‘ä»¬æš‚ä¸”ä¸å»ç®¡æ•°å­¦ï¼Œè€Œæ˜¯ç”¨ PyTorch è‡ªåŠ¨æ±‚å¯¼ã€‚ä¹‹ååœ¨è¿›é˜¶ç‰ˆï¼Œæˆ‘ä»¬ä¼šå†™æ¸…æ¥šæ¯ä¸€æ­¥çš„æ¢¯åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import re \n",
    "import torch.nn as nn\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯è¡¨å¤§å°: 89\n",
      "æ‰€æœ‰çš„è¯: ['a', 'an', 'apple', 'at', 'ball', 'barks', 'bed', 'bench', 'bird', 'book', 'boy', 'branch', 'bread', 'by', 'cat', 'catches', 'chair', 'chases', 'child', 'clouds', 'coffee', 'digs', 'dog', 'doll', 'door', 'drinks', 'east', 'eats', 'fire', 'fish', 'flies', 'float', 'from', 'garden', 'girl', 'grass', 'hide', 'hides', 'hill', 'house', 'in', 'juice', 'jumps', 'man', 'mat', 'meat', 'milk', 'moon', 'mouse', 'night', 'on', 'over', 'paper', 'park', 'plays', 'reads', 'rises', 'river', 'rock', 'runs', 'school', 'sea', 'seed', 'sets', 'shine', 'shines', 'sings', 'sits', 'sky', 'sleeps', 'stars', 'store', 'sun', 'swims', 'table', 'tea', 'the', 'to', 'tree', 'under', 'walks', 'wall', 'watches', 'water', 'west', 'window', 'with', 'woman', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename:str):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower() # å…¨éƒ¨è½¬å°å†™\n",
    "    \n",
    "    # å»æ ‡ç‚¹ç¬¦å·ï¼Œåªä¿ç•™å•è¯\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    sentences = text.strip().split('\\n')\n",
    "\n",
    "    # å»ºç«‹è¯è¡¨\n",
    "    words = text.split()\n",
    "    unique_words = sorted(list(set(words)))\n",
    "    unique_words += ['[MASK]'] \n",
    "\n",
    "    word2id = {w:i for i, w in enumerate(unique_words)}\n",
    "    id2word = {i:w for i, w in enumerate(unique_words)}\n",
    "\n",
    "    return sentences, word2id, id2word \n",
    "\n",
    "sentences, word2id, id2word = load_data('data/simple_sentences.txt')\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "print(f\"è¯è¡¨å¤§å°: {vocab_size}\")\n",
    "print(f\"æ‰€æœ‰çš„è¯: {list(word2id.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBert(nn.Module):\n",
    "    def __init__(self, vocab_size:int, d_model:int):\n",
    "        super().__init__()\n",
    "        # æŠŠåˆå§‹åŒ–çš„æ–¹å·®è°ƒå°ï¼Œä¹˜ä»¥ 0.01 æˆ–è€… 0.1\n",
    "        scale = 0.01\n",
    "\n",
    "        # 1. Token Embedding è¡¨\n",
    "        self.E = nn.Parameter(\n",
    "            torch.randn(vocab_size, d_model) * scale\n",
    "        )\n",
    "        # è¿™é‡Œéœ€è¦å¼ºè°ƒä¸€ä¸‹ï¼ŒE å…¶å®æ˜¯æœ‰é¡ºåºçš„ï¼Œå°±æ˜¯ word2id é‡Œçš„é¡ºåº\n",
    "        # å› ä¸ºä¹‹åæˆ‘ä»¬ç”¨åˆ° input_ids = [word2id[w] for w in words]\n",
    "\n",
    "        # 2. Attention æƒé‡\n",
    "        self.W_Q = nn.Parameter(\n",
    "            torch.randn(d_model, d_model) * scale\n",
    "        )\n",
    "        self.W_K = nn.Parameter(\n",
    "            torch.randn(d_model, d_model) * scale\n",
    "        )\n",
    "        self.W_V = nn.Parameter(\n",
    "            torch.randn(d_model, d_model) * scale\n",
    "        )\n",
    "\n",
    "        # 3. MLM Head ä¸­é—´æ˜ å°„\n",
    "        self.W_dense = nn.Parameter(\n",
    "            torch.randn(d_model, d_model) * scale\n",
    "        )\n",
    "        self.b_dense = nn.Parameter(\n",
    "            torch.zeros(d_model)\n",
    "        )\n",
    "\n",
    "        # 4. vocab biasï¼ˆembedding æƒé‡å…±äº«ï¼‰\n",
    "        self.b_vocab = nn.Parameter(\n",
    "            torch.zeros(vocab_size)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1) # BERT æ ‡å‡†æ˜¯ 0.1 (10%)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def bert_forward(self, input_ids):\n",
    "        X = self.E[input_ids] # Shape: [seq_len, d_model]\n",
    "\n",
    "        # 2. è®¡ç®— Q, K, V\n",
    "        Q = X @ self.W_Q # Shape: [seq_len, d_model]\n",
    "        K = X @ self.W_K # Shape: [seq_len, d_model]\n",
    "        V = X @ self.W_V # Shape: [seq_len, d_model]\n",
    "\n",
    "        # 3. Self Attention è®¡ç®—\n",
    "        d_k = Q.shape[-1]\n",
    "        # transpose(-2, -1) å€’æ•°ç¬¬2ä¸ªç»´åº¦ å’Œ å€’æ•°ç¬¬1ä¸ªç»´åº¦ äº’æ¢\n",
    "        # è¿™æ˜¯ä¸ºäº†åº”å¯¹ä¹‹å torch çš„ batchã€‚ä½†è¿™é‡Œçš„è¯ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ Q @ K.T\n",
    "        scores = Q @ K.transpose(-2, -1)\n",
    "        # scores = Q @ K.T \n",
    "        # Shape: [seq_len, seq_len]\n",
    "        scaled_scores = scores/torch.sqrt(torch.tensor(d_k)) \n",
    "        A = torch.softmax(scaled_scores, dim = -1) # Shape: [seq_len, seq_len]\n",
    "\n",
    "        Z = A @ V # (seq_len, d_model)\n",
    "\n",
    "        # å¯¹ Attention çš„ç»“æœåšä¸€æ¬¡ Dropout\n",
    "        Z = self.dropout(Z) # \n",
    "\n",
    "        # æ®‹å·®è¿æ¥ + LayerNorm\n",
    "        # å…¬å¼çš„æœ¬è´¨ï¼šOutput = Norm(Input + Attention(Input))\n",
    "        # è¿™è®©æ¢¯åº¦æœ‰äº†â€œé«˜é€Ÿå…¬è·¯â€ï¼Œå¯ä»¥ç›´æ¥æµå› Input\n",
    "        Z = self.layer_norm(Z + X) # (seq_len, d_model)\n",
    "\n",
    "        # 4. Refinement (Dense Layer + Activation)\n",
    "        # Z_refined = GELU(Z * W + b)\n",
    "        # Shape: (seq_len, d_model)\n",
    "        Z_refined = torch.nn.functional.gelu(Z @ self.W_dense + self.b_dense)\n",
    "        \n",
    "        # 5. Output Logits (Weight Tying)\n",
    "        # å¤ç”¨ E çš„è½¬ç½®è¿›è¡Œé¢„æµ‹\n",
    "        # å› ä¸º E æ˜¯ word2id çš„é¡ºåºï¼Œæ‰€ä»¥ logits ä¹Ÿæ˜¯ word2id çš„é¡ºåº\n",
    "        logits = Z_refined @ self.E.T + self.b_vocab # Shape: (seq_len, vocab_size)\n",
    "        \n",
    "        return logits, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vocab_size:int, d_model:int, learning_rate:float, epoches:int):\n",
    "    model = MiniBert(vocab_size=vocab_size, d_model=d_model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        total_loss = 0 \n",
    "        for sent in sentences:\n",
    "            words = sent.split()\n",
    "            if len(words) < 2: continue # å°‘äºä¸¤ä¸ªè¯ï¼Œæˆ‘ä»¬ç›´æ¥ç•¥è¿‡\n",
    "\n",
    "            # 1. æŠŠæ–‡å­—è½¬æˆ ID\n",
    "            input_ids = [word2id[w] for w in words]\n",
    "            \n",
    "            # 2. éšæœºé€‰ä¸€ä¸ªä½ç½®é®ä½\n",
    "            mask_pos = np.random.randint(0, len(words))\n",
    "            target_word_id = input_ids[mask_pos] # æ­£ç¡®ç­”æ¡ˆ\n",
    "            \n",
    "            # 3. æŠŠè¾“å…¥é‡Œçš„è¿™ä¸ªä½ç½®æ›¿æ¢æˆ [MASK]\n",
    "            input_ids[mask_pos] = word2id['[MASK]']\n",
    "            \n",
    "            # è½¬æˆ Tensor å–‚ç»™æ¨¡å‹\n",
    "            input_tensor = torch.tensor(input_ids) \n",
    "\n",
    "            # --- å‰å‘ä¼ æ’­ ---\n",
    "            optimizer.zero_grad() # æ¸…ç©ºä¸Šä¸€æ­¥çš„æ¢¯åº¦\n",
    "            \n",
    "            logits, _ = model.bert_forward(input_tensor)\n",
    "\n",
    "            # ==========================================\n",
    "            #   è®¡ç®— Loss å¼€å§‹\n",
    "            # ==========================================\n",
    "\n",
    "            # 1. é”å®šä½ç½®ï¼šæˆ‘ä»¬åªå…³å¿ƒ [MASK] è¿™ä¸ªä½ç½®çš„é¢„æµ‹ç»“æœ\n",
    "            # mask_logits æ˜¯ä¸€ä¸ªé•¿é•¿çš„å‘é‡ï¼Œä»£è¡¨æ¯ä¸ªè¯çš„åˆ†æ•°\n",
    "            mask_logits = logits[mask_pos] \n",
    "            \n",
    "            # 2. æ‰‹åŠ¨ Softmaxï¼šæŠŠåˆ†æ•°å˜æˆæ¦‚ç‡ (0.0 ~ 1.0)\n",
    "            # dim=0 è¡¨ç¤ºå¯¹è¿™ä¸€ä¸ªå‘é‡å†…éƒ¨åšå½’ä¸€åŒ–\n",
    "            probs = torch.softmax(mask_logits, dim=0)\n",
    "            \n",
    "            # 3. é”å®šç­”æ¡ˆï¼šæŠŠâ€œæ­£ç¡®ç­”æ¡ˆâ€å¯¹åº”çš„é‚£ä¸ªæ¦‚ç‡æŠ å‡ºæ¥\n",
    "            # æ¯”å¦‚ target_word_id æ˜¯ 5ï¼Œæˆ‘ä»¬å°±çœ‹ç¬¬ 5 ä¸ªæ¦‚ç‡æ˜¯å¤šå°‘\n",
    "            target_prob = probs[target_word_id]\n",
    "            \n",
    "            # 4. è®¡ç®— Lossï¼š-log(æ¦‚ç‡)\n",
    "            # cross entropy loss: - sum (y_i * log(p_i))\n",
    "            # æ¦‚ç‡è¶Šæ¥è¿‘ 1ï¼Œlog åè¶Šæ¥è¿‘ 0ï¼Œloss è¶Šå°\n",
    "            # åŠ ä¸€ä¸ªæå°å€¼ 1e-9 æ˜¯ä¸ºäº†é˜²æ­¢ target_prob ä¸º 0 æ—¶ log æŠ¥é”™\n",
    "            loss = -torch.log(target_prob + 1e-9)\n",
    "\n",
    "            # ==========================================\n",
    "            #   è®¡ç®— Loss ç»“æŸ\n",
    "            # ==========================================\n",
    "\n",
    "            # --- åå‘ä¼ æ’­ & æ›´æ–°å‚æ•° ---\n",
    "            loss.backward()  # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨ç®—å‡º W_Q, E ç­‰æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦\n",
    "            optimizer.step() # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨æ‰§è¡Œ param -= lr * grad\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss/len(sentences):.4f}\")\n",
    "        \n",
    "    print(\"è®­ç»ƒå®Œæˆï¼\")\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.8556\n",
      "Epoch 50, Loss: 1.2090\n",
      "Epoch 100, Loss: 0.6556\n",
      "Epoch 150, Loss: 0.2926\n",
      "Epoch 200, Loss: 0.2074\n",
      "Epoch 250, Loss: 0.1870\n",
      "Epoch 300, Loss: 0.1895\n",
      "Epoch 350, Loss: 0.2070\n",
      "Epoch 400, Loss: 0.0788\n",
      "Epoch 450, Loss: 0.1339\n",
      "è®­ç»ƒå®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(\n",
    "    vocab_size=vocab_size, \n",
    "    d_model=128, \n",
    "    learning_rate=0.001, \n",
    "    epoches=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œæœ‰ä¸å°‘å€¼å¾—è¯´çš„ç‚¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸Šé¢åªæ˜¯ç”¨åˆ°äº† $Z_\\text{mask}$ ä½†æ˜¯æˆ‘ä»¬åœ¨ä¸Šé¢çš„è®­ç»ƒä»£ç ä¸­ï¼Œå´æŠŠå…¨éƒ¨çš„ $Z$ éƒ½ç”¨èµ·æ¥äº†ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ\n",
    "\n",
    "- GPU è®¡ç®—çš„æ˜¯å¹¶è¡Œè®¡ç®—ï¼Œæ‰€ä»¥åªç®— $Z_\\text{mask}$ å’Œå…¨éƒ¨çš„ $Z$ éƒ½ç®—ï¼Œåœ¨è®¡ç®—èµ„æºçš„ä½¿ç”¨é‡ä¸Šå¹¶æ²¡æœ‰æ˜¾è‘—å·®å¼‚ã€‚\n",
    "- åœ¨çœŸå®çš„ BERT è®­ç»ƒä¸­ï¼Œå¹¶ä¸åªæ˜¯éšæœºé®ä½ä¸€ä¸ªå­—ï¼Œè€Œæ˜¯å¤šä¸ªå­—ã€‚\n",
    "- åœ¨çœŸå®çš„ BERT è®­ç»ƒä¸­ï¼ŒSelf Attention æœ‰å¾ˆå¤šå±‚ï¼Œæ¯”å¦‚ 12 å±‚ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€å±‚ã€‚é‚£ç¬¬äºŒå±‚çš„è¾“å…¥æ˜¯ $Z$ï¼Œè€Œä¸æ˜¯ $Z_\\text{mask}$ã€‚\n",
    "  \n",
    "å¦å¤–ï¼Œæˆ‘ä»¬ç”¨åˆ°äº† dropoutï¼š`Z = self.dropout(Z)`ã€‚è¿™ä¸ªå…¶å®å¾ˆå¥½ç†è§£ã€‚é¦–å…ˆï¼Œ$Z$ çš„ç»´åº¦æ˜¯ $m \\times d$ã€‚Dropout æ˜¯è¯´ï¼Œæ¯ä¸€ä¸ªå…ƒç´ ï¼Œæˆ–è€…è¯´ï¼Œæ¯ä¸€è¡Œï¼ˆå³ï¼Œæ¯ä¸€ä¸ªå­—ï¼‰çš„æ¯ä¸€ä¸ªæ•°å­—ï¼Œéƒ½æœ‰ç™¾åˆ†ä¹‹åï¼ˆè¿™æ˜¯ä¸€ä¸ªå‚æ•°ï¼Œå¯ä»¥è°ƒèŠ‚ï¼‰çš„æ¦‚ç‡å˜ä¸º 0ã€‚ç„¶åå‘¢ï¼Œæ¯ä¸€è¡Œï¼Œæˆ‘ä»¬è¦ç¡®ä¿ dropout å‰åçš„æœŸæœ›å€¼ä¸å˜ï¼Œé‚£å°±éœ€è¦æŠŠåˆ«çš„æ•°å­—æ”¹å˜ä¸€ä¸‹ã€‚å…¶å®å°±æ˜¯æ¯ä¸ªæ•°å­—é™¤ä»¥ $0.9$ å°±å¯ä»¥ã€‚è¿™å°±æ˜¯æ‰€è°“çš„ inverted dropout. è¿™ä¹ˆåšæœ‰ä»€ä¹ˆç”¨ï¼Ÿå®ƒå¯ä»¥é€¼è¿«æ¨¡å‹åœ¨ç¼ºå°‘æ•°æ®çš„æƒ…å†µä¸‹æ‹¼å°½å…¨åŠ›å»åšå¥½é¢„æµ‹ã€‚\n",
    "\n",
    "æˆ‘ä»¬è¿˜ç”¨åˆ°äº† `Z = self.layer_norm(Z + X)`ã€‚å®ƒçš„æœ¬è´¨æ˜¯ Output = Norm(Input + Attention(Input)). è¿™æ˜¯ä½•å‡¯æ˜å¤§ç¥é‚£ç¯‡è‘—åçš„ ResNet è®ºæ–‡çš„çš„ç²¾é«“ã€‚å®ƒå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„æ‹Ÿåˆã€‚\n",
    "\n",
    "è¿˜æœ‰å°±æ˜¯æˆ‘ä»¬çœ‹åˆ° Loss å¤§è‡´æ˜¯åœ¨ä¸æ–­ä¸‹é™çš„ï¼Œä½†æ˜¯å¶å°”ä¼šæœ‰èµ·ä¼ã€‚è¿™å¾ˆæ­£å¸¸ï¼Œå› ä¸ºé®çš„è¯æ˜¯éšæœºçš„ï¼Œæœ‰æ—¶å€™å°±æ¯”è¾ƒéš¾ã€‚\n",
    "\n",
    "æˆ‘ä»¬ç°åœ¨æ¥ç”¨å‡ ä¸ªå¥å­æ¥æ£€æµ‹ä¸€ä¸‹ï¼š\n",
    "\n",
    "- The man eats the MASK\n",
    "- The man read the MASK\n",
    "- The man MASK the book\n",
    "\n",
    "æ³¨æ„ï¼Œè¿™ä¸¤å¥è¯å¹¶æ²¡æœ‰å‡ºç°åœ¨è®­ç»ƒé›†é‡Œï¼Œä½†æ˜¯æ‰€æœ‰çš„è¯éƒ½æ˜¯è®­ç»ƒæ—¶è§è¿‡çš„ã€‚æˆ‘ä»¬çœ‹çœ‹æ¨¡å‹æ˜¯å¦å¯ä»¥å‡†ç¡®é¢„æµ‹å‡ºæ­£ç¡®çš„è¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model:MiniBert, sentence:str, word2id:Dict, id2word:Dict):\n",
    "    # --- 1. å‡†å¤‡å·¥ä½œ ---\n",
    "    model.eval() # å…³é—­ Dropout\n",
    "\n",
    "    words = sentence.split()\n",
    "    try:\n",
    "        mask_idx = words.index('[MASK]')\n",
    "    except ValueError:\n",
    "        print(f\"å‡ºé”™: å¥å­ '{sentence}' é‡Œæ²¡æœ‰æ‰¾åˆ° [MASK]\")\n",
    "        return\n",
    "    \n",
    "    input_ids = [word2id.get(w,0) for w in words]\n",
    "    input_tensor = torch.tensor([input_ids]) # è½¬æˆ Tensor å¹¶å¢åŠ  Batch ç»´åº¦: [seq_len] -> [1, seq_len]\n",
    "\n",
    "    # --- 2. æ¨¡å‹æ¨ç† ---\n",
    "    with torch.no_grad(): # é¢„æµ‹æ¨¡å¼ä¸éœ€è¦ç®—æ¢¯åº¦ï¼Œçœå†…å­˜\n",
    "        logits, _ = model.bert_forward(input_tensor)\n",
    "    \n",
    "    # --- 3. æå–ç»“æœ ---\n",
    "    # logits å½¢çŠ¶: [batch=1, seq_len, vocab_size]\n",
    "    # æˆ‘ä»¬åªå…³å¿ƒ [MASK] é‚£ä¸ªä½ç½®çš„é¢„æµ‹ç»“æœ\n",
    "    # å› ä¸º E æ˜¯ word2id çš„é¡ºåºï¼Œæ‰€ä»¥ logits ä¹Ÿæ˜¯ word2id çš„é¡ºåº\n",
    "    mask_logits = logits[0, mask_idx, :] # Shape: (1, 1, vocab_size)\n",
    "\n",
    "    probs = torch.softmax(mask_logits, dim = -1)\n",
    "\n",
    "    # å–å‡ºæ¦‚ç‡æœ€å¤§çš„å‰ 5 ä¸ªè¯\n",
    "    top_k = torch.topk(probs, k=5)\n",
    "    top_ids = top_k.indices.tolist()\n",
    "    top_values = top_k.values.tolist()\n",
    "\n",
    "    # --- 4. æ‰“å°ç»“æœ ---\n",
    "    print(f\"\\nğŸ“ è¾“å…¥å¥å­: {sentence}\")\n",
    "    print(f\"ğŸ“ [MASK] ä½ç½®: ç¬¬ {mask_idx} ä¸ªè¯\")\n",
    "    print(\"ğŸ¤– æ¨¡å‹é¢„æµ‹ç»“æœ:\")\n",
    "    print(\"-\" * 30)\n",
    "    for i in range(len(top_ids)):\n",
    "        word = id2word[top_ids[i]]\n",
    "        score = top_values[i]\n",
    "        print(f\"  {i+1}. {word:<10} (ä¿¡å¿ƒ: {score:.2%})\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ è¾“å…¥å¥å­: the man eats the [MASK]\n",
      "ğŸ“ [MASK] ä½ç½®: ç¬¬ 4 ä¸ªè¯\n",
      "ğŸ¤– æ¨¡å‹é¢„æµ‹ç»“æœ:\n",
      "------------------------------\n",
      "  1. meat       (ä¿¡å¿ƒ: 99.51%)\n",
      "  2. ball       (ä¿¡å¿ƒ: 0.31%)\n",
      "  3. woman      (ä¿¡å¿ƒ: 0.06%)\n",
      "  4. bread      (ä¿¡å¿ƒ: 0.05%)\n",
      "  5. fish       (ä¿¡å¿ƒ: 0.02%)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•ç”¨ä¾‹ 1: è€ƒå¯Ÿå®¾è¯­ (Object)\n",
    "# é¢„æœŸï¼šeats åé¢åº”è¯¥æ˜¯é£Ÿç‰© (apple, fish, meat)\n",
    "predict(trained_model, \"the man eats the [MASK]\", word2id, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ è¾“å…¥å¥å­: the man read the [MASK]\n",
      "ğŸ“ [MASK] ä½ç½®: ç¬¬ 4 ä¸ªè¯\n",
      "ğŸ¤– æ¨¡å‹é¢„æµ‹ç»“æœ:\n",
      "------------------------------\n",
      "  1. book       (ä¿¡å¿ƒ: 47.85%)\n",
      "  2. meat       (ä¿¡å¿ƒ: 20.08%)\n",
      "  3. house      (ä¿¡å¿ƒ: 11.64%)\n",
      "  4. store      (ä¿¡å¿ƒ: 7.70%)\n",
      "  5. bread      (ä¿¡å¿ƒ: 3.66%)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•ç”¨ä¾‹ 2: \n",
    "predict(trained_model, \"the man read the [MASK]\", word2id, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ è¾“å…¥å¥å­: the man [MASK] the book\n",
      "ğŸ“ [MASK] ä½ç½®: ç¬¬ 2 ä¸ªè¯\n",
      "ğŸ¤– æ¨¡å‹é¢„æµ‹ç»“æœ:\n",
      "------------------------------\n",
      "  1. man        (ä¿¡å¿ƒ: 71.53%)\n",
      "  2. plays      (ä¿¡å¿ƒ: 22.49%)\n",
      "  3. woman      (ä¿¡å¿ƒ: 5.23%)\n",
      "  4. over       (ä¿¡å¿ƒ: 0.59%)\n",
      "  5. cat        (ä¿¡å¿ƒ: 0.08%)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•ç”¨ä¾‹ 3: è€ƒå¯Ÿè°“è¯­ (Verb) / è¯­æ³•\n",
    "# é¢„æœŸï¼šman å’Œ book ä¸­é—´åº”è¯¥æ˜¯åŠ¨ä½œ (reads, likes, buys, sees)\n",
    "predict(trained_model, \"the man [MASK] the book\", word2id, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå‰ä¸¤ä¸ªè¿˜è¡Œï¼Œç¬¬ä¸‰ä¸ªå°±å¾ˆä¸€èˆ¬äº†ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
