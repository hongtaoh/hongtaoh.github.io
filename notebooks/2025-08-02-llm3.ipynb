{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"从零开始自己手写语言模型 (三): 玩转注意力机制\"\n",
    "date: 2025-08-10\n",
    "author: \"郝鸿涛\"\n",
    "slug: llm\n",
    "draft: false\n",
    "toc: false\n",
    "tags: llm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动手实现简易自我注意力机制\n",
    "\n",
    "![Goal of self-attention](img/self_attention_goal.png)\n",
    "\n",
    "我们上一节讲过了，如果相同的词有一样的 embedding，那这种简单的办法并无法把位置和上下文信息囊括进去。一个好的办法是根据某一个词的上下文来调整其 embedding。注意力机制就是帮助我们实现这一点。\n",
    "\n",
    "上图显示了这一目标。需要注意的是，$z^{(2)}$ 对应的是 $x^{(2)}$。最后的结果是，每一个 $x^{(i)}$ 都有对应的 $z^{(i)}$。上图想表示的是，$z^{(2)}$ 的计算结果需要综合所有的 $x^{i}$。\n",
    "\n",
    "那具体怎么算？很简单，$z^{(i)}$ 的计算结果是\n",
    "\n",
    "$$z^{(i)} = \\sum_{j=1}^T x^{(j)} \\cdot \\alpha_{ij}$$\n",
    "\n",
    "那问题是我们如何得到 $\\alpha_{ij}$。\n",
    "\n",
    "我们先计算 $\\omega_{ij}$:\n",
    "\n",
    "![Self-attention w](img/self_attention_omega.png)\n",
    "\n",
    "$$\\omega_{ij} = \\sum_{j=1}^T x^{(j)} \\cdot x^{(i)}$$\n",
    "\n",
    "也就是计算 dot product。如果你看过我的[这篇博客](/cn/2025/03/16/bert/)，你就知道，其实我们计算的是两个向量的余弦相似度，但因为两个向量的长度均为一，所以分母可以忽略，那结果就是向量点乘。其结果是一个数字，代表了两个向量的相似程度。这样说的话是不是挺合理的？$\\omega_{ij}$ 的结果是 $x^{(i)}$ 与 $x^{(j)}$ 的相似程度。越相似，权重就越大，那 $x^{(j)}$ 对 $z^{(i)}$ 的影响也就越大。\n",
    "\n",
    "这样计算完之后，$\\omega_i$ 之和不一定为 1，我们先将其标准化，得到 $\\alpha$ 需要用到 [softmax function](https://en.wikipedia.org/wiki/Softmax_function)。\n",
    "\n",
    "然后就按上面我们说的计算就好：\n",
    "\n",
    "![Self-attention z](img/self_attention_z.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
