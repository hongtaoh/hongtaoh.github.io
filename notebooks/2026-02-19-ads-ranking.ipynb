{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Building An Ads Ranking System From Scratch\"\n",
    "date: 2026-02-20\n",
    "author: \"Hongtao Hao\"\n",
    "slug: ads-ranking\n",
    "draft: false\n",
    "toc: true\n",
    "tags: ML\n",
    "---\n",
    "\n",
    "We are building an Ads ranking ML system from scratch. \n",
    "\n",
    "The basic idea is this: think about Facebook or Instagram. The platform has millions of ads to show, but it needs algorithms to decide which ads to show. Why? Because, if it shows you ads randomly, you'll get annoyed and leave the platform, let alone clicking the ads and purchasing. What Meat wants:\n",
    "\n",
    "- You keep happy and like the ads, and even better, click it and make the purchase.\n",
    "- Advertisers are happy and make money and keep putting ads on Meta. \n",
    "- Meta makes money and has more revenue and the stock prices keep going up and paying the employees. \n",
    "\n",
    "The problem is, how to create that algorithm? That's the focus of this blog post. \n",
    "\n",
    "Before we talk about algorithms, let's look at what data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>ad_id</th>\n",
       "      <th>device</th>\n",
       "      <th>location</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>clicked</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>mobile</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>mobile</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>mobile</td>\n",
       "      <td>US</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Asia</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Asia</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  ad_id  device location  weekday  month  hour  clicked  converted\n",
       "0        1      4  mobile       US        2      3    23        1          0\n",
       "1        1     18  mobile       US        1      2     6        0          0\n",
       "2        1     20  mobile       US        6     12    17        0          0\n",
       "3        1     15  mobile     Asia        1      3    22        0          0\n",
       "4        1      9  mobile     Asia        3      2     2        0          0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event = pd.read_csv('data/ads_ranking/event_log.csv')\n",
    "event.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ad_id</th>\n",
       "      <th>category</th>\n",
       "      <th>advertiser_id</th>\n",
       "      <th>format</th>\n",
       "      <th>bid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>shoes</td>\n",
       "      <td>1</td>\n",
       "      <td>image</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>shoes</td>\n",
       "      <td>1</td>\n",
       "      <td>video</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>shoes</td>\n",
       "      <td>1</td>\n",
       "      <td>carousel</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>shoes</td>\n",
       "      <td>2</td>\n",
       "      <td>image</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>shoes</td>\n",
       "      <td>2</td>\n",
       "      <td>video</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ad_id category  advertiser_id    format  bid\n",
       "0      1    shoes              1     image  1.2\n",
       "1      2    shoes              1     video  2.5\n",
       "2      3    shoes              1  carousel  1.8\n",
       "3      4    shoes              2     image  1.0\n",
       "4      5    shoes              2     video  2.2"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ads = pd.read_csv('data/ads_ranking/ad_catalog.csv')\n",
    "ads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>18-24</td>\n",
       "      <td>M</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>25-34</td>\n",
       "      <td>F</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>35-44</td>\n",
       "      <td>M</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>25-34</td>\n",
       "      <td>F</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>18-24</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id age_group gender  country\n",
       "0        1     18-24      M       US\n",
       "1        2     25-34      F       UK\n",
       "2        3     35-44      M  Germany\n",
       "3        4     25-34      F       US\n",
       "4        5     18-24      M    Japan"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv('data/ads_ranking/user_catalog.csv')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Navigation and Data\n",
    "\n",
    "The `ads` is the ads catalog. In it, the `bid` is the amount of money each advertiser is willing to pay for that ad to be shown to a user. \n",
    "\n",
    "The `users` is the user raw data. You can assume it's accurate. \n",
    "\n",
    "The `event` is the historical dataset. The `clicked` is a binary variable, showing whether the user clicked the ad or not. `converted` is also binary, telling us whether the user actually purchased that item. \n",
    "\n",
    "Our goal: If I give you the user features (who they are), ads features (what every ad is), and the circumstances of the impression (when, where, on what device), you tell me which ads to show to the user at that moment. \n",
    "\n",
    "Note that in real life, we don't have the `clicked` and `converted` in the `event`, which are historical data. In real life, we need to rank ads and decide which ad to show to that user under those circumstances in realtime.\n",
    "\n",
    "The general formula we have is:\n",
    "\n",
    "$$\\text{Expected revenue} = \\text{bid} \\times P(\\text{clicked}) \\times P(\\text{conversion} \\mid \\text{clicked})$$\n",
    "\n",
    "Therefore, for each future \"event\", i.e., a given user and given contexts, we need to calcualte P(clicked) and p(conversion) for each ad. The problem is: calculating these is expensive. Before we calculate them, we need a shortlist of ads (say, from 100 millions ads, we need a shortlist of 1,000). Then we calculate the two probabilities and get a final list of 5-10 ads to show to the user. \n",
    "\n",
    "Some terminologies:\n",
    "\n",
    "- `pCTR`: short for $P(\\text{clicked})$. CTR means click through rate. \n",
    "- `pCVR`: short for $P(\\text{conversion} \\mid \\text{clicked}$. CVR means conversion rate. \n",
    "- Impression: an ads showing to the user is an impression. \n",
    "\n",
    "Then, you might wonder, why don't we simply calculate $P(\\text{purchase} \\mid \\text{shown ad})$? Well, technically we could, but the `bid` is about each click, not each \"purchase\". If we predict $P(\\text{purchase} \\mid \\text{shown ad})$ directly, we won't be able to calculate the Expected Revenue correctly, right?\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "We humans can understand something like `category: shoes, tech, food`, and `user_38291047`, but it's hard for computers. It needs numbers. A naive approach is like this: `category: 1, 2, 3` and `user_id: 0`. The problem is that computers will treat them ordinal: food is three times shoes. This doesn't make sense. One-hot encoding solves the issue: instead of having `category`, we make each category a column and use 0/1 binary. \n",
    "\n",
    "If we have three ads and they belong to `shoe, tech, food`, then, the `shoe` column will be `1, 0, 0`, `tech` col will be `0, 1, 0` and `food` col is `0, 0, 1`.\n",
    "\n",
    "But we cannot use one-hot encoding for the high-cardinality user ids. Why? Suppose we have 100 millions, then each user is a binary variable. That's impossible. We cannot handle so many variables. A better approach is to use embeddings. Say we have a 128 dimensional space and we assign a coordinate for each user id. How to find the coordinate? Machine Learning. And that's what we are going to do. \n",
    "\n",
    "Now, you understand embedding, and you might wonder, wait, why don't we use embedding for the low-cardinality variables? Bingo. We can. In fact, it's better and more flexible than one-hot encoding. One simple reason is that the one-hot encoding 0/1 is fixed but the embedding coordinates are learned, so it's better. \n",
    "\n",
    "Another thing is that for numerical variables, we can also project one scalar to high dimensional representations through linear projection. However, to keep things simple in this blog, I'll just use the original scalars instead (after standardization, i.e., converting to z scores).\n",
    "\n",
    "So we have these three sources of data:\n",
    "\n",
    "```txt\n",
    "User catalog   : user_id, age_group, gender, country\n",
    "Ad catalog     : ad_id, advertiser_id, category, format, bid\n",
    "Event log      : device, location, weekday, month, hour\n",
    "```\n",
    "And this is our encoding scheme:\n",
    "\n",
    "```txt\n",
    "user_id        → embedding lookup  (10 users, but in real world 500M)\n",
    "age_group      → embedding lookup  (4 groups)\n",
    "gender         → embedding lookup  (2 values)\n",
    "country        → embedding lookup  (5 countries)\n",
    "\n",
    "ad_id          → embedding lookup  (20 ads, but in real world 10M+)\n",
    "advertiser_id  → embedding lookup  (5 advertisers)\n",
    "category       → embedding lookup  (3 categories)\n",
    "format         → embedding lookup  (3 formats)\n",
    "bid            → numerical  (already a meaningful number)\n",
    "\n",
    "device         → embedding lookup  (3 devices)\n",
    "location       → embedding lookup  (3 locations)\n",
    "weekday        → embedding lookup  (7 values, non-linear pattern)\n",
    "month          → embedding lookup  (12 values, non-linear pattern)\n",
    "hour           → numerical  (0-23, natural ordering exists)\n",
    "```\n",
    "\n",
    "### Feature Store \n",
    "\n",
    "Besides these stand-alone features, we can have some aggregate features:\n",
    "\n",
    "- `user_historical_ctr`: Is this user a clicker in general?\n",
    "- `ad_historical_ctr`: Is this ad generally appealing?\n",
    "- `user_x_category_ctr`: Does this user like this category?\n",
    "- `user_x_advertiser_converted`: Has this user bought from this advertiser before?\n",
    "\n",
    "If the data is gigantic, say, you have 100 million users, you might store this gigantic table in Redis. But in essense, it's just a precomputed table ready for lookup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_merged = event.merge(\n",
    "    ads[[\"ad_id\", \"category\", \"advertiser_id\"]], on=\"ad_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. user historical CTR\n",
    "user_ctr = (\n",
    "    event\n",
    "    .groupby(\"user_id\")[\"clicked\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"clicked\": \"user_historical_ctr\"})\n",
    ")\n",
    "\n",
    "# 2. ad historical CTR\n",
    "ad_ctr = (\n",
    "    event\n",
    "    .groupby(\"ad_id\")[\"clicked\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"clicked\": \"ad_historical_ctr\"})\n",
    ")\n",
    "\n",
    "# 3. user x category CTR\n",
    "user_category_ctr = (\n",
    "    event_merged\n",
    "    .groupby([\"user_id\", \"category\"])[\"clicked\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"clicked\": \"user_x_category_ctr\"})\n",
    ")\n",
    "\n",
    "# 4. user x advertiser conversion (ever converted = max)\n",
    "user_advertiser_conv = (\n",
    "    event_merged\n",
    "    .groupby([\"user_id\", \"advertiser_id\"])[\"converted\"]\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"converted\": \"user_x_advertiser_converted\"})\n",
    ")\n",
    "\n",
    "# 5: user x category conversion (ever converted = max)\n",
    "user_category_conv = (\n",
    "    event_merged\n",
    "    .groupby([\"user_id\", \"category\"])[\"converted\"]\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"converted\": \"user_x_category_converted\"})\n",
    ")\n",
    "\n",
    "# save\n",
    "user_ctr.to_csv(\"data/ads_ranking/fs_user_ctr.csv\", index=False)\n",
    "ad_ctr.to_csv(\"data/ads_ranking/fs_ad_ctr.csv\", index=False)\n",
    "user_category_ctr.to_csv(\"data/ads_ranking/fs_user_category_ctr.csv\", index=False)\n",
    "user_advertiser_conv.to_csv(\"data/ads_ranking/fs_user_advertiser_conv.csv\", index=False)\n",
    "user_category_conv.to_csv(\"data/ads_ranking/fs_user_category_conv.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have these tables:\n",
    "\n",
    "```txt\n",
    "event_log\n",
    "user_catalog\n",
    "ad_catalog\n",
    "fs_user_ctr\n",
    "fs_ad_ctr\n",
    "fs_user_category_ctr\n",
    "fs_user_advertiser_conv\n",
    "fs_user_category_conv\n",
    "```\n",
    "\n",
    "Joining them will give us a giant table with all these variables:\n",
    "\n",
    "```txt\n",
    "user_id | ad_id | device | location | weekday | month | hour |\n",
    "age_group | gender | country |\n",
    "category | advertiser_id | format | bid |\n",
    "user_historical_ctr | ad_historical_ctr |\n",
    "user_x_category_ctr | user_x_advertiser_converted | user_x_category_cnverted |\n",
    "clicked | converted\n",
    "```\n",
    "\n",
    "One question: how do we deal with missing data? Say, when a user never clicks nor purchased a category or advertiser? For `ctr`, we can use the average `ctr` of that category across all users. Why? Because `ctr` is a continuous variable. We don't have historical data for this user and this category, so our best guess is to use mean imputation. This is different from when we have missing data in the `conv`, which is a binary variable (0/1). We will just use 0 for the missing data for the `conv`. \n",
    "\n",
    "Now we merge all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (500, 21)\n",
      "Columns: Index(['user_id', 'ad_id', 'device', 'location', 'weekday', 'month', 'hour',\n",
      "       'clicked', 'converted', 'age_group', 'gender', 'country', 'category',\n",
      "       'advertiser_id', 'format', 'bid', 'user_historical_ctr',\n",
      "       'ad_historical_ctr', 'user_x_category_ctr',\n",
      "       'user_x_advertiser_converted', 'user_x_category_converted'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# start with event log\n",
    "df = event.copy()\n",
    "\n",
    "# join user catalog\n",
    "df = df.merge(users, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# join ad catalog\n",
    "df = df.merge(ads, on=\"ad_id\", how=\"left\")\n",
    "\n",
    "# join feature store — user historical ctr\n",
    "df = df.merge(user_ctr, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# join feature store — ad historical ctr\n",
    "df = df.merge(ad_ctr, on=\"ad_id\", how=\"left\")\n",
    "\n",
    "# join feature store — user x category ctr\n",
    "df = df.merge(user_category_ctr, on=[\"user_id\", \"category\"], how=\"left\")\n",
    "\n",
    "# join feature store — user x advertiser converted\n",
    "df = df.merge(user_advertiser_conv, on=[\"user_id\", \"advertiser_id\"], how=\"left\")\n",
    "\n",
    "# join feature store — user x category converted\n",
    "df = df.merge(user_category_conv, on=[\"user_id\", \"category\"], how=\"left\")\n",
    "\n",
    "# handle missing values\n",
    "category_mean_ctr = user_category_ctr.groupby(\"category\")[\"user_x_category_ctr\"].mean()\n",
    "df[\"user_x_category_ctr\"] = df.apply(\n",
    "    lambda row: category_mean_ctr[row[\"category\"]] \n",
    "    if pd.isna(row[\"user_x_category_ctr\"]) \n",
    "    else row[\"user_x_category_ctr\"],\n",
    "    axis=1\n",
    ")\n",
    "df[\"user_x_advertiser_converted\"] = df[\"user_x_advertiser_converted\"].fillna(0)\n",
    "df[\"user_x_category_converted\"]   = df[\"user_x_category_converted\"].fillna(0)\n",
    "\n",
    "# save\n",
    "df.to_csv(\"data/ads_ranking/features.csv\", index=False)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Two Tower Model \n",
    "\n",
    "Predicting `pCTR` and `pCVR` is expensive. When we have 500 million users and 100 million ads, we cannot calcualte these two numbers for each `user x ad`: after all, users won't sit there for half an hour for you to do the math and then scroll the screen. We need to find a cheaper way to get a short list of 1k ads, based on the user. Two tower model is the correct way to go. \n",
    "\n",
    "How does the two tower model work? There are two towers: user tower and ads tower. We have the historical data and know which users clicked which ads and didn't click which other ads. Using this information, we try to get a coordinate for each ad in a high dimensional space and get a coordinate for each user as well. We then freeze the ads vectors, but compute the user vectors at request time. We use dot product to find the nearest neighbors (using `FAISS`). This way, we can quickly get a short list of 1k ads for the current user under a certain context. \n",
    "\n",
    "This is the architecture:\n",
    "\n",
    "```txt\n",
    "User Tower input:\n",
    "  categorical: age_group, gender, country, device, location, weekday, month\n",
    "               → each gets its own embedding table\n",
    "  numerical:   hour, user_historical_ctr, user_x_category_ctr,\n",
    "               user_x_advertiser_converted, user_x_category_converted\n",
    "               → standardize → use directly\n",
    "  concatenate all → MLP → 64D vector\n",
    "\n",
    "Ad Tower input:\n",
    "  categorical: category, advertiser_id, format\n",
    "               → each gets its own embedding table\n",
    "  numerical:   bid, ad_historical_ctr\n",
    "               → standardize → use directly\n",
    "  concatenate all → MLP → 64D vector\n",
    "\n",
    "Training:\n",
    "  for each clicked impression:\n",
    "    positive = clicked ad\n",
    "    negatives = 4 randomly sampled ads\n",
    "  loss = softmax cross entropy\n",
    "  goal: dot(user_vector, positive_ad_vector) > all 4 negatives\n",
    "```\n",
    "\n",
    "And we freeze the ads tower:\n",
    "\n",
    "```txt\n",
    "Run all 20 ads through the trained Ad Tower\n",
    "Ad 1  → Ad Tower → [0.3, 0.5, ...] (64D vector) → store\n",
    "Ad 2  → Ad Tower → [0.1, 0.8, ...] (64D vector) → store\n",
    "...\n",
    "Ad 20 → Ad Tower → [0.4, 0.2, ...] (64D vector) → store\n",
    "\n",
    "Store all 20 vectors in FAISS index.\n",
    "```\n",
    "\n",
    "We also freeze the leanred parameters for the user tower:\n",
    "\n",
    "```txt\n",
    "age_group embedding table  → learned during training, fixed at serving\n",
    "gender embedding table     → learned during training, fixed at serving\n",
    "country embedding table    → learned during training, fixed at serving\n",
    "device embedding table     → learned during training, fixed at serving\n",
    "...\n",
    "MLP weights                → learned during training, fixed at serving\n",
    "```\n",
    "\n",
    "And calculate the user tower online:\n",
    "\n",
    "```txt\n",
    "age_group=\"25-34\" → lookup row 1 from frozen embedding table → [0.4, 0.1, ...]\n",
    "gender=\"F\"        → lookup row 1 from frozen embedding table → [0.2, 0.9, ...]\n",
    "hour=21           → standardize → 0.875\n",
    "...\n",
    "concatenate all → frozen MLP → 64D user vector\n",
    "FAISS: find 1000 nearest ad vectors\n",
    "→ these 1000 ads go to CTR/CVR model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# STEP 1: LABEL ENCODING\n",
    "# nn.Embedding expects 0-indexed integers\n",
    "# ─────────────────────────────────────────────\n",
    "df[\"age_group\"]     = df[\"age_group\"].map({\"18-24\": 0, \"25-34\": 1, \"35-44\": 2, \"45+\": 3})\n",
    "df[\"gender\"]        = df[\"gender\"].map({\"M\": 0, \"F\": 1})\n",
    "df[\"country\"]       = df[\"country\"].map({\"US\": 0, \"UK\": 1, \"Germany\": 2, \"Japan\": 3, \"Brazil\": 4})\n",
    "df[\"device\"]        = df[\"device\"].map({\"mobile\": 0, \"desktop\": 1, \"tablet\": 2})\n",
    "df[\"location\"]      = df[\"location\"].map({\"US\": 0, \"EU\": 1, \"Asia\": 2})\n",
    "df[\"category\"]      = df[\"category\"].map({\"shoes\": 0, \"tech\": 1, \"food\": 2})\n",
    "df[\"format\"]        = df[\"format\"].map({\"image\": 0, \"video\": 1, \"carousel\": 2})\n",
    "df[\"advertiser_id\"] = df[\"advertiser_id\"].map({1: 0, 2: 1, 3: 2, 4: 3, 5: 4})\n",
    "df[\"weekday\"]       = df[\"weekday\"] - 1   # 1-7 → 0-6\n",
    "df[\"month\"]         = df[\"month\"] - 1     # 1-12 → 0-11\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# STEP 2: STANDARDIZE NUMERICAL FEATURES\n",
    "# ─────────────────────────────────────────────\n",
    "num_cols = [\"hour\", \"user_historical_ctr\", \"ad_historical_ctr\",\n",
    "            \"user_x_category_ctr\", \"user_x_advertiser_converted\",\n",
    "            \"user_x_category_converted\", \"bid\"]\n",
    "\n",
    "for col in num_cols:\n",
    "    mean = df[col].mean()\n",
    "    std  = df[col].std()\n",
    "    df[col] = (df[col] - mean) / std if std > 0 else 0.0\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# STEP 3: DEFINE FEATURE COLUMNS\n",
    "# ─────────────────────────────────────────────\n",
    "USER_CAT_COLS = [\"age_group\", \"gender\", \"country\", \"device\", \"location\", \"weekday\", \"month\"]\n",
    "USER_NUM_COLS = [\"hour\", \"user_historical_ctr\", \"user_x_category_ctr\",\n",
    "                 \"user_x_advertiser_converted\", \"user_x_category_converted\"]\n",
    "\n",
    "AD_CAT_COLS   = [\"category\", \"advertiser_id\", \"format\"]\n",
    "AD_NUM_COLS   = [\"bid\", \"ad_historical_ctr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# STEP 4: DATASET\n",
    "# For each clicked impression, return:\n",
    "#   - user categorical features\n",
    "#   - user numerical features\n",
    "#   - positive ad categorical features (the clicked ad)\n",
    "#   - positive ad numerical features\n",
    "#   - negative ad categorical features (4 random ads)\n",
    "#   - negative ad numerical features\n",
    "# ─────────────────────────────────────────────\n",
    "class TwoTowerDataset(Dataset):\n",
    "    def __init__(self, df, n_negatives=4):\n",
    "        self.clicks      = df[df[\"clicked\"] == 1].reset_index(drop=True)\n",
    "        self.all_ads     = df[AD_CAT_COLS + AD_NUM_COLS + [\"ad_id\"]].drop_duplicates(\"ad_id\").reset_index(drop=True)\n",
    "        self.n_negatives = n_negatives\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clicks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.clicks.iloc[idx]\n",
    "\n",
    "        # user features\n",
    "        user_categorical = torch.tensor(row[USER_CAT_COLS].values.astype(int),   dtype=torch.long)\n",
    "        user_numerical   = torch.tensor(row[USER_NUM_COLS].values.astype(float), dtype=torch.float)\n",
    "\n",
    "        # positive ad = the ad that was clicked\n",
    "        pos_categorical  = torch.tensor(row[AD_CAT_COLS].values.astype(int),     dtype=torch.long)\n",
    "        pos_numerical    = torch.tensor(row[AD_NUM_COLS].values.astype(float),   dtype=torch.float)\n",
    "\n",
    "        # negative ads = 4 random ads that were NOT the clicked one\n",
    "        neg_pool             = self.all_ads[self.all_ads[\"ad_id\"] != row[\"ad_id\"]]\n",
    "        negatives            = neg_pool.sample(self.n_negatives)\n",
    "        neg_categorical      = torch.tensor(negatives[AD_CAT_COLS].values.astype(int),   dtype=torch.long)\n",
    "        neg_numerical        = torch.tensor(negatives[AD_NUM_COLS].values.astype(float), dtype=torch.float)\n",
    "\n",
    "        return user_categorical, user_numerical, pos_categorical, pos_numerical, neg_categorical, neg_numerical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# STEP 5: MODEL\n",
    "# ─────────────────────────────────────────────\n",
    "EMBED_DIM  = 8\n",
    "OUTPUT_DIM = 64\n",
    "\n",
    "class UserTower(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb_age_group = nn.Embedding(4,  EMBED_DIM)\n",
    "        self.emb_gender    = nn.Embedding(2,  EMBED_DIM)\n",
    "        self.emb_country   = nn.Embedding(5,  EMBED_DIM)\n",
    "        self.emb_device    = nn.Embedding(3,  EMBED_DIM)\n",
    "        self.emb_location  = nn.Embedding(3,  EMBED_DIM)\n",
    "        self.emb_weekday   = nn.Embedding(7,  EMBED_DIM)\n",
    "        self.emb_month     = nn.Embedding(12, EMBED_DIM)\n",
    "\n",
    "        # 7 embeddings × 8 dims + 5 numerical features\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(7 * EMBED_DIM + len(USER_NUM_COLS), 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, OUTPUT_DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, categorical, numerical):\n",
    "        # look up embedding for each categorical feature\n",
    "        age_group_emb = self.emb_age_group(categorical[:, 0])  # (B, 8)\n",
    "        gender_emb    = self.emb_gender(categorical[:, 1])     # (B, 8)\n",
    "        country_emb   = self.emb_country(categorical[:, 2])    # (B, 8)\n",
    "        device_emb    = self.emb_device(categorical[:, 3])     # (B, 8)\n",
    "        location_emb  = self.emb_location(categorical[:, 4])   # (B, 8)\n",
    "        weekday_emb   = self.emb_weekday(categorical[:, 5])    # (B, 8)\n",
    "        month_emb     = self.emb_month(categorical[:, 6])      # (B, 8)\n",
    "\n",
    "        # concatenate all embeddings + numerical into one vector\n",
    "        x = torch.cat([\n",
    "            age_group_emb, gender_emb, country_emb,\n",
    "            device_emb, location_emb, weekday_emb, month_emb,\n",
    "            numerical,\n",
    "        ], dim=1)  # (B, 61)\n",
    "\n",
    "        return self.mlp(x)  # (B, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdTower(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb_category   = nn.Embedding(3, EMBED_DIM)\n",
    "        self.emb_advertiser = nn.Embedding(5, EMBED_DIM)\n",
    "        self.emb_format     = nn.Embedding(3, EMBED_DIM)\n",
    "\n",
    "        # 3 embeddings × 8 dims + 2 numerical features\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(3 * EMBED_DIM + len(AD_NUM_COLS), 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, OUTPUT_DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, categorical, numerical):\n",
    "        category_emb   = self.emb_category(categorical[:, 0])    # (B, 8)\n",
    "        advertiser_emb = self.emb_advertiser(categorical[:, 1])  # (B, 8)\n",
    "        format_emb     = self.emb_format(categorical[:, 2])      # (B, 8)\n",
    "\n",
    "        x = torch.cat([\n",
    "            category_emb, advertiser_emb, format_emb,\n",
    "            numerical,\n",
    "        ], dim=1)  # (B, 26)\n",
    "\n",
    "        return self.mlp(x)  # (B, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Loss: 1.2147\n",
      "Epoch 10/20 | Loss: 0.7753\n",
      "Epoch 15/20 | Loss: 0.5993\n",
      "Epoch 20/20 | Loss: 0.5407\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# STEP 6: TRAINING\n",
    "# ─────────────────────────────────────────────\n",
    "dataset    = TwoTowerDataset(df, n_negatives=4)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "user_tower = UserTower()\n",
    "ad_tower   = AdTower()\n",
    "optimizer  = torch.optim.Adam(\n",
    "    list(user_tower.parameters()) + list(ad_tower.parameters()),\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "\n",
    "    for user_categorical, user_numerical, pos_categorical, pos_numerical, neg_categorical, neg_numerical in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size     = user_categorical.shape[0]  # number of examples in this batch\n",
    "        n_negatives    = neg_categorical.shape[1]   # number of negatives per example = 4\n",
    "\n",
    "        # compute user vector\n",
    "        user_vec = user_tower(user_categorical, user_numerical)  # (batch_size, 64)\n",
    "\n",
    "        # compute positive ad vector\n",
    "        pos_vec  = ad_tower(pos_categorical, pos_numerical)      # (batch_size, 64)\n",
    "\n",
    "        # compute negative ad vectors\n",
    "        # neg_categorical shape: (batch_size, 4, 3) → reshape to (batch_size*4, 3) for ad tower\n",
    "        # then reshape back to (batch_size, 4, 64)\n",
    "        neg_vecs = ad_tower(\n",
    "            neg_categorical.view(batch_size * n_negatives, -1),\n",
    "            neg_numerical.view(batch_size * n_negatives, -1)\n",
    "        ).view(batch_size, n_negatives, OUTPUT_DIM)              # (batch_size, 4, 64)\n",
    "\n",
    "        # dot product: user vs positive ad\n",
    "        pos_score  = (user_vec * pos_vec).sum(dim=1, keepdim=True)           # (batch_size, 1)\n",
    "\n",
    "        # dot product: user vs each negative ad\n",
    "        neg_scores = torch.bmm(\n",
    "            neg_vecs,                        # (batch_size, 4, 64)\n",
    "            user_vec.unsqueeze(2)            # (batch_size, 64, 1)\n",
    "        ).squeeze(2)                         # (batch_size, 4)\n",
    "\n",
    "        # concatenate scores: [pos_score, neg_scores] → (batch_size, 5)\n",
    "        # label = 0 means \"correct answer is at index 0\" = the positive ad\n",
    "        all_scores = torch.cat([pos_score, neg_scores], dim=1)   # (batch_size, 5)\n",
    "        labels     = torch.zeros(batch_size, dtype=torch.long)   # always 0\n",
    "\n",
    "        loss = F.cross_entropy(all_scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/20 | Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top 5 ads per user ===\n",
      "\n",
      "User 1 (prefers shoes):\n",
      "  Ad  5 (shoes) score: 1.766\n",
      "  Ad  7 (shoes) score: 1.623\n",
      "  Ad  4 (shoes) score: 1.620\n",
      "  Ad  3 (shoes) score: 1.359\n",
      "  Ad  6 (shoes) score: 0.443\n",
      "\n",
      "User 4 (prefers tech):\n",
      "  Ad 13 (tech ) score: 5.230\n",
      "  Ad 14 (tech ) score: 4.197\n",
      "  Ad 10 (tech ) score: 3.958\n",
      "  Ad 11 (tech ) score: 3.500\n",
      "  Ad 12 (tech ) score: 3.285\n",
      "\n",
      "User 7 (prefers food):\n",
      "  Ad 20 (food ) score: 3.634\n",
      "  Ad 17 (food ) score: 3.302\n",
      "  Ad 10 (tech ) score: 2.979\n",
      "  Ad 13 (tech ) score: 2.355\n",
      "  Ad  3 (shoes) score: 1.912\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# STEP 7: VERIFY\n",
    "# user 1 (shoes) → top 5 ads should be shoes\n",
    "# user 4 (tech)  → top 5 ads should be tech\n",
    "# user 7 (food)  → top 5 ads should be food\n",
    "# ─────────────────────────────────────────────\n",
    "user_tower.eval()\n",
    "ad_tower.eval()\n",
    "\n",
    "all_ads  = df[AD_CAT_COLS + AD_NUM_COLS + [\"ad_id\"]].drop_duplicates(\"ad_id\").reset_index(drop=True)\n",
    "cat_name = {0: \"shoes\", 1: \"tech\", 2: \"food\"}\n",
    "pref     = {1: \"shoes\", 4: \"tech\", 7: \"food\"}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # precompute all 20 ad vectors\n",
    "    ad_vecs = ad_tower(\n",
    "        torch.tensor(all_ads[AD_CAT_COLS].values.astype(int),   dtype=torch.long),\n",
    "        torch.tensor(all_ads[AD_NUM_COLS].values.astype(float), dtype=torch.float)\n",
    "    )  # (20, 64)\n",
    "\n",
    "    print(\"\\n=== Top 5 ads per user ===\")\n",
    "    for user_id in [1, 4, 7]:\n",
    "        row = df[df[\"user_id\"] == user_id].iloc[0]\n",
    "\n",
    "        user_vec = user_tower(\n",
    "            torch.tensor(row[USER_CAT_COLS].values.astype(int),   dtype=torch.long).unsqueeze(0),\n",
    "            torch.tensor(row[USER_NUM_COLS].values.astype(float), dtype=torch.float).unsqueeze(0)\n",
    "        )  # (1, 64)\n",
    "\n",
    "        # score all ads by dot product\n",
    "        scores = (ad_vecs @ user_vec.T).squeeze()  # (20,)\n",
    "        top5   = scores.argsort(descending=True)[:5]\n",
    "\n",
    "        print(f\"\\nUser {user_id} (prefers {pref[user_id]}):\")\n",
    "        for i in top5:\n",
    "            ad_row = all_ads.iloc[i.item()]\n",
    "            print(f\"  Ad {int(ad_row['ad_id']):2d} \"\n",
    "                  f\"({cat_name[int(ad_row['category'])]:5s}) \"\n",
    "                  f\"score: {scores[i].item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTR and CVR Models\n",
    "\n",
    "In the two tower model, the parameters we learned are the embedding lookup tables for categorical variables and the MLP weights for the user tower and also the ad tower. \n",
    "\n",
    "As mentioned earlier, we freeze these parameters and also all ads vectors. At request time, we compute the user vector and use FAISS to get the top 1k candidates for our CTR and CVR models. \n",
    "\n",
    "The CTR model is meant to predict the `pCTR` and CVR model the `pCVR`. The basic idea is that for each row in the giant `df` where we have all the user, ad and context information, we process the variables the same way as we did for the two tower model, but now we are not doing the dot procedure the way we did before. Rather, with all these variables, AND separate user ID embedding lookup and ad ID embedding look up, we are predicting the pCTR and pCVR. \n",
    "\n",
    "Notice that user and context features do have predictive powers, but they are \"generalized\" and not specific to a user. With user embedding lookup tables, each user obtains one coordinate specific to them, allowing more granular prediction. Same for Ads. \n",
    "\n",
    "For CTR and CVR models, we are using binary cross entropy loss, because the predicted variable is binary (0/1). However, since in the production reality, the class is very imbalanced (maybe over 99% of the impressions are not clicked), it's better to use focal loss:\n",
    "\n",
    "```txt\n",
    "FL = -α × (1-p)^γ × log(p)    for positive (y=1)\n",
    "FL = -(1-α) × p^γ × log(1-p)  for negative (y=0)\n",
    "```\n",
    "\n",
    "Two hyperparameters:\n",
    "\n",
    "- γ (gamma): how much to down-weight easy examples. Typically 2.\n",
    "- α (alpha): class weight for positives. Typically 0.25.\n",
    "\n",
    "```txt\n",
    "positive examples (clicked)     → weight 0.25\n",
    "negative examples (not clicked) → weight 0.75\n",
    "```\n",
    "\n",
    "In plain English, take the example of when $y=1$:\n",
    "\n",
    "- If the model is already confident and correct (easy example) → (1-p)^γ is small → small loss contribution\n",
    "- If the model is wrong or uncertain (hard example) → (1-p)^γ is large → large loss contribution\n",
    "\n",
    "\n",
    "Another thing is that after we have all the user, ads and context predictors, we want some interactions between them. That is to say, we do not want to use the basic representation; rather, we want to use transformers to further process the representations. However, transformers are expensive. Therefore, rather than using transformers, we compute the dot product between feature vectors. \n",
    "\n",
    "The architecture of CTR and CVR models are:\n",
    "\n",
    "```txt\n",
    "Input features:\n",
    "  categorical → embedding lookup \n",
    "  numerical   → standardize + use directly \n",
    "  user_id     → separate embedding lookup\n",
    "  ad_id       → separate embedding lookup \n",
    "\n",
    "Interaction step:\n",
    "  project all embeddings to same dimension (say 16D)\n",
    "  compute dot product between every pair\n",
    "  → gives us interaction scores\n",
    "\n",
    "Final prediction:\n",
    "  concatenate [all embeddings + numerical + interaction scores]\n",
    "  → MLP → single number → sigmoid → pCTR\n",
    "```\n",
    "\n",
    "In the following, I will only cover the CTR model. The CVR model has identical architecture to the CTR model. The only difference is that it trains on clicked impressions only, with `converted` as the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# FOCAL LOSS\n",
    "# ─────────────────────────────────────────────\n",
    "def focal_loss(predicted_prob, label, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"\n",
    "    predicted_prob: model output after sigmoid, shape (batch_size,)\n",
    "    label:          ground truth 0 or 1,        shape (batch_size,)\n",
    "    \"\"\"\n",
    "    # clip to avoid log(0)\n",
    "    predicted_prob = predicted_prob.clamp(min=1e-7, max=1 - 1e-7)\n",
    "\n",
    "    # loss for positive examples (label = 1)\n",
    "    loss_positive  = -alpha * (1 - predicted_prob) ** gamma * torch.log(predicted_prob)\n",
    "\n",
    "    # loss for negative examples (label = 0)\n",
    "    loss_negative  = -(1 - alpha) * predicted_prob ** gamma * torch.log(1 - predicted_prob)\n",
    "\n",
    "    # pick the right loss for each example based on its label\n",
    "    loss = torch.where(label == 1, loss_positive, loss_negative)\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# DATASET\n",
    "# all impressions, label = clicked (0 or 1)\n",
    "# ─────────────────────────────────────────────\n",
    "USER_CAT_COLS = [\"age_group\", \"gender\", \"country\", \"device\", \"location\", \"weekday\", \"month\"]\n",
    "USER_NUM_COLS = [\"hour\", \"user_historical_ctr\", \"user_x_category_ctr\",\n",
    "                 \"user_x_advertiser_converted\", \"user_x_category_converted\"]\n",
    "AD_CAT_COLS   = [\"category\", \"advertiser_id\", \"format\"]\n",
    "AD_NUM_COLS   = [\"bid\", \"ad_historical_ctr\"]\n",
    "\n",
    "class CTRDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        user_categorical = torch.tensor(row[USER_CAT_COLS].values.astype(int),   dtype=torch.long)\n",
    "        user_numerical   = torch.tensor(row[USER_NUM_COLS].values.astype(float), dtype=torch.float)\n",
    "        ad_categorical   = torch.tensor(row[AD_CAT_COLS].values.astype(int),     dtype=torch.long)\n",
    "        ad_numerical     = torch.tensor(row[AD_NUM_COLS].values.astype(float),   dtype=torch.float)\n",
    "\n",
    "        # user_id and ad_id for memorization embeddings\n",
    "        user_id          = torch.tensor(row[\"user_id\"] - 1,  dtype=torch.long)  # 1-10 → 0-9\n",
    "        ad_id            = torch.tensor(row[\"ad_id\"] - 1,    dtype=torch.long)  # 1-20 → 0-19\n",
    "\n",
    "        label            = torch.tensor(row[\"clicked\"], dtype=torch.float)\n",
    "\n",
    "        return user_categorical, user_numerical, ad_categorical, ad_numerical, user_id, ad_id, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# CTR MODEL (DLRM-style)\n",
    "#\n",
    "# Architecture:\n",
    "#   1. embedding lookup for all categorical features\n",
    "#   2. separate user_id and ad_id embeddings (memorization)\n",
    "#   3. project all embeddings to same dimension (INTERACT_DIM)\n",
    "#   4. compute dot product between every pair of projected embeddings\n",
    "#      → interaction scores\n",
    "#   5. concatenate everything + interaction scores\n",
    "#   6. MLP → sigmoid → pCTR\n",
    "# ─────────────────────────────────────────────\n",
    "EMBED_DIM    = 16   # embedding dimension for categorical features\n",
    "INTERACT_DIM = 16   # dimension for dot product interactions\n",
    "OUTPUT_DIM   = 1    # single probability output\n",
    "\n",
    "class CTRModel(nn.Module):\n",
    "    def __init__(self, n_users=10, n_ads=20):\n",
    "        super().__init__()\n",
    "\n",
    "        # ── categorical embeddings (same as two-tower) ──\n",
    "        self.emb_age_group  = nn.Embedding(4,  EMBED_DIM)\n",
    "        self.emb_gender     = nn.Embedding(2,  EMBED_DIM)\n",
    "        self.emb_country    = nn.Embedding(5,  EMBED_DIM)\n",
    "        self.emb_device     = nn.Embedding(3,  EMBED_DIM)\n",
    "        self.emb_location   = nn.Embedding(3,  EMBED_DIM)\n",
    "        self.emb_weekday    = nn.Embedding(7,  EMBED_DIM)\n",
    "        self.emb_month      = nn.Embedding(12, EMBED_DIM)\n",
    "        self.emb_category   = nn.Embedding(3,  EMBED_DIM)\n",
    "        self.emb_advertiser = nn.Embedding(5,  EMBED_DIM)\n",
    "        self.emb_format     = nn.Embedding(3,  EMBED_DIM)\n",
    "\n",
    "        # ── user_id and ad_id embeddings (memorization) ──\n",
    "        self.emb_user_id    = nn.Embedding(n_users, EMBED_DIM)\n",
    "        self.emb_ad_id      = nn.Embedding(n_ads,   EMBED_DIM)\n",
    "\n",
    "        # ── projection layers: project all embeddings to INTERACT_DIM ──\n",
    "        # needed so all vectors are same size for dot product interactions\n",
    "        # we have 12 embeddings total (10 categorical + user_id + ad_id)\n",
    "        self.projection     = nn.Linear(EMBED_DIM, INTERACT_DIM)\n",
    "\n",
    "        # ── MLP ──\n",
    "        # input = all embeddings flattened + numerical features + interaction scores\n",
    "        n_embeddings        = 12                              # total number of embeddings\n",
    "        n_interactions      = n_embeddings * (n_embeddings - 1) // 2  # pairs = 12*11/2 = 66\n",
    "        n_numerical         = len(USER_NUM_COLS) + len(AD_NUM_COLS)   # 5 + 2 = 7\n",
    "        mlp_input_dim       = n_embeddings * EMBED_DIM + n_numerical + n_interactions\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(mlp_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),        # ← add dropout to prevent overfitting \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),        # ← add dropout to prevent overfitting \n",
    "            nn.Linear(64, OUTPUT_DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_categorical, user_numerical, ad_categorical, ad_numerical, user_id, ad_id, return_logit:bool=False):\n",
    "\n",
    "        # ── step 1: look up all embeddings ──\n",
    "        age_group_emb   = self.emb_age_group(user_categorical[:, 0])   # (B, 16)\n",
    "        gender_emb      = self.emb_gender(user_categorical[:, 1])      # (B, 16)\n",
    "        country_emb     = self.emb_country(user_categorical[:, 2])     # (B, 16)\n",
    "        device_emb      = self.emb_device(user_categorical[:, 3])      # (B, 16)\n",
    "        location_emb    = self.emb_location(user_categorical[:, 4])    # (B, 16)\n",
    "        weekday_emb     = self.emb_weekday(user_categorical[:, 5])     # (B, 16)\n",
    "        month_emb       = self.emb_month(user_categorical[:, 6])       # (B, 16)\n",
    "        category_emb    = self.emb_category(ad_categorical[:, 0])      # (B, 16)\n",
    "        advertiser_emb  = self.emb_advertiser(ad_categorical[:, 1])    # (B, 16)\n",
    "        format_emb      = self.emb_format(ad_categorical[:, 2])        # (B, 16)\n",
    "        user_id_emb     = self.emb_user_id(user_id)                    # (B, 16)\n",
    "        ad_id_emb       = self.emb_ad_id(ad_id)                        # (B, 16)\n",
    "\n",
    "        # collect all embeddings into a list\n",
    "        all_embeddings = [\n",
    "            age_group_emb, gender_emb, country_emb,\n",
    "            device_emb, location_emb, weekday_emb, month_emb,\n",
    "            category_emb, advertiser_emb, format_emb,\n",
    "            user_id_emb, ad_id_emb\n",
    "        ]  # 12 embeddings, each (B, 16)\n",
    "\n",
    "        # ── step 2: project all embeddings to INTERACT_DIM ──\n",
    "        projected = [self.projection(emb) for emb in all_embeddings]  # 12 × (B, 16)\n",
    "\n",
    "        # ── step 3: compute dot product between every pair ──\n",
    "        interaction_scores = []\n",
    "        for i in range(len(projected)):\n",
    "            for j in range(i + 1, len(projected)):\n",
    "                # dot product between embedding i and embedding j\n",
    "                dot = (projected[i] * projected[j]).sum(dim=1, keepdim=True)  # (B, 1)\n",
    "                interaction_scores.append(dot)\n",
    "\n",
    "        # stack all interaction scores → (B, 66)\n",
    "        interactions = torch.cat(interaction_scores, dim=1)  # (B, 66)\n",
    "\n",
    "        # ── step 4: concatenate everything ──\n",
    "        all_features = torch.cat(\n",
    "            [emb for emb in all_embeddings]   # all embeddings flattened: (B, 12*16)\n",
    "            + [user_numerical, ad_numerical]  # numerical features:       (B, 7)\n",
    "            + [interactions],                 # interaction scores:        (B, 66)\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        # ── step 5: MLP → sigmoid → pCTR ──\n",
    "        logit = self.mlp(all_features)           # (B, 1)\n",
    "\n",
    "        if return_logit:\n",
    "            return logit.squeeze(1)              # raw logit, for calibration\n",
    "        \n",
    "        return torch.sigmoid(logit).squeeze(1)   # probability, for normal use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Loss: 0.0240\n",
      "Epoch 10/20 | Loss: 0.0113\n",
      "Epoch 15/20 | Loss: 0.0028\n",
      "Epoch 20/20 | Loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# TRAINING\n",
    "# ─────────────────────────────────────────────\n",
    "dataset    = CTRDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "ctr_model = CTRModel(n_users=10, n_ads=20)\n",
    "optimizer = torch.optim.Adam(ctr_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "\n",
    "    for user_categorical, user_numerical, ad_categorical, ad_numerical, user_id, ad_id, label in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pCTR = ctr_model(user_categorical, user_numerical, ad_categorical, ad_numerical, user_id, ad_id)\n",
    "        loss = focal_loss(pCTR, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/20 | Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top 5 ads per user (by pCTR) ===\n",
      "\n",
      "User 1 (prefers shoes):\n",
      "  Ad  5 (shoes): pCTR = 0.982\n",
      "  Ad  7 (shoes): pCTR = 0.918\n",
      "  Ad  4 (shoes): pCTR = 0.909\n",
      "  Ad  2 (shoes): pCTR = 0.901\n",
      "  Ad  1 (shoes): pCTR = 0.779\n",
      "\n",
      "User 4 (prefers tech):\n",
      "  Ad 14 (tech ): pCTR = 0.934\n",
      "  Ad 11 (tech ): pCTR = 0.540\n",
      "  Ad 13 (tech ): pCTR = 0.512\n",
      "  Ad  9 (tech ): pCTR = 0.327\n",
      "  Ad 15 (food ): pCTR = 0.252\n",
      "\n",
      "User 7 (prefers food):\n",
      "  Ad  5 (shoes): pCTR = 0.011\n",
      "  Ad 12 (tech ): pCTR = 0.009\n",
      "  Ad 17 (food ): pCTR = 0.006\n",
      "  Ad 13 (tech ): pCTR = 0.006\n",
      "  Ad 16 (food ): pCTR = 0.005\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# VERIFY\n",
    "# for user 1 (shoes), shoe ads should have higher pCTR\n",
    "# for user 4 (tech),  tech ads should have higher pCTR\n",
    "# for user 7 (food),  food ads should have higher pCTR\n",
    "# ─────────────────────────────────────────────\n",
    "ctr_model.eval()\n",
    "cat_name = {0: \"shoes\", 1: \"tech\", 2: \"food\"}\n",
    "pref     = {1: \"shoes\", 4: \"tech\", 7: \"food\"}\n",
    "\n",
    "all_ads  = df[AD_CAT_COLS + AD_NUM_COLS + [\"ad_id\"]].drop_duplicates(\"ad_id\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Top 5 ads per user (by pCTR) ===\")\n",
    "with torch.no_grad():\n",
    "    for user_id_val in [1, 4, 7]:\n",
    "        user_row = df[df[\"user_id\"] == user_id_val].iloc[0]\n",
    "        scores   = []\n",
    "\n",
    "        for _, ad_row in all_ads.iterrows():\n",
    "            user_categorical = torch.tensor(user_row[USER_CAT_COLS].values.astype(int),     dtype=torch.long).unsqueeze(0)\n",
    "            user_numerical   = torch.tensor(user_row[USER_NUM_COLS].values.astype(float),   dtype=torch.float).unsqueeze(0)\n",
    "            ad_categorical   = torch.tensor(ad_row[AD_CAT_COLS].values.astype(int),         dtype=torch.long).unsqueeze(0)\n",
    "            ad_numerical     = torch.tensor(ad_row[AD_NUM_COLS].values.astype(float),       dtype=torch.float).unsqueeze(0)\n",
    "            user_id_tensor   = torch.tensor(user_id_val - 1,           dtype=torch.long).unsqueeze(0)\n",
    "            ad_id_tensor     = torch.tensor(int(ad_row[\"ad_id\"]) - 1,  dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "            pCTR = ctr_model(user_categorical, user_numerical, ad_categorical, ad_numerical, user_id_tensor, ad_id_tensor)\n",
    "            scores.append((int(ad_row[\"ad_id\"]), cat_name[int(ad_row[\"category\"])], pCTR.item()))\n",
    "\n",
    "        scores.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        print(f\"\\nUser {user_id_val} (prefers {pref[user_id_val]}):\")\n",
    "        for ad_id, category, score in scores[:5]:\n",
    "            print(f\"  Ad {ad_id:2d} ({category:5s}): pCTR = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration \n",
    "\n",
    "Because we used focal loss, the model's predicted probabilities are not accurate in absolute terms. They're inflated or deflated. The ranking order might be correct but the actual numbers are wrong.\n",
    "\n",
    "This matters because the scoring formula is:\n",
    "\n",
    "```txt\n",
    "score = bid × pCTR × pCVR\n",
    "```\n",
    "\n",
    "If pCTR is inflated, the scores are wrong. Calibration fixes the absolute probability values while preserving the ranking order.\n",
    "\n",
    "We use Platt Scaling, which is quite simple:\n",
    "\n",
    "```txt\n",
    "p_calibrated = sigmoid(a × logit + b)\n",
    "```\n",
    "\n",
    "The `logit` is the raw pCTR score from our CTR model. We fit a logistic regression to calibrate it:\n",
    "\n",
    "1. Freeze the trained CTR model completely\n",
    "2. Take held-out data (e.g., our `test_event_log.csv`) — data the model never saw\n",
    "3. Run every example through the frozen model → get logits\n",
    "4. Fit a tiny logistic regression with 1 input (logit) and 2 parameters (a, b) using regular binary cross entropy loss\n",
    "5. Only 2 parameters, converges in seconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test event log\n",
    "test_event = pd.read_csv(\"data/ads_ranking/test_event_log.csv\")\n",
    "\n",
    "# join feature store — same as training\n",
    "test_df = test_event.copy()\n",
    "test_df = test_df.merge(users,                on=\"user_id\",                    how=\"left\")\n",
    "test_df = test_df.merge(ads,                  on=\"ad_id\",                      how=\"left\")\n",
    "test_df = test_df.merge(user_ctr,             on=\"user_id\",                    how=\"left\")\n",
    "test_df = test_df.merge(ad_ctr,               on=\"ad_id\",                      how=\"left\")\n",
    "test_df = test_df.merge(user_category_ctr,    on=[\"user_id\", \"category\"],      how=\"left\")\n",
    "test_df = test_df.merge(user_advertiser_conv, on=[\"user_id\", \"advertiser_id\"], how=\"left\")\n",
    "test_df = test_df.merge(user_category_conv,   on=[\"user_id\", \"category\"],      how=\"left\")\n",
    "\n",
    "# fill missing\n",
    "test_df[\"user_x_category_ctr\"]        = test_df[\"user_x_category_ctr\"].fillna(test_df[\"user_x_category_ctr\"].mean())\n",
    "test_df[\"user_x_advertiser_converted\"] = test_df[\"user_x_advertiser_converted\"].fillna(0)\n",
    "test_df[\"user_x_category_converted\"]   = test_df[\"user_x_category_converted\"].fillna(0)\n",
    "\n",
    "# label encoding — same maps as training\n",
    "test_df[\"age_group\"]     = test_df[\"age_group\"].map({\"18-24\": 0, \"25-34\": 1, \"35-44\": 2, \"45+\": 3})\n",
    "test_df[\"gender\"]        = test_df[\"gender\"].map({\"M\": 0, \"F\": 1})\n",
    "test_df[\"country\"]       = test_df[\"country\"].map({\"US\": 0, \"UK\": 1, \"Germany\": 2, \"Japan\": 3, \"Brazil\": 4})\n",
    "test_df[\"device\"]        = test_df[\"device\"].map({\"mobile\": 0, \"desktop\": 1, \"tablet\": 2})\n",
    "test_df[\"location\"]      = test_df[\"location\"].map({\"US\": 0, \"EU\": 1, \"Asia\": 2})\n",
    "test_df[\"category\"]      = test_df[\"category\"].map({\"shoes\": 0, \"tech\": 1, \"food\": 2})\n",
    "test_df[\"format\"]        = test_df[\"format\"].map({\"image\": 0, \"video\": 1, \"carousel\": 2})\n",
    "test_df[\"advertiser_id\"] = test_df[\"advertiser_id\"].map({1: 0, 2: 1, 3: 2, 4: 3, 5: 4})\n",
    "test_df[\"weekday\"]       = test_df[\"weekday\"] - 1\n",
    "test_df[\"month\"]         = test_df[\"month\"] - 1\n",
    "\n",
    "# standardize — IMPORTANT: use training data (df) stats, not test stats\n",
    "for col in num_cols:\n",
    "    mean = df[col].mean()\n",
    "    std  = df[col].std()\n",
    "    test_df[col] = (test_df[col] - mean) / std if std > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_model.eval()\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_df)):\n",
    "        row = test_df.iloc[i]\n",
    "\n",
    "        user_categorical = torch.tensor(row[USER_CAT_COLS].values.astype(int),   dtype=torch.long).unsqueeze(0)\n",
    "        user_numerical   = torch.tensor(row[USER_NUM_COLS].values.astype(float), dtype=torch.float).unsqueeze(0)\n",
    "        ad_categorical   = torch.tensor(row[AD_CAT_COLS].values.astype(int),     dtype=torch.long).unsqueeze(0)\n",
    "        ad_numerical     = torch.tensor(row[AD_NUM_COLS].values.astype(float),   dtype=torch.float).unsqueeze(0)\n",
    "        user_id_tensor   = torch.tensor(int(row[\"user_id\"]) - 1, dtype=torch.long).unsqueeze(0)\n",
    "        ad_id_tensor     = torch.tensor(int(row[\"ad_id\"])   - 1, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        logit = ctr_model(user_categorical, user_numerical, ad_categorical, ad_numerical, user_id_tensor, ad_id_tensor, return_logit=True)\n",
    "        all_logits.append(logit.item())\n",
    "        all_labels.append(row[\"clicked\"])\n",
    "\n",
    "logits = torch.tensor(all_logits, dtype=torch.float)\n",
    "labels = torch.tensor(all_labels, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=0.618, b=-0.980\n"
     ]
    }
   ],
   "source": [
    "a = nn.Parameter(torch.tensor(1.0))\n",
    "b = nn.Parameter(torch.tensor(0.0))\n",
    "optimizer = torch.optim.Adam([a, b], lr=0.01)\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    calibrated_prob = torch.sigmoid(a * logits + b)\n",
    "    loss = F.binary_cross_entropy(calibrated_prob, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"a={a.item():.3f}, b={b.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw pCTR mean:        0.446\n",
      "Calibrated pCTR mean: 0.306\n",
      "Actual CTR:           0.302\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    raw_probs        = torch.sigmoid(logits)\n",
    "    calibrated_probs = torch.sigmoid(a * logits + b)\n",
    "\n",
    "print(f\"Raw pCTR mean:        {raw_probs.mean():.3f}\")\n",
    "print(f\"Calibrated pCTR mean: {calibrated_probs.mean():.3f}\")\n",
    "print(f\"Actual CTR:           {labels.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End\n",
    "\n",
    "We have covered everything. It's not for production, so we did not cover FAISS, nor online A/B testing. \n",
    "\n",
    "### Summary\n",
    "\n",
    "This is an overview of this system design problem. \n",
    "\n",
    "The business objective is three-fold: 1) keep users on the platform and open the ads and become converted; 2) the advertisers make money; 3) the platform (Meta) makes money. \n",
    "\n",
    "The ML objective: the log loss of positive ads to be as small as possible; ROC-AUC to be as close to 1 as possible; expcted calibration error (ECE) to be as small as possible. \n",
    "\n",
    "Data: we need the event log, users and ads information. \n",
    "\n",
    "Featuer engineering: \n",
    "- We compute cross features. \n",
    "- We use embedding lookup tables for categorical variables. \n",
    "- We use the standardized values for the numerical variables. \n",
    "\n",
    "Models:\n",
    "- Two tower model for quick candidate generation (objective is 1k short list ads). Basica idea is to have the concatenated user tower and the concatenated ads tower. We have one positive ad and four random negative ads. Each ad vector will have a dot product with the user tower. Then use the softmax cross entropy loss. \n",
    "- CTR and CVR models used to predict pCTR and pCVR. Now we do the training impression by impression. The input is the concatenated user + ad + context and the goal is to predict the pCTR and pCVR. Use focal loss to account for the extreme class imbalance. Notice that we have the user_id and ad_id embedding for each user and ad, which is a big difference from the two tower model. Also, we added the dot product interaction between all feature pairs to the predictors. \n",
    "- Calibration: the problem is that the output pCTR and pCVR, although having the correct relative magnitude, might be off in terms of absolute values. We fit a simple logistic regression to map the raw predicted values to the actual values, using held-out test data. \n",
    "\n",
    "Evaluation and Deployment:\n",
    "- offline metrics: \n",
    "  - recall@k for two tower models. \n",
    "  - log loss, ROC-AUC for CTR and CVR models\n",
    "  - ECE (Expected Calibration Error) for calibration\n",
    "- online metrics: AB testing for the business metrics\n",
    "- serving: use two tower first to get the short list using FAISS, then use the CTR and CVR plus the calibration model to get the final top 5 ads to show to the user. \n",
    "- Do regular retraining every 24 hours and monitor data shift and performance. \n",
    "  \n",
    "```txt\n",
    "User + Context\n",
    "      ↓\n",
    "  User Tower → 64D vector\n",
    "                    ↓\n",
    "                  FAISS → top 1k ads\n",
    "                    ↓\n",
    "              CTR Model → pCTR → calibration\n",
    "              CVR Model → pCVR → calibration\n",
    "                    ↓\n",
    "         bid × pCTR × pCVR → top 5-10 ads\n",
    "```\n",
    "\n",
    "## Follow-up Questions\n",
    "\n",
    "- Why bid × pCTR × pCVR (not just bit × pCVR)\n",
    "  - Because bid is based on pCTR. \n",
    "- Why CTR and CVR are separate models\n",
    "  - Same reason above. \n",
    "- Why two-tower for retrieval\n",
    "  - Because it's fast.\n",
    "- Why embeddings over one-hot\n",
    "  - More flexible. \n",
    "- Why two-tower has no user_id/ad_id embeddings\n",
    "  - Ad Tower input must contain ONLY ad information\n",
    "        → ad vector is the same regardless of which user is being served\n",
    "        → precompute once, reuse for everyone\n",
    "- Why calibration is needed after focal loss\n",
    "  - More accurate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
