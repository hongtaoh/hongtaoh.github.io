{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"从零开始自己手写语言模型 (二): 文本数据的处理方法\"\n",
    "date: 2025-07-20T09:05:03-05:00\n",
    "author: \"郝鸿涛\"\n",
    "slug: llm\n",
    "draft: false\n",
    "toc: false\n",
    "tags: llm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算机无法直接处理文本，我们需要把文本信息转换为计算机可以理解的数字。怎么做呢？你可以这样想：我们是否可以给每个字在多维空间中找到一个坐标。我们的目标是，相近的词，坐标要接近。这个方法叫做 Word Emebedding。\n",
    "\n",
    "如果把这个多维空间中的许多坐标投影到二维，我们希望看到的结果是：\n",
    "\n",
    "![Word Embedding](img/word_embedding.png)\n",
    "\n",
    "当然，一个词可以有位置，那我们也可以给每个句子、段落、文章找到位置。这些在 retrieval-augmented generation (RAG) 中比较常用，比如每个电影的介绍，我们确定一个位置，然后用户搜索后，确定搜索文本的位置，然后找到和这个搜索文本位置最接近的电影介绍，最后喂给 LLM，输出结果。具体实现可以看我的[这篇博客](/cn/2025/03/16/bert/)。\n",
    "\n",
    "下面，我们用一篇文章来展示处理文本的主要步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# 下载文本信息\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"data/the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "# 读取\n",
    "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of character:\", len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married \n"
     ]
    }
   ],
   "source": [
    "# 打印前 199 个字符\n",
    "print(raw_text[:199])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本到 ID 相互转换\n",
    "\n",
    "之前提到过，计算机只能读取数字。我们需要给每一个 token 安排一个 ID。比如，我们说 I 对应 0, always 对应 1 等等。你可以把这个想象成一个字典或者电台密码本。但有个问题是，小的文本我们可以给每个词一个 id，但是在大语言模型训练中，肯定会出现新的词，以及很奇怪的词，那怎么办？比如，一篇文章里突然出现了 SomeStrangeWordYouHaveNeverSeenBefore 这个词，怎么办？\n",
    "\n",
    "有一个办法是所有字典里没有的词，统一用 `<|unk|>` 来代替，这里提一句，还有一个非常重要的 id 是 `<|endoftext|>`，用来指示一段文本的结束。这可以在训练时告诉 LLM，下面内容来自不同的来源。\n",
    "\n",
    "用 `<|unk|>` 的弊端至少有两个：1. 错过一些有意义的陌生词，比如我上面举的那个例子。也就是 LLM 训练时，我们没办法正确给它位置，因为我们看不到它的内容。2. 从 ID 转换到文本这个过程会失败。比如，我们知道 1 对应 always，但是我们肯定无法根据 ID 转换回 SomeStrangeWordYouHaveNeverSeenBefore。\n",
    "\n",
    "一个更有效的方法是 byte pair encoding (BPE)，也是 GPT 训练时用的方法。我们会用到 OpenAI 开发的包 [tiktoken](https://github.com/openai/tiktoken)。\n",
    "\n",
    "在 Terminal 输入：\n",
    "\n",
    "```sh\n",
    "pip install tiktoken\n",
    "```\n",
    "\n",
    "然后正常使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 290, 2773, 38114, 26449, 1639, 11980, 12295, 4653, 268, 8421, 13]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "# 初始化\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# 测试从文本到 token ID\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces \"\n",
    "    \"of someunknownPlace and SomeStrangeWordYouHaveNeverSeenBefore.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace and SomeStrangeWordYouHaveNeverSeenBefore.\n"
     ]
    }
   ],
   "source": [
    "# 检测从 ID 回到文本\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看到转换很顺利。BEP 的机制如下：\n",
    "\n",
    "![byte pair encoding](img/byte_pair_encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备训练数据\n",
    "\n",
    "还记得我们之前提到过的 LLM 训练方法吗？根据之前的文本，预测下一个词：\n",
    "\n",
    "![LLM Prediction](img/llm_predict.png)\n",
    "\n",
    "那训练数据是不是要这样子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -----> 4920\n",
      "[290, 4920] -----> 2241\n",
      "[290, 4920, 2241] -----> 287\n",
      "[290, 4920, 2241, 287] -----> 257\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "enc_sample = enc_text[50:] # 此步非必须\n",
    "context_size = 4 \n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"----->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ----->  established\n",
      " and established ----->  himself\n",
      " and established himself ----->  in\n",
      " and established himself in ----->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"----->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的最终目标是得到 inputs tensor 和 target tensor:\n",
    "\n",
    "![Data Loader](img/data_loader.png)\n",
    "\n",
    "这里有两个问题，或者说变量。第一个是每个 vector 有多大？我们用 `max_length` 表示。这里显示是 4。但这只是为了显示，在真正的 LLM 训练中，`max_length >= 256`。\n",
    "\n",
    "另一个变量是 `stride`: 一句话结束之后，我们下一句的开始在哪里？\n",
    "\n",
    "![Stride illustration](img/stride.png)\n",
    "\n",
    "这里面再加一个变量：`batch size`，也就是训练数据的长度。在图 2.13 中，如果省略号代表的是最后一组数据，那么 `batch size = 4`，因为有四组数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt:str, tokenizer, max_length:int, stride:int) -> None:\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i+1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  40,  367, 2885, 1464]),\n",
       " tensor([1807, 3619,  402,  271]),\n",
       " tensor([10899,  2138,   257,  7026]),\n",
       " tensor([15632,   438,  2016,   257]),\n",
       " tensor([ 922, 5891, 1576,  438])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_Dataset = GPTDatasetV1(txt=raw_text, tokenizer=tokenizer, max_length=4, stride=4)\n",
    "\n",
    "# input\n",
    "GPT_Dataset.input_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 367, 2885, 1464, 1807]),\n",
       " tensor([ 3619,   402,   271, 10899]),\n",
       " tensor([ 2138,   257,  7026, 15632]),\n",
       " tensor([ 438, 2016,  257,  922]),\n",
       " tensor([5891, 1576,  438,  568])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target\n",
    "GPT_Dataset.target_ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们继续"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "        txt, batch_size = 4, max_length = 256, stride = 128, shuffle=True,\n",
    "        drop_last=True, num_workers = 0\n",
    "):\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们试一下不一样的设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs: \\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 制作 token embeddings\n",
    "\n",
    "准备大语言模型训练数据的最后一步是把 token id 转化成 embedding vectors。也就是说，给每一个 token id 代表的文本找到其在 embedding 空间中的位置。\n",
    "\n",
    "![Token ID to Embedding vectors](img/id2embedding.png)\n",
    "\n",
    "我们举个例子。假设全世界总共有 6 个字，每个字在三维的 embedding 空间中有坐标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 假设我们有四个 token ids\n",
    "input_ids = torch.tensor([2,3,5,1])\n",
    "\n",
    "# 打印出这四个 token 的 embedding 空间位置\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但这里有一个问题，这种在 embedding layer 中查找词然后确定 embedding vector 的做法有一个缺陷：它无法告诉我们位置信息。比如，这样一组词：fox jumps over fox。两个 fox 的 embedding vector 是相同的：\n",
    "\n",
    "![Same embedding vector](img/same_embedding_vector.png)\n",
    "\n",
    "有两种解决办法。第一种办法是每一个位置有一个相对应的 positional embedding。然后每一个词的 embedding 是 token embedding + positional embedding。这也是 GPT 模型所采用的方法。第二种方法是让模型学习每一个不同位置的词的相对距离，这里我们不展开，大家知道有这种方法即可。\n",
    "\n",
    "![Absolute positional embeddings](img/absolute_positional_embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好，我们现在用更加现实的方法来举例子。\n",
    "\n",
    "我们就用刚才 `dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)` 的结果。我们来看一下 `inputs` 的维度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inputs shape: \n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInputs shape: \\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们刚才举的 embedding 例子，用到\n",
    "\n",
    "```py\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "```\n",
    "\n",
    "但是在现实中这不现实。比如，GPT-3 的 `output_dim` 高达 12,288。更现实的情况是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那现在想一下 `token_embedding_layer(inputs)` 会是什么样的维度？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们制作 position embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们加工 input embeddings:\n",
    "\n",
    "![Input embeddings pipeline](img/input_embeddings.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
