{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"自己动手从零开始写语言模型 (二): 文本数据的处理方法\"\n",
    "date: 2025-07-20T09:05:03-05:00\n",
    "author: \"郝鸿涛\"\n",
    "slug: llm\n",
    "draft: true\n",
    "toc: false\n",
    "tags: llm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算机无法直接处理文本，我们需要把文本信息转换为计算机可以理解的数字。怎么做呢？你可以这样想：我们是否可以给每个字在多维空间中找到一个坐标。我们的目标是，相近的词，坐标要接近。这个方法叫做 Word Emebedding。\n",
    "\n",
    "如果把这个多维空间中的许多坐标投影到二维，我们希望看到的结果是：\n",
    "\n",
    "![Word Embedding](img/word_embedding.png)\n",
    "\n",
    "当然，一个词可以有位置，那我们也可以给每个句子、段落、文章找到位置。这些在 retrieval-augmented generation (RAG) 中比较常用，比如每个电影的介绍，我们确定一个位置，然后用户搜索后，确定搜索文本的位置，然后找到和这个搜索文本位置最接近的电影介绍，最后喂给 LLM，输出结果。具体实现可以看我的[这篇博客](/cn/2025/03/16/bert/)。\n",
    "\n",
    "下面，我们用一篇文章来展示处理文本的主要步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# 下载文本信息\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"data/the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "# 读取\n",
    "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of character:\", len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married \n"
     ]
    }
   ],
   "source": [
    "# 打印前 199 个字符\n",
    "print(raw_text[:199])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本到 ID 相互转换\n",
    "\n",
    "之前提到过，计算机只能读取数字。我们需要给每一个 token 安排一个 ID。比如，我们说 I 对应 0, always 对应 1 等等。你可以把这个想象成一个字典或者电台密码本。但有个问题是，小的文本我们可以给每个词一个 id，但是在大语言模型训练中，肯定会出现新的词，以及很奇怪的词，那怎么办？比如，一篇文章里突然出现了 SomeStrangeWordYouHaveNeverSeenBefore 这个词，怎么办？\n",
    "\n",
    "有一个办法是所有字典里没有的词，统一用 `<|unk|>` 来代替，这里提一句，还有一个非常重要的 id 是 `<|endoftext|>`，用来指示一段文本的结束。这可以在训练时告诉 LLM，下面内容来自不同的来源。\n",
    "\n",
    "用 `<|unk|>` 的弊端至少有两个：1. 错过一些有意义的陌生词，比如我上面举的那个例子。也就是 LLM 训练时，我们没办法正确给它位置，因为我们看不到它的内容。2. 从 ID 转换到文本这个过程会失败。比如，我们知道 1 对应 always，但是我们肯定无法根据 ID 转换回 SomeStrangeWordYouHaveNeverSeenBefore。\n",
    "\n",
    "一个更有效的方法是 byte pair encoding (BPE)，也是 GPT 训练时用的方法。我们会用到 OpenAI 开发的包 [tiktoken](https://github.com/openai/tiktoken)。\n",
    "\n",
    "在 Terminal 输入：\n",
    "\n",
    "```sh\n",
    "pip install tiktoken\n",
    "```\n",
    "\n",
    "然后正常使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 290, 2773, 38114, 26449, 1639, 11980, 12295, 4653, 268, 8421, 13]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "# 初始化\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# 测试从文本到 token ID\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces \"\n",
    "    \"of someunknownPlace and SomeStrangeWordYouHaveNeverSeenBefore.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace and SomeStrangeWordYouHaveNeverSeenBefore.\n"
     ]
    }
   ],
   "source": [
    "# 检测从 ID 回到文本\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看到转换很顺利。BEP 的机制如下：\n",
    "\n",
    "![byte pair encoding](img/byte_pair_encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备训练数据\n",
    "\n",
    "还记得我们之前提到过的 LLM 训练方法吗？根据之前的文本，预测下一个词：\n",
    "\n",
    "![LLM Prediction](img/llm_predict.png)\n",
    "\n",
    "那训练数据是不是要这样子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -----> 4920\n",
      "[290, 4920] -----> 2241\n",
      "[290, 4920, 2241] -----> 287\n",
      "[290, 4920, 2241, 287] -----> 257\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "enc_sample = enc_text[50:] # 此步非必须\n",
    "context_size = 4 \n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"----->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ----->  established\n",
      " and established ----->  himself\n",
      " and established himself ----->  in\n",
      " and established himself in ----->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"----->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的最终目标是得到 inputs tensor 和 target tensor:\n",
    "\n",
    "![Data Loader](img/data_loader.png)\n",
    "\n",
    "这里有两个问题，或者说变量。第一个是每个 vector 有多大？我们用 `max_length` 表示。这里显示是 4。但这只是为了显示，在真正的 LLM 训练中，`max_length >= 256`。\n",
    "\n",
    "另一个变量是 `stride`: 一句话结束之后，我们下一句的开始在哪里？\n",
    "\n",
    "![Stride illustration](img/stride.png)\n",
    "\n",
    "这里面再加一个变量：`batch size`，也就是训练数据的长度。在图 2.13 中，如果省略号代表的是最后一组数据，那么 `batch size = 4`，因为有四组数据。\n",
    "\n",
    "## 制作 token embeddings\n",
    "\n",
    "准备大语言模型训练数据的最后一步是把 token id 转化成 embedding vectors。也就是说，给每一个 token id 代表的文本找到其在 embedding 空间中的位置。\n",
    "\n",
    "![Token ID to Embedding vectors](img/id2embedding.png)\n",
    "\n",
    "我们举个例子。假设全世界总共有 6 个字，每个字在三维的 embedding 空间中有坐标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 假设我们有四个 token ids\n",
    "input_ids = torch.tensor([2,3,5,1])\n",
    "\n",
    "# 打印出这四个 token 的 embedding 空间位置\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但这里有一个问题，这种在 embedding layer 中查找词然后确定 embedding vector 的做法有一个缺陷：它无法告诉我们位置信息。比如，这样一组词：fox jumps over fox。两个 fox 的 embedding vector 是相同的：\n",
    "\n",
    "![Same embedding vector](img/llm/same_embedding_vector.png)\n",
    "\n",
    "有两种解决办法。第一种办法是每一个位置有一个相对应的 positional embedding。然后每一个词的 embedding 是 token embedding + positional embedding。这也是 GPT 模型所采用的方法。第二种方法是让模型学习每一个不同位置的词的相对距离，这里我们不展开，大家知道有这种方法即可。\n",
    "\n",
    "![Absolute positional embeddings](img/llm/absolute_positional_embeddings.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "        txt, batch_size = 4, max_length = 256, stride = 128, shuffle=True,\n",
    "        drop_last=True, num_workers = 0\n",
    "):\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
